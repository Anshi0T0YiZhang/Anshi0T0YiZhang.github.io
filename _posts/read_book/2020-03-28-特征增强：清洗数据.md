---
title: "特征增强：清洗数据"
subtitle: "《特征工程入门与实践》02"
layout: post
author: "echisenyang"
header-style: text
hidden: true
catalog: true
tags:
  - book
  - 《特征工程入门与实践》
---



## 特征增强：清洗数据

> 本章概要：
>
> 1）识别数据中的缺失值； 
> 
> 2）删除有害数据； 
> 
> 3）输入（填充）缺失值； 
> 
> 4）对数据进行归一化/标准化； 
> 
> 5）构建新特征； 
> 
> 6）手动或自动选择（移除）特征； 
> 
> 7）使用数学矩阵计算将数据集转换到不同的维度。

> 本章将使用之前学到的知识，并且开始进一步修改我们的数据集。具体来说，我们将开始**清洗和增强数据**：前者是指**调整已有的列和行**，后者则是指**在数据集中删除和添加新的列**。和以往 一样，所有这些操作的目标都是优化机器学习流水线。这些方法会帮助我们**更好地了解数据中的哪些特征更重要**。本章将深入讨论前 4 种方法，后 3 种方法留到之后章节中讨论。
>
> 本章只处理定量特征，因 为目前没有足够的工具处理缺失的定性特征。下一章会研究特征构建，解决这个问题。

### 1）识别数据中的缺失值

- 皮马印第安人糖尿病预测数据集

  (1) 怀孕次数；times_pregnant

  (2) 口服葡萄糖耐量试验中的 2 小时血浆葡萄糖浓度；plasma_glucose_concentration

  (3) 舒张压（mmHg）；diastolic_blood_pressure

  (4) 三头肌皮褶厚度（mm）；triceps_thickness

  (5) 2 小时血清胰岛素浓度（μU/ml）；serum_insulin

  (6) 体重指数［BMI，即体重（kg）除以身高（m）的平方］；bmi

  (7) 糖尿病家族函数；pedigree_function

  (8) 年龄（岁）；age

  (9) 类变量（0 或 1，代表无或有糖尿病）。onset_diabetes

### 2）探索性数据分析

```python
# 导入探索性数据分析所需的包 
import pandas as pd # 存储表格数据 
import numpy as np # 数学计算包 
import matplotlib.pyplot as plt # 流行的数据可视化工具 
import seaborn as sns # 另一个流行的数据可视化工具 
%matplotlib inline 
plt.style.use('fivethirtyeight') # 流行的数据可视化主题

pima['onset_diabetes'].value_counts(normalize=True)
>>> # 空准确率，65%的人没有糖尿病

    0 0.651042 
    1 0.348958 
    Name: onset_diabetes, dtype: float64
```

> 既然终极目标是研究数据的规律以预测是否会患糖尿病，那么可以对糖尿病患者和健康人的 区别进行可视化。希望直方图可以显示一些规律，或者这两类之间的显著差异：

```python
# 对 plasma_glucose_concentration 列绘制两类的直方图

col = 'plasma_glucose_concentration' 
plt.hist(pima[pima['onset_diabetes']==0][col], 10, alpha=0.5, label='non-diabetes') plt.hist(pima[pima['onset_diabetes']==1][col], 10, alpha=0.5, label='diabetes') plt.legend(loc='upper right') 
plt.xlabel(col) 
plt.ylabel('Frequency') 
plt.title('Histogram of {}'.format(col)) 
plt.show()
```

> ‼️ 看起来患者和常人的血浆葡萄糖浓度（plasma_glucose_concentration）有很大的差异。
>
> 我们继续按此绘制其他列的直方图：

| <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/H3aubx.jpg" alt="image-20200328222441490" style="zoom:33%;" /> | <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/BpWlqF.png" style="zoom:33%;" /> |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/frMHPx.png" style="zoom:33%;" /> | <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/NHIO8n.png" style="zoom:33%;" /> |

> ‼️ **观察几张直方图后，可以看出两类人存在明显区别**。例如，对于最终患有糖尿病的患者，其 血浆葡萄糖浓度会有很大的增长。我们**可以用线性相关矩阵来量化这些变量间的关系**。用本章一 开始导入的 Seaborn 作为可视化工具：

```python
# 数据集相关矩阵的热力图 
sns.heatmap(pima.corr()) 
# plasma_glucose_concentration 很明显是重要的变量

# 我们进一步研究 onset_diabetes 列的相关性数值
pima.corr()['onset_diabetes'] # 相关矩阵

>>> times_pregnant 0.221898 
    plasma_glucose_concentration 0.466581  # plasma_glucose_concentration 很明显是重要的变量
    diastolic_blood_pressure 0.065068 
    triceps_thickness 0.074752 
    serum_insulin 0.130548 
    bmi 0.292695 
    pedigree_function 0.173844 
    age 0.238356 
    onset_diabetes 1.000000 
    Name: onset_diabetes, dtype: float64
```

> 第 4 章会进一步研究相关性的各种功能，而目前探索性数据分析提示，**plasma_glucose_ concentration 是预测糖尿病的重要变量**。

> 下一步更重要：我们要看看数据集中是否有数据点是空的（缺失值）。用 Pandas DataFrame 内置的 isnull()方法：
>
> 很好！数据中没有缺失值。继续探索数据，首先用 shape 方法看看数据的行数和列数：
>
> 我们确定数据有 9 列（包括要预测的变量）和 768 个观察值（行）。用下面的代码看看糖尿 病的发病率：
>
> DataFrame 内置的 describe 方法可以提供数据 基本的描述性统计：

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/7ns1Id.png)

> ‼️ 因为 onset_diabetes 中的 0 代表没有糖尿病，人也可以怀孕 0 次，所以可以**得出结论， 下面这些列中的缺失值用 0 填充了**：plasma_glucose_concentration、diastolic_blood_pressure、triceps_thickness、serum_insulin、bmi

> ⚠️ **数据中还是存在缺失值的！我们已经知道缺失的数据用 0 填充过了，真不走运**。作为数据科学家，你必须时刻保持警惕，尽可能地了解数据集，以便找到使用其他符号填充的缺失数据。

### 3）处理数据集中的缺失值

> 虽然办法有很多变种， 但是两个最主要的处理方法是：
>
> 1）删除缺少值的行；
>
> 2）填充缺失值。

> 在进一步处理前，先用Python中的 None 填充所有的数字0，这样Pandas的 fillna 和 dropna方法就可以正常工作了

```python
# 直接对所有列操作，快一些
columns = ['serum_insulin', 'bmi', 'plasma_glucose_concentration', 'diastolic_blood_pressure', 'triceps_thickness']

for col in columns:
		pima[col].replace([0], [None], inplace=True)
    
pima.isnull().sum()
# 现在有意义多了
times_pregnant                0
plasma_glucose_concentration  5
diastolic_blood_pressure      35
triceps_thickness             227
serum_insulin                 374
bmi                           11
pedigree_function             0
age                           0
onset_diabetes                0
dtype: int64
```

> 现在数据有意义多了。我们可以看见其中 5 列有缺失值，**缺失程度有所不同**。有些列，例如 plasma_glucose_concentration 只缺少 5 个值，但是 serum_insulin 差不多一半的值都是 缺失的。

> 我们已经将缺失值正确插入了数据集，缺失数据不再是原来的占位符 0。这样**探索性数据分析也会更准确**：
>
> ⚠️ 注意 describe 方法不包括有缺失值的列。

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/VAB96i.png)

#### 3.1 删除有害的行

> 在处理缺失数据的两种办法中，最常见也最容易的方法大概是直接删除存在缺失值的行。通过这种操作，我们会留下具有数据的完整数据点。可以在 Pandas 中利用 dropna 方法获取新的 DataFrame，如下所示：

```python
# 删除存在缺失值的行 
pima_dropped = pima.dropna()
num_rows_lost = round(100*(pima.shape[0] - pima_dropped.shape[0])/float(pima.shape[0]))
print("retained {}% of rows".format(num_rows_lost))
>>> retained 49% of rows
```

> ⚠️ **我们丢失了原始数据集中大约 51%的行！**从机器学习的角度考虑，尽管数据都有值、很干净， 但是我们没有利用尽可能多的数据，忽略了一半以上的观察值。就像医生在研究心脏病发作原理 时，忽略了一半以上的患者。

> 下面对数据集做进一步的探索性数据分析，**比较一下丢弃缺失值前后的统计数据**：

```python
# 丢弃缺失值前后的探索性数据分析
# 分成 True 和 False 两组 
pima['onset_diabetes'].value_counts(normalize=True)

>>> 0 0.651042 
    1 0.348958 
    Name: onset_diabetes, dtype: float64

# 看一下丢弃数据后的统计数据，如下面代码所示：
pima_dropped['onset_diabetes'].value_counts(normalize=True)

>>> 0 0.668367 
		1 0.331633 
		Name: onset_diabetes, dtype: float64
```

> 前后的 True 和 False 比例差不多。在大刀阔斧的转换后，二元响应看起来没什么变化。我们用 pima.mean 函数比较一下转换 前后列的均值，看看数据的形状，结果如下：

| <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Dz5SdL.png" style="zoom:50%;" /> | <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/2BCLon.png" style="zoom:50%;" /> |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
|                         丢弃缺失值前                         |                         丢弃缺失值后                         |
| <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/WMcFPs.png" style="zoom:50%;" /> | <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/pYCwo4.png" style="zoom:50%;" /> |
|                    均值变化的百分比可视化                    |                     均值变化百分比条形图                     |

> 为了更好地了解这些数的变化，我们创建一个新图表，将每列均值变化的百分比可视化。首先创建一个表格，列出每列均值变化的百分比
>
> ```python
> # 均值变化百分比 
> (pima_dropped.mean() - pima.mean()) / pima.mean()
> ```
>
> 现在用条形图对这些变化进行可视化：
>
> ```python
> # 均值变化百分比条形图 
> ax = ((pima_dropped.mean() - pima.mean()) / pima.mean()).plot(kind='bar', title='% change in average column values') 
> ax.set_ylabel('% change')
> ```

> ‼️ 可以看到，times_pregnant（怀孕次数）的均值在删除缺失值后**下降了 14%，变化很大**！ pedigree_function（糖尿病血系功能）也**上升了 11%，也是个飞跃**。可以看到，**删除行（观 察值）会严重影响数据的形状，所以应该保留尽可能多的数据**。在介绍下一个处理方法前，我们 先进行一些机器学习。

```python
# 开始机器学习
# 注意使用删除缺失值后的数据
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.model_selection import GridSearchCV

X_dropped = pima_dropped.drop('onset_diabetes', axis=1) 
# 删除响应变量，建立特征矩阵 
print("learning from {} rows".format(X_dropped.shape[0]))

>>> learning from 392 rows

y_dropped = pima_dropped['onset_diabetes']
# 网格搜索所需的变量和实例
# 需要试验的 KNN 模型参数 
knn_params = {'n_neighbors':[1, 2, 3, 4, 5, 6, 7]}
knn = KNeighborsClassifier() # 设置 KNN 模型
grid = GridSearchCV(knn, knn_params) 
grid.fit(X_dropped, y_dropped)
print(grid.best_score_, grid.best_params_) # 但是我们只学习了很少的行

>>> 0.7448979591836735 {'n_neighbors': 7}
```

> ❌ 现在已经看出问题了：机器学习算法使用的数据比一开始拿到的数据少得多。
>
> ⭕️ 看来，最好的邻居数是 7 个，此时 KNN 模型的准确率是 74.5%（比空准确率 65%好）。但是 要记住，这个模型只用了 49%的数据，**如果能用到所有数据，会不会更好一些**？

#### 3.2 填充缺失值

> 填充数据是处理缺失值的一种更复杂的方法。填充指的是利用现有知识/数据来确定缺失的数 量值并填充的行为。我们有几个选择，**最常见的是用此列其余部分的均值填充缺失值**，还可以用 **scikit-learn 预处理类的 Imputer 模块**，Imputer 在很大程度上解决了填充缺失值的琐事。

```python
imputer = Imputer(strategy='mean')
pima_imputed = imputer.fit_transform(pima) # ndarray
pima_imputed = pd.DataFrame(pima_imputed, columns=pima_column_names)
```

> 我们尝试填充一些别的值，看 看对 KNN 模型的影响。首先尝试一种更简单的填充方法，用 0 替代所有的缺失值。实验结论：用 0 填充准确率下降，会低于直接删掉有缺失值的行。

> 🚫 不恰当的做法：在划分前填充值。
>
> ‼️ 恰当的做法：在划分后填充值。我们先计算出训练集的均值，然后**用训练集的均值填充训练集和测试集的缺失值**。 这个过程再一次测试了模型用训练数据的均值预测未知测试集的能力

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/lMLzUM.png)

#### 3.3 标准化和归一化

> 到目前为止，我们已经知道了如何识别数据类型，如何识别缺失值，以及如何处理缺失值。 现在继续讨论如何处理数据（和特征），以进一步增强机器学习流水线。目前，我们已经用过 4 种不同的方式处理数据集，最佳的 KNN 交叉验证准确率是 0.745。如果回头看之前的探索性数据 分析，会发现一些特征的性质：

```python
impute = Imputer(strategy='mean') 
# 填充所有的缺失值
pima_imputed_mean = pd.DataFrame(impute.fit_transform(pima), columns=pima_column_names)
pima_imputed_mean.hist(figsize=(15, 15))
```

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/kchXGz.png)

> ‼️ 很好， 但是你注意到什么了吗？**每列的均值、最小值、最大值和标准差差别很大**。 通过 describe 方法也可以看到很明显的差别：

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/k5JCLH.png)

> ⚠️ 这有什么关系呢？**因为一些机器学习模型受数据尺度（scale）的影响很大**。这意味着如果 diastolic_blood_pressure 列的舒张压在 24～122，但是年龄是 21～81，那么算法不会达到 最优化状态。我们可以在直方图方法中调用可选的 sharex 和 sharey 参数，在同一比例下查看 每个图表：
>
> `pima_imputed_mean.hist(figsize=(15, 15), sharex=True) # x 轴相同（y 轴不重要）`

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/2Za0En.png)

> ‼️ **很明显，所有的数据尺度都不同**。数据工程师可以选用某种归一化操作，在机器学习流水 线上处理该问题。**归一化操作旨在将行和列对齐并转化为一致的规则**。例如，归一化的一种常 见形式是将所有定量列转化为同一个静态范围中的值（例如，所有数都位于 0～1）。我们也可 以使用数学规则，例如所有列的均值和标准差必须相同，以便在同一个直方图上显示（和上面 皮马人的直方图不同）。标准化通过确保所有行和列在机器学习中得到平等对待，让数据的处理 保持一致。

##### 3.3.1 z 分数标准化 $z=(x-\mu) / \sigma$

```python
# 内置的 z 分数归一化 
from sklearn.preprocessing import StandardScaler
# z 分数标准化前的均值和标准差 
pima['plasma_glucose_concentration'].mean(),
pima['plasma_glucose_concentration'].std()
>>> (121.68676277850591, 30.435948867207657)
ax = pima['plasma_glucose_concentration'].hist() 
ax.set_title('Distribution of plasma_glucose_concentration')

# 归一化后
scaler = StandardScaler()
glucose_z_score_standardized = scaler.fit_transform(pima[['plasma_glucose_concentration']])
glucose_z_score_standardized.mean(), glucose_z_score_standardized.std()
>>> (-3.561965537339044e-16, 1.0)
```

| <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/q27OMi.png" style="zoom:50%;" /> | <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/7hpqj2.png" style="zoom:50%;" /> |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
|                       处理前的分布情况                       |                     缩放后数据的分布情况                     |

> 我们观察到 x 轴更紧密了，y 轴则没有变化。注意，**数据的形状没有变化**。

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/UCiY0W.png)

> 现在我们正在寻 找 KNN 中策略和邻居数的最佳组合，**目前的得分是 0.742，这是至今为止的最佳得分**，接近目标 0.745。这条流水线从整个 768 行中学习。我们现在看看另一种标准化方法。

##### 3.3.2 min-max 标准化 $m=\left(x-x_{\min }\right) /\left(x_{\max }-x_{\min }\right)$

```python
# 导入 sklearn 模块 
from sklearn.preprocessing import MinMaxScaler
# 实例化 min_max = MinMaxScaler()
# 使用 min-max 标准化 
pima_min_maxed = pd.DataFrame(min_max.fit_transform(pima_imputed), columns=pima_column_names)
# 得到描述性统计 
pima_min_maxed.describe()
```

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/MZg8lj.png)

> ⚠️ 注意，最小值都是 0，最大值都是 1。**进一步注意，这种缩放的副作用是标准差都非常小。这 有可能不利于某些模型，因为异常值的权重降低了**。0.74609375，这是至今使用包括缺失数据的全部 768 行的最好结果。看起来 min-max 缩放对 KNN 有很大 帮助！

##### 3.3.3 行归一化 

> 最后这个标准化方法是关于行，而不是关于列的。行归一化不是计算每列的统计值（均值、 最小值、最大值等），而是会保证每行有单位范数（unit norm），意味着每行的向量长度相同。想 象一下，如果每行数据都在一个 n 维空间内，那么每行都有一个向量范数（长度）。**我们认为每行都是空间内的一个向量**。

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/rhCuhl.png)
