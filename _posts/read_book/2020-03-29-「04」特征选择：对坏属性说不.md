---
title: "「04」特征选择：对坏属性说不"
subtitle: "《特征工程入门与实践》"
layout: post
author: "echisenyang"
header-style: text
hidden: true
catalog: true
tags:
  - book
  - 《特征工程入门与实践》
---



## 特征选择：对坏属性说不

> 本章会讨论特征工程的一个子集，称为**特征选择**。特征选择是从原始数据中选择对于预测流 水线而言最好的特征的过程。更正式地说，给定 n 个特征，我们搜索其中包括 k（k < n）个特征 的子集来改善机器学习流水线的性能。一般来说，我们的意思是：**特征选择尝试剔除数据中的噪声**。
>
> - 这个定义包括两个需要解决的问题：
>
>   1) 找到 **k 特征子集**的办法； 
>
>   2) 在机器学习中对“**更好**”的定义。

> 本章概要：
>
> 1）在特征工程中实现更好的性能； 
>
> 2）创建基准机器学习流水线； 
>
> 3）特征选择的类型； 
>
> 4）选用正确的特征选择方法。

### 1）在特征工程中实现更好的性能

> 特征选择算法可以智能地**从数据中提取最重要的信号并忽略噪声**，达到以下两个结果。
>
> 1.**提升模型性能**：在删除冗余数据后，**基于噪声和不相关数据做出错误决策的情况会减少**， 而且模型可以在重要的特征上练习，提高预测性能。 
>
> 2.减少训练和预测时间：因为拟合的数据更少，所以模型一般在拟合和训练上有速度提升，让流水线的整体速度更快。

### 2）创建基准机器学习流水线

> 在本章中，我们会做一些工作， 寻找最符合我们需求的机器学习模型，然后通过特征选择来增强模型。

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/X85o9o.png)

### 3）特征选择的类型

#### 基于统计的特征选择

- 使用皮尔逊相关系数

> 皮尔逊相关系数（是 Pandas 默认的）会测量列之间的线性关系。该系数在-1～1 变化，0 代 表没有线性关系。**相关性接近-1 或 1 代表线性关系很强**。
>
> ‼️ 注意，heatmap 函数会自动选择最相关的特征进行展示。不过，我们目前关注特征和响应变量的相关性。**我们假设，和响应变量越相关，特征就越有用**。不太相关的特征应该没有什么用。

```python
# 存储特征 
highly_correlated_features = credit_card_default.columns[credit_card_default.corr()['default payment next month'].abs() > .2]

>>> Index(['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'default payment ne
```

> 可以通过gridsearch的方式选择阈值，通过模型的表现来决定最后要留下的topn个重要性特征

- 使用假设检验

> **在特征选择中，假设测试的原则是：“特征与响应变量没有关系”（零假设）为真还是假**。我 们需要在每个特征上进行检验，并决定其与响应变量是否有显著关系。在某种程度上说，我们的 相关性检测逻辑也是这样运作的。我们的意思是，**如果某个特征与响应变量的相关性太弱，那么 认为“特征与响应变量没有关系”这个假设为真**。**如果相关系数足够强，那么拒绝该假设，认为 特征与响应变量有关**。

#### Univariate feature selection[¶](https://scikit-learn.org/stable/modules/feature_selection#univariate-feature-selection)

```python
# SelectKBest 在给定目标函数后选择 k 个最高分 
from sklearn.feature_selection import SelectKBest

# ANOVA 测试 
from sklearn.feature_selection import f_classif

# f_classif 可以使用负数，但不是所有类都支持 
# chi2（卡方）也很常用，但只支持正数 
# 回归分析有自己的假设检验
```

> **理解 p 值**
>
> - p 值是介于 0 和 1 的小数，代表在假设检验下，给定数据偶然出现的概率。简而言之，p 值 越低，拒绝零假设的概率越大。在我们的例子中，p 值越低，这个特征与响应变量有关联的概率 就越大，我们应该保留这个特征。

> ⭕️ 得到最重要的topn个特征后，试着加入其他特征然后在模型上看效果

#### 基于模型的特征选择

- 使用机器学习选择特征

> 我们可以继续尝试几种基于树的模型，例如 RandomForest（随机森林）和 ExtraTreesClassifier（极限随机树）等，但是效果应该比不基于树的方法差。
>
> *上表显示，拟合中最重要的特征是 PAY_0，和上一章统计模型的结果相匹配。更值得注意的 是第 2、第 3 和第 5 个特征，这 3 个特征在进行统计测试前没有显示出重要性。**这意味着，这种 特征选择方法有可能带来一些新的结果。***

- 线性模型和正则化

> SelectFromModel 可以处理任何包括 $feature\_importances\_$ 或 $coef\_$ 属性的机器学习模型。基于树的模型会暴露前者，线性模型则会暴露后者。在拟合后，线性回归、逻辑回归、支 持向量机（SVM，support vector machine）等线性模型会将一个系数放在特征的斜率（重要性） 前面。SelectFromModel 会认为这个系数等同于重要性，并根据拟合时的系数选择特征。
>

> **正则化也有助于解决多重共线性的问题**，也就是说，数据中有多个线性相关的特 征。L1 惩罚可以强制其他线性相关特征的系数为 0，保证选择器不会选择这些 线性相关的特征，有助于解决过拟合问题。

### 4）选用正确的特征选择方法

> 现在你有可能感到本章的信息过多，难以消化。我们演示了几种选择特征的方法，其中一部 分基于统计学，另一部分基于机器学习模型的二次输出。一个很自然的问题是：应该如何选用特 征选择方法？理论上说，最理想的状况是，你可以像本章这样多次尝试，但我们知道这样是不可 行的。下面是一些经验，可以在判断特征选择方法的优劣时参考。
>
> 1.如果特征是分类的，那么从 SelectKBest 开始，用卡方或基于树的选择器。 
>
> 2.如果特征基本是定量的（例如本例），用线性模型和基于相关性的选择器一般效果更好。 
>
> 3.如果是二元分类问题，考虑使用 SelectFromModel 和 SVC，因为 SVC 会查找优化二元 分类任务的系数。
>
> 4.在手动选择前，探索性数据分析会很有益处。不能低估领域知识的重要性。

