---
title: "循环神经网络"
subtitle: "TensorFlow：实战Google深度学习框架「08」"
layout: post
author: "echisenyang"
header-style: text
hidden: true
catalog: true
tags:
  - TensorFlow
---



## 循环神经网络

循环神经网络的主要用途是**处理和预测序列数据**。**在之前介绍的全连接神经网络或卷积神经网络模型中**，网络结构都是从输入层到隐含层再到输出层，层与层之间是全连接或部分连接的，**但每层之间的节点是无连接的**。而**循环神经网络的隐藏层之间的结点是有连接的**，隐藏层的输入不仅包括输入层的输出 ，还包括上一时刻隐藏层的输出。

### 循环神经网络简介

源自于 1982 年霍普菲尔德网络

- 由于实现困难与提出时机不合适，该网络结构也于 1986 年后被全连接神经网络以及一些传统的机器学习算法所取代。
- 但是***传统的机器学习算法***非常依赖于人工提取的特征，使得基于传统机器学习的图像识别、语音识别以及自然语言处理等问题**存在特征提取的瓶颈**。而基于***全连接神经网络***的方法也存在**参数太多**、**无法利用数据中时间序列信息**等问题。

- 随着更加有效的循环神经网络结构被不断提出，循环神经网络挖掘数据中的**时序信息**以及**语义信息的深度表达能力**被充分利用。

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/J7NkCI.png" alt="J7NkCI"/>

![No6iVB](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/No6iVB.png)

![t0ifkr](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/t0ifkr.png)

```python
import numpy as np
# 1. 定义RNN的参数
X = [1,2]
state = [0.0, 0.0]

w_cell_state = np.asarray([[0.1, 0.2], [0.3, 0.4]])
w_cell_input = np.asarray([0.5, 0.6])
b_cell = np.asarray([0.1, -0.1])
w_output = np.asarray([[1.0], [2.0]])
b_output = 0.1
# 2. 执行前向传播过程
for i in range(len(X)):
    before_activation = np.dot(state, w_cell_state) + X[i] * w_cell_input + b_cell
    state = np.tanh(before_activation)
    final_output = np.dot(state, w_output) + b_output
    print "before activation: ", before_activation
    print "state: ", state
    print "output: ", final_output
    
>>> 
before activation:  [ 0.6  0.5]
state:  [ 0.53704957  0.46211716]
output:  [ 1.56128388]
before activation:  [ 1.2923401   1.39225678]
state:  [ 0.85973818  0.88366641]
output:  [ 2.72707101]
```

### 长短时记忆网络 (LSTM) 结构

**循环神经网络通过保存历史信息来帮助当前的决策**，例如使用之前出现的单词来加强对当前文字的理解。循环神经网络可以更好地利用传统神经网络结构所不能建模的信息，但同时，这也带来了更大的技术挑战一一**长期依赖（long-term dependencies）问题**。

长短时记忆网络（long short-term memory，LSTM）的设计就是为了解决这个问题。

- 1997年Hochreiter与Schrnidhuber提出

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/cZWGLA.png" alt="cZWGLA" style="zoom:50%;" />

![O8cmCF](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/O8cmCF.png)

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/fUgQZC.png" alt="fUgQZC" style="zoom:25%;" />

<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png" style="zoom: 33%;" />

<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png" style="zoom: 33%;" />

<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png" style="zoom: 33%;" />

<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png" style="zoom: 33%;" />

```python
def lstm_model(X, y, is_training):
    """
    使用单层 LSTM 结构的循环神经网络的前向传播过程
    """
	lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_hidden_size)
    # 将LSTM中的状态初始化为全0数组。BasicLSTMCell类提供了zero_state函数来生成 
    # 全零的初始状态。state是一个包含两个张量的LSTMStateTuple类，其中state.c和 
    # state.h分别对应了图8-7中的 c 状态和 h 状态。 
    # 和其他神经网络类似，在优化循环神经网络时，每次也会使用一个batch的训练样本。 
    # 以下代码中，batch_size 给出了一个 batch 的大小。
    state= lstm.zero_state(batch size,tf.float32)
    
    # 定义损失函数
    loss = 0.0
    # 虽然在测试时循环神经网络可以处理任意长度的序列，但是在训练中为了将循环网络展开成
  	# 前馈神经网络，我们需要知道训练数据的序列长度。在以下代码巾，用num_steps来表示 
    # 这个长度。 第9章将中介绍使用dynamic_rnn动态处理变长序列的方法。
    for i in range(num_steps):
        # 在第一个时刻声明LSTM结构中使用的变量，在之后的时刻都需要复用之前定义好的变量。
        if i > 0 : 
            tf.get_variable_scope().reuse_variables()
        # 每一步处理时间序列中的一个时刻。将当前输入current_input（图8-7中的X_t) 
       	# 和前一时刻状态 state (ht-l 和 Ct-1 ）传入定义的 LSTM 结构可以得到当前 LSTM 
        # 的输出 lstm_output (ht）和更新后状态 state (ht和Ct）。lstm_output 用于输出给 
        # 其他层，state用于输出给下一时刻，它们在dropout等方面可以有不问的处理方式。
        lstm_output, state = lstm(current_input, state)
        # 将当前时刻 LSTM 结构的输出传入一个全连接层得到最后的输出。
        final_output = fully_connected(lstm_output) 
        # 计算当前时刻输出的损失。
        loss+= calc_loss(final_output, expected_output)
```

### 循环神经网络的变种

- 双向循环神经网络和深层循环神经网络
  - 在经典的循环神经网络中，状态的传输是从前往后单向的 。然而，在有些问题中，**当前时刻的输出不仅和之前的状态有关系，也和之后的状态相关**。这时就需要使用双向循环神经网络（bidirectional RNN）来解决这类问题。
  - 为了增强模型的表达能力，可以在网络中设置多个循环层，将每层循环网络的输出传给下一层进行处理。**一个很浅的神经网络，从输入中提取抽象信息的能力将受到限制**。

```python
def stacked_lstm_model(X, y, is_training):
    """
    实现深层循环神经网络
    在 TensorFlow 中只需要在 BasicLSTMCell 的基础上再封装一层 MultiRNNCell 就可以非常容易地
    实现深层循环神经网络了。
    """
    # 使用多层的LSTM结构。
    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell() 
    stacked_lstm = tf.nn.rnn_cell.MultiRNNCell(
        [lstm_cell(lstm_hidden_size) for _ in range(number_of_layers)])
    
    # 和经典的循环神经网络一样，可以通过zero_state函数来获取初始状态
    state = stacked_lstm.zero_state(batch_size, tf.float32)

    # 和 使用单层 LSTM 结构的循环神经网络的前向传播过程 一样计算每一时刻的前向传播结果
    for i in range(num_steps):
        # 在第一个时刻声明LSTM结构中使用的变量，在之后的时刻都需要复用之前定义好的变量。
        if i > 0 : 
            tf.get_variable_scope().reuse_variables()
        stacked_lstm_output, state = stacked_lstm(current_input, state)
        # 将当前时刻 LSTM 结构的输出传入一个全连接层得到最后的输出。
        final_output = fully_connected(stacked_lstm_output) 
        # 计算当前时刻输出的损失。
        loss+= calc_loss(final_output, expected_output)
```

- 循环神经网络的 dropout
  - 循环神经网络一般**只在不同层循环体结构之间使用 dropout，而不在同一层的循环体结构之间使用**。也就是说从时刻t-1传递到时刻t时，循环神经网络不会进行状态的dropout；而在同一个时刻t 中，不同层循环体之间会使用dropout。

```python
def deep_lstm_model(X, y, is_training):
	# 定义使用LSTM结构及训练时使用dropout。
    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell() 
    stacked_lstm = tf.nn.rnn_cell.MultiRNNCell(
        [tf.contrib.rnn.DropoutWrapper(lstm_cell(lstm_size))
        for _ in range(number_of_layers)])
```

### 循环神经网络样例应用

这一节将以时序预测为例，利用循环神经网络实现对 $sin(x)$ 函数取值的预测

- 下面的篇幅将给出具体的TensorFlow程序来实现预测正弦函数sin。因为循环神经网络模型预测的是离散时刻的取值，所以在程序中需要**将连续的sin函数曲线离散化**。所谓离散化就是在一个给定的区间［O,MAX］内，通过有限个采样点模拟一个连续的曲线。比如在以下程序中每隔SAMPLE_ITERVAL对sin函数进行一次来样，来样得到的序列就是sin函数离散化之后的结果。


```python
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

##################################################################################
# 1. 定义RNN的参数
HIDDEN_SIZE = 30                            # LSTM中隐藏节点的个数。
NUM_LAYERS = 2                              # LSTM的层数。
TIMESTEPS = 10                              # 循环神经网络的训练序列长度。
TRAINING_STEPS = 10000                      # 训练轮数。
BATCH_SIZE = 32                             # batch大小。
TRAINING_EXAMPLES = 10000                   # 训练数据个数。
TESTING_EXAMPLES = 1000                     # 测试数据个数。
SAMPLE_GAP = 0.01                           # 采样间隔。
##################################################################################

##################################################################################
# 2. 产生正弦数据。
def generate_data(seq):
    """
    :param seq:
    :return: X shape [10000,1,10], y shape [10000,1]
    """
    X = []
    y = []
    # 序列的第i项和后面的TIMESTEPS-1项合在一起作为输入；第i + TIMESTEPS项作为输
    # 出。即用sin函数前面的TIMESTEPS个点的信息，预测第i + TIMESTEPS个点的函数值。
    for i in range(len(seq) - TIMESTEPS):
        X.append([seq[i: i + TIMESTEPS]])
        y.append([seq[i + TIMESTEPS]])
    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)  

# 用正弦函数生成训练和测试数据集合。
"""
train_X: <class 'tuple'>: (10000, 1, 10)
train_y: <class 'tuple'>: (10000, 1)
test_X: <class 'tuple'>: (1000, 1, 10)
test_y: <class 'tuple'>: (1000, 1)
"""
test_start = (TRAINING_EXAMPLES + TIMESTEPS) * SAMPLE_GAP
test_end = test_start + (TESTING_EXAMPLES + TIMESTEPS) * SAMPLE_GAP
train_X, train_y = generate_data(np.sin(np.linspace(
    0, test_start, TRAINING_EXAMPLES + TIMESTEPS, dtype=np.float32)))
test_X, test_y = generate_data(np.sin(np.linspace(
    test_start, test_end, TESTING_EXAMPLES + TIMESTEPS, dtype=np.float32)))
##################################################################################

##################################################################################
# 3. 定义网络结构和优化步骤。
def lstm_model(X, y, is_training):
    # 使用多层的LSTM结构。
    cell = tf.nn.rnn_cell.MultiRNNCell([
        tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE) 
        for _ in range(NUM_LAYERS)])    

    # 使用TensorFlow接口将多层的LSTM结构连接成RNN网络并计算其前向传播结果。
    outputs, _ = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)
    output = outputs[:, -1, :]

    # 对LSTM网络的输出再做加一层全链接层并计算损失。注意这里默认的损失为平均
    # 平方差损失函数。
    predictions = tf.contrib.layers.fully_connected(
        output, 1, activation_fn=None)
    
    # 只在训练时计算损失函数和优化步骤。测试时直接返回预测结果。
    if not is_training:
        return predictions, None, None
        
    # 计算损失函数。
    loss = tf.losses.mean_squared_error(labels=y, predictions=predictions)

    # 创建模型优化器并得到优化步骤。
    train_op = tf.contrib.layers.optimize_loss(
        loss, tf.train.get_global_step(),
        optimizer="Adagrad", learning_rate=0.1)
    return predictions, loss, train_op
##################################################################################

##################################################################################
# 4. 定义测试方法。
def run_eval(sess, test_X, test_y):
    # 将测试数据以数据集的方式提供给计算图。
    ds = tf.data.Dataset.from_tensor_slices((test_X, test_y))
    ds = ds.batch(1)
    X, y = ds.make_one_shot_iterator().get_next()
    
    # 调用模型得到计算结果。这里不需要输入真实的y值。
    with tf.variable_scope("model", reuse=True):
        prediction, _, _ = lstm_model(X, [0.0], False)
    
    # 将预测结果存入一个数组。
    predictions = []
    labels = []
    for i in range(TESTING_EXAMPLES):
        p, l = sess.run([prediction, y])
        predictions.append(p)
        labels.append(l)

    # 计算rmse作为评价指标。
    predictions = np.array(predictions).squeeze()
    labels = np.array(labels).squeeze()
    rmse = np.sqrt(((predictions - labels) ** 2).mean(axis=0))
    print("Root Mean Square Error is: %f" % rmse)
    
    #对预测的sin函数曲线进行绘图。
    plt.figure()
    plt.plot(predictions, label='predictions')
    plt.plot(labels, label='real_sin')
    plt.legend()
    plt.show()
##################################################################################

##################################################################################
# 5. 执行训练和测试。
# 将训练数据以数据集的方式提供给计算图。
ds = tf.data.Dataset.from_tensor_slices((train_X, train_y))
ds = ds.repeat().shuffle(1000).batch(BATCH_SIZE)
X, y = ds.make_one_shot_iterator().get_next()

# 定义模型，得到预测结果、损失函数，和训练操作。
with tf.variable_scope("model"):
    _, loss, train_op = lstm_model(X, y, True)
    
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    
    # 测试在训练之前的模型效果。
    print "Evaluate model before training."
    run_eval(sess, test_X, test_y)
    
    # 训练模型。
    for i in range(TRAINING_STEPS):
        _, l = sess.run([train_op, loss])
        if i % 1000 == 0:
            print("train step: " + str(i) + ", loss: " + str(l))
    
    # 使用训练好的模型对测试数据进行预测。
    print "Evaluate model after training."
    run_eval(sess, test_X, test_y)
##################################################################################
```

#### Dataset API

在TensorFlow 1.4中，Dataset API已经从contrib包中移除，变成了核心API的一员：`tf.data.Dataset`

![dwnH98](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/dwnH98.jpg)

**Dataset可以看作是相同类型“元素”的有序列表**。在实际使用时，单个“元素”可以是向量，也可以是字符串、图片，甚至是tuple或者dict。

```python
import tensorflow as tf
import numpy as np

dataset = tf.data.Dataset.from_tensor_slices(np.array([1.0, 2.0, 3.0, 4.0, 5.0]))
```

这样，我们就创建了一个dataset，这个dataset中含有5个元素，分别是1.0, 2.0, 3.0, 4.0, 5.0。

如何将这个dataset中的元素取出呢？**方法是从Dataset中示例化一个Iterator，然后对Iterator进行迭代。在非Eager模式下**，读取上述dataset中元素的方法为：

```python
iterator = dataset.make_one_shot_iterator()
one_element = iterator.get_next()
with tf.Session() as sess:
    for i in range(5):
        print(sess.run(one_element))
```

对应的输出结果应该就是从1.0到5.0。语句`iterator = dataset.make_one_shot_iterator()`从dataset中实例化了一个Iterator，这个Iterator是一个“one shot iterator”，即只能从头到尾读取一次。`one_element = iterator.get_next()`表示从iterator里取出一个元素。**由于这是非Eager模式，所以one_element只是一个Tensor，并不是一个实际的值。调用`sess.run(one_element)`后，才能真正地取出一个值。**

如果一个dataset中元素被读取完了，再尝试sess.run(one_element)的话，就会抛出tf.errors.OutOfRangeError异常，**这个行为与使用队列方式读取数据的行为是一致的。**

**在Eager模式中，创建Iterator的方式有所不同。**是通过tfe.Iterator(dataset)的形式直接创建Iterator并迭代。迭代时可以直接取出值，不需要使用sess.run()：

```python
import tensorflow.contrib.eager as tfe
tfe.enable_eager_execution()

dataset = tf.data.Dataset.from_tensor_slices(np.array([1.0, 2.0, 3.0, 4.0, 5.0]))

for one_element in tfe.Iterator(dataset):
    print(one_element)
```

