---
title: "MNIST数字识别问题"
subtitle: "TensorFlow：实战Google深度学习框架「05」"
layout: post
author: "echisenyang"
header-style: text
hidden: true
catalog: true
tags:
  - TensorFlow
---



## MNIST数字识别问题

在神经网络的结构上，深度学习**一方面需要使用激活函数实现神经网络模型的去线性化**，**另一方面需要使用一个或多个隐藏层使得神经网络的结构更深，以解决复杂问题**。在训练神经网络时，第4节介绍了使用带指数衰减的学习率设置、使用正则化来避免过度拟合，以及使用滑动平均模型来使得最终模型更加健壮。

- mnist最佳实践
  - mnist_inference.py
  - mnist_train.py
  - mnist_eval.py

这个样例将神经网络的**训练、测试和使用拆分**成了不同的程序，并且将神经网络的**前向传播过程抽象**成了 一个独立的库函数。通过这种方式可以将**训练过程和测试、使用过程解耦合**，从而使得整个流程更加灵活。

```python
# mnist_inference.py
import tensorflow as tf

INPUT_NODE = 784
OUTPUT_NODE = 10
LAYER1_NODE = 500 # 隐藏层节点数。 这里使用只有一个隐藏层的网络结构作为样例 
				  # 这个隐藏层有 500 个节点。

def get_weight_variable(shape, regularizer):
    weights = tf.get_variable("weights", shape, initializer=tf.truncated_normal_initializer(stddev=0.1))
    if regularizer != None: tf.add_to_collection('losses', regularizer(weights))
    return weights

# 一个辅助函数，给定神经网络的输入和所有参数，计算神经网络的前向传播结果。在这里定义了一个使用 ReLU 激活函数的三层全连接神经网络。通过加入隐藏层实现了多层网络结构，通过 ReLU 激活函数实现了去线性化。在这个函数中也支持传入用于计算参数平均值的类，这样方便在测试时使用滑动平均模型。
def inference(input_tensor, regularizer):
    
    with tf.variable_scope('layer1'):
        weights = get_weight_variable([INPUT_NODE, LAYER1_NODE], regularizer)
        biases = tf.get_variable("biases", [LAYER1_NODE], initializer=tf.constant_initializer(0.0))
        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)

    with tf.variable_scope('layer2'):
        weights = get_weight_variable([LAYER1_NODE, OUTPUT_NODE], regularizer)
        biases = tf.get_variable("biases", [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))
        layer2 = tf.matmul(layer1, weights) + biases

    return layer2
```

```python
# mnist_train.py
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
import mnist_inference
import os

BATCH_SIZE = 100 # 一个训练batch中的训练数据个数。数字越小时，训练过程越接近随机梯度下降
				 # 数字越大时，训练越接近梯度下降。
LEARNING_RATE_BASE = 0.8
LEARNING_RATE_DECAY = 0.99
REGULARIZATION_RATE = 0.0001
TRAINING_STEPS = 30000 # 训练轮数。
MOVING_AVERAGE_DECAY = 0.99
MODEL_SAVE_PATH="MNIST_model/"
MODEL_NAME="mnist_model"


def train(mnist):

    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name='x-input')
    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name='y-input')

    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)
    y = mnist_inference.inference(x, regularizer)
    # 定义在储训练轮数的变旺。这个变盘不需要计算滑动平均值，所以这里指定这个变盘为不可训练的变盘 Ctrainable=Fasle）。在使用TensorFlow训练神经网络时，－般会将代友训练轮数的变量指定为不可训练的参数。
    global_step = tf.Variable(0, trainable=False) 

    # 给定消动平均哀减率和训练轮数的变量，初始化滑动平均类。
    # 给定训练轮数的变量可以加快训练早期变量的更新速度。
    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)
    # 在所有代友神经网络参数的变量上使用滑动平均。其他辅助变量（比如 global_step 就不需要了。tf.trainable_variables返回的就是图上集合GraphKeys.TRAINABLE_VARIABLES中的元索。这个集合的元索就是所有没有指定trainable=False的参数。
    variables_averages_op = variable_averages.apply(tf.trainable_variables())
    
    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))
    # 计算在当前batch中所有样例的交叉熵平均值。
    cross_entropy_mean = tf.reduce_mean(cross_entropy)
    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))
    learning_rate = tf.train.exponential_decay(
        LEARNING_RATE_BASE,
        global_step,
        mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY,
        staircase=True)
    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)
    # 在训练神经网络模型时，每过一遍数据既需要通过反向传播来更新神经网络中的参数，又要更新每一个参数的滑动平均值。为了一次完成多个操作，TensorFlow提供了tf.control_dependencies和tf.group两种机制。
    with tf.control_dependencies([train_step, variables_averages_op]):
        train_op = tf.no_op(name='train')


    saver = tf.train.Saver()
    with tf.Session() as sess:
        tf.global_variables_initializer().run()

        for i in range(TRAINING_STEPS):
            xs, ys = mnist.train.next_batch(BATCH_SIZE)
            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})
            if i % 1000 == 0:
                print("After %d training step(s), loss on training batch is %g." % (step, loss_value))
                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)


def main(argv=None):
    mnist = input_data.read_data_sets("../../../datasets/MNIST_data", one_hot=True)
    train(mnist)

if __name__ == '__main__':
    tf.app.run()


```

```python
# mnist_eval.py
import time
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
import mnist_inference
import mnist_train

# 加载的时间间隔。
EVAL_INTERVAL_SECS = 10

def evaluate(mnist):
    with tf.Graph().as_default() as g:
        x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name='x-input')
        y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name='y-input')
        validate_feed = {x: mnist.validation.images, y_: mnist.validation.labels}

        y = mnist_inference.inference(x, None)
        # 检验使用了滑动平均模型的神经网络前向传播结果是否正确。tf.argmax(average_y,1)计算每一个样例的预测答案。其中average_y是一个batch_ size*10的二维数组，每一行表示一个样例的前向传播结果。tf.argmax 的第二个参数”l“表示选取最大值的操作仅在第一个维度中进行，也就是说，只在每一行选取最大值对应的下标。于是得到的结果是一个长度为batch的一维数组，这个一维数组中的值就表示了每一个样例对应的数字识别结果。tf.equal 判断两个张量的每一维是否相等，如果相等返回True，否则返回False。
        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
        # 这个运算首先将一个布尔型的数值转换为实数型，然后计算平均值。这个平均值就是模型在这一组数据上的正确率。
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

        variable_averages = tf.train.ExponentialMovingAverage(mnist_train.MOVING_AVERAGE_DECAY)
        variables_to_restore = variable_averages.variables_to_restore()
        saver = tf.train.Saver(variables_to_restore)

        while True:
            with tf.Session() as sess:
                ckpt = tf.train.get_checkpoint_state(mnist_train.MODEL_SAVE_PATH)
                if ckpt and ckpt.model_checkpoint_path:
                    saver.restore(sess, ckpt.model_checkpoint_path)
                    global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]
                    accuracy_score = sess.run(accuracy, feed_dict=validate_feed)
                    print("After %s training step(s), validation accuracy = %g" % (global_step, accuracy_score))
                else:
                    print('No checkpoint file found')
                    return
            time.sleep(EVAL_INTERVAL_SECS)
            
def main(argv=None):
    mnist = input_data.read_data_sets("../../../datasets/MNIST_data", one_hot=True)
    evaluate(mnist)

if __name__ == '__main__':
    main()
```

- 使用验证数据集判断模型效果
  - 虽然一个神经网络模型的效果最终是通过测试数据来评判的，但是我们不能直接通过模型在测试数据上的效果来选择参数。使用测试数据来选取参数可能会导致神经网络模型过度拟合测试数据，从而失去对未知数据的预判能力。因为一个神经网络模型的最终目标是对未知数据提供判断，所以为了估计模型在未知数据上的效果，**需要保证测试数据在训练过程中是不可见的**。只有这样才能保证通过测试数据评估出来的效果和在真实应用场景下模型对未知数据预判的效果是接近的。于是，为了评测神经网络模型在不同参数下的效果，**一般会从训练数据中抽取一部分作为验证数据**。使用验证数据就可以评判不同参数取值下模型的表现。除了使用验证数据集，还可以采用交叉验证（cross_validation）的方式来验证模型效果。**但因为神经网络训练时间本身就比较长，采用cross_validation会花费大量时间。所以在海量数据的情况下，一般会更多地采用验证数据集的形式来评测模型的效果**。
  - 不同问题的数据分布不一样，**如果验证数据分布不能很好地代表测试数据分布，那么模型在这两个数据集上的表现就有可能不一样**。所以，验证数据的选取方法是非常重要的，一般来说**选取的验证数据分布越接近测试数据分布，模型在验证数据上的表现越可以体现模型在测试数据上的表现**。

![vVR7fL](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/vVR7fL.png)

### 变量管理

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/A4ucik.png" alt="A4ucik" style="zoom:33%;" />

`tf.get_variable`函数与`tf.Variable`函数最大的区别在于指定变量名称的参数。对于tf.Variable函数变量名称是一个可选的参数，通过name＝“v”的形式给出。**但是对于tf.get_variable函数，变量名称是一个必填的参数。tf.get_variable会根据这个名字去创建或者获取变量**。在以上样例程序中，tf.get_variable首先会试图去创建一个名字为v的参数，如果创建失败（比如已经有同名的参数），那么这个程序就会报错。这是为了避免无意识的变量复用造成的错误。

- 在上下文管理器“foo”中创建变量“v”

```python
# 在名字为foo的命名空间里创建名为v的变量 
with tf.variable_scope("foo"):
    v = tf.get_variable("v", [1], initializer=tf.constant_initializer(1.0))
    
# 因为在命名空间里已经存在名字为v的变置，所以以下代码将会报错：Variable foo/v already exists, disallowed. Did you mean to set reuse=True in VarScope?
#with tf.variable_scope("foo"):
   # v = tf.get_variable("v", [1])
    
# 在生成上下文管理器时, 将参数reuse设置为True, 这样tf.get_variable函数将直接获取已经声明的变量。
with tf.variable_scope("foo", reuse=True):
    v1 = tf.get_variable("v", [1])
print v == v1
>>> True

# 将参敖reuse设置为True时，tf.variable_scope将只能获取巳经创建过的变量。因为在命名空间bar中还没有创建变量v，所以以下代码将会报错：Variable bar/v does not exist, disallowed. Did you mean to set reuse=True in VarScope?

#with tf.variable_scope("bar", reuse=True):
   # v = tf.get_variable("v", [1])
```

- 嵌套上下文管理器中的reuse参数的使用

```python
with tf.variable_scope("root"):
    # 可以通过tf.get_variable_scope().reuse函数来获取当前上下文管理器中reuse参数的取值。
    print tf.get_variable_scope().reuse 
    >>> False # 输出False，即最外层reuse是False
    
    # 新建一个嵌套的上下文管理器，并指定reuse为True。
    with tf.variable_scope("foo", reuse=True):
        print tf.get_variable_scope().reuse
        >>> True # 输出 True
        
        # 新建一个嵌套的上下文管理器但不指定reuse，这时reuse的取值会和外面一层保持一致。
        with tf.variable_scope("bar"):
            print tf.get_variable_scope().reuse
            >>>True  # 输出 True
            
    print tf.get_variable_scope().reuse
    >>> False # 输出False。退出reuse设置为True的上下文管理器之后，reuse的值又问到了False
```

- 通过variable_scope来管理变量

```python
v1 = tf.get_variable("v", [1])
print v1.name
>>> v:0 # “v”为变量的名称，“:0”表示这个变量是生成变量这个运算的第一个结果。

with tf.variable_scope("foo",reuse=True):
    v2 = tf.get_variable("v", [1])
print v2.name
>>> foo/v:0 # 在tf.variable_scope中创建的变量，名称前面会加入命名空间的名称，并通过／来分隔命名空间的名称和变量的名称。

with tf.variable_scope("foo"):
    with tf.variable_scope("bar"):
        v3 = tf.get_variable("v", [1])
        print v3.name
        >>> foo/bar/v:0 # 命名空间可以嵌套， 同时变量的名称也会加入所有命名空间的名称作为前缀。
        
v4 = tf.get_variable("v1", [1])
print v4.name
>>> v1:0 # 当命名空间退出之后，变量名称也就不会再被加入其前缀了。
```

### TensorFlow模型持久化

- TensorFlow提供了一个非常简单的API来保存和还原一个神经网络模型。这个API就是tf.train.Saver类

```python
v1 = tf.Variable(tf.random_normal([1], stddev=1, seed=1))
v2 = tf.Variable(tf.random_normal([1], stddev=1, seed=1))
result = v1 + v2

init_op = tf.global_variables_initializer()
saver = tf.train.Saver()

# 保存计算两个变量和的模型
with tf.Session() as sess:
    sess.run(init_op)
    saver.save(sess, "Saved_model/model.ckpt")
    
>>> 
model.ckpt.meta # 它保存了TensorFlow计算图的结构
model.ckpt # 保存了TensorFlow程序中每一个变量的取值。
checkpoint # 保存了一个目录下所有的模型文件列表
```

以上代码实现了持久化一个简单的TensorFlow模型的功能。在这段代码中，通过`saver.save`函数将TensorFlow模型保存到了/path/to/model/model.ckpt文件中。TensorFlow模型一般会存在后缀为`.ckpt`的文件中。虽然以上程序只指定了一个文件路径，但是在**这个文件目录下会出现三个文件。这是因为TensorFlow会将计算图的结构和图上参数取值分开保存。**

```python
v1 = tf.Variable(tf.random_normal([1], stddev=1, seed=1))
v2 = tf.Variable(tf.random_normal([1], stddev=1, seed=1))
result = v1 + v2

init_op = tf.global_variables_initializer()
saver = tf.train.Saver()
# 加载保存了两个变量和的模型
with tf.Session() as sess:
    saver.restore(sess, "Saved_model/model.ckpt")
    print sess.run(result)
>>> INFO:tensorflow:Restoring parameters from Saved_model/model.ckpt
[-1.62263644]
```

**这段加载模型的代码基本上和保存模型的代码是一样的**。在加载模型的程序中也是**先定义了TensorFlow计算图上的所有运算**，并声明了一个`tf.train.Saver`类。两段代码唯一不同的是，在加载模型的代码中没有运行变量的初始化过程，而是将变量的值通过己经保存的模型加载进来。**如果不希望重复定义图上的运算，也可以直接加载己经持久化的图。**

```python
# 直接加载持久化的图。因为之前没有导出v3，所以这里会报错
saver = tf.train.import_meta_graph("Saved_model/model.ckpt.meta")
v3 = tf.Variable(tf.random_normal([1], stddev=1, seed=1))

with tf.Session() as sess:
    saver.restore(sess, "Saved_model/model.ckpt")
    print sess.run(v1) 
    print sess.run(v2) 
    print sess.run(v3) 
>>> INFO:tensorflow:Restoring parameters from Saved_model/model.ckpt
[-0.81131822]
[-0.81131822]
FailedPreconditionError: Attempting to use uninitialized value Variable_3
	 [[Node: _retval_Variable_3_0_0 = _Retval[T=DT_FLOAT, index=0, _device="/job:localhost/replica:0/task:0/device:CPU:0"](Variable_3)]]
```

在这个程序中，对变量v1和v2的名称进行了修改。如果直接通过`tf.train.Saver`默认的构造函数来加载保存的模型，那么程序会报变量找不到的错误。因为保存时候变量的名称和加载时变量的名称不一致。为了解决这个问题，**TensorFlow可以通过字典（dictionary)将模型保存时的变量名和需要加载的变量联系起来**。

```python
# 变量重命名
v1 = tf.Variable(tf.constant(1.0, shape=[1]), name = "other-v1")
v2 = tf.Variable(tf.constant(2.0, shape=[1]), name = "other-v2")
saver = tf.train.Saver({"v1": v1, "v2": v2})
```

- 这样做主要目的之一是方便使用变量的滑动平均值

```python
# 1.使用滑动平均
v = tf.Variable(0, dtype=tf.float32, name="v")
for variables in tf.global_variables(): print variables.name
>>> v:0 # 在没有申明滑动平均模型时只有一 个变量v，所以以下语句只会 输出“v:0”
    
ema = tf.train.ExponentialMovingAverage(0.99)
maintain_averages_op = ema.apply(tf.global_variables())
for variables in tf.global_variables(): print variables.name
>>> v:0
v/ExponentialMovingAverage:0 
# 在申明滑动平均模型之后，TensorFlow会自动生成一个影子变量 v/Exponential Moving Average
    
# 2.保存滑动平均模型
saver = tf.train.Saver()
with tf.Session() as sess:
    init_op = tf.global_variables_initializer()
    sess.run(init_op)
    
    sess.run(tf.assign(v, 10))
    sess.run(maintain_averages_op)
    # 保存的时候会将v:0  v/ExponentialMovingAverage:0这两个变量都存下来。
    saver.save(sess, "Saved_model/model2.ckpt")
    print sess.run([v, ema.average(v)])
	>>> [10.0, 0.099999905]
    
# 3.加载滑动平均模型
v = tf.Variable(0, dtype=tf.float32, name="v")

# 通过变量重命名将原来变量v的滑动平均值直接赋值给v。
saver = tf.train.Saver({"v/ExponentialMovingAverage": v})
with tf.Session() as sess:
    saver.restore(sess, "Saved_model/model2.ckpt")
    print sess.run(v)
  	>>> INFO:tensorflow:Restoring parameters from Saved_model/model2.ckpt
		0.099999905 # 这个值就是原来模型中变量v的滑动平均值。
```

- pb文件的保存方法

使用`tf.train.Saver`会保存运行TensorFlow程序所需要的全部信息，然而**有时并不需要某些信息**。比如在测试或者离线预测时，只需要知道如何从神经网络的输入层经过前向传播计算得到输出层即可，而**不需要类似于变量初始化、模型保存等辅助节点的信息**。在第6章介绍迁移学习时，会遇到类似的情况。而且，将变量取值和计算图结构分成不同的文件存储有时候也不方便，于是TensorFlow提供了`convert_variables_to_constants`函数，通过这个函数**可以将计算图中的变量及其取值通过常量的方式保存**，这样整个TensorFlow计算图可以统一存放在一个文件中。以下程序提供了一个样例。

```python
# pb文件的保存方法
import tensorflow as tf
from tensorflow.python.framework import graph_util

v1 = tf.Variable(tf.constant(1.0, shape=[1]), name = "v1")
v2 = tf.Variable(tf.constant(2.0, shape=[1]), name = "v2")
result = v1 + v2

init_op = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init_op)
    # 导出当前计算图的GraphDef部分，只需要这一部分就可以完成从输入层到输出层的计算过程。
    graph_def = tf.get_default_graph().as_graph_def()
    # 将圈中的变量及其取值转化为常量，同时将图中不必要的节点去掉。在5.4.2节中将会看到一些系统运算也会被转化为计算图中的节点（比如变量初始化操作）。如果只关心程序中定义的某些计算时，和这些计算无关的节点就没有必要导出并保存了。在下面一行代码中， 最后一个参数［＇add＇］给出了需要保存的节点名称。add节点是上面定义的两个变量相加的操作。注意这里给出的是计算节点的名称，所以没有后面的:0。
    output_graph_def = graph_util.convert_variables_to_constants(sess, graph_def, ['add'])
    with tf.gfile.GFile("Saved_model/combined_model.pb", "wb") as f:
           f.write(output_graph_def.SerializeToString())
>>> INFO:tensorflow:Froze 2 variables.
Converted 2 variables to const ops.
```

```python
from tensorflow.python.platform import gfile
# 加载pb文件
with tf.Session() as sess:
    model_filename = "Saved_model/combined_model.pb"
   
    with gfile.FastGFile(model_filename, 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())

    result = tf.import_graph_def(graph_def, return_elements=["add:0"])
    print sess.run(result)
```

### 持久化原理及数据格式

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/wEG08p.png" alt="wEG08p" style="zoom:33%;" />

```python
v1 = tf.Variable(tf.random_normal([1], stddev=1, seed=1))
v2 = tf.Variable(tf.random_normal([1], stddev=1, seed=1))
result = v1 + v2

saver = tf.train.Saver()
# 加载保存了两个变量和的模型
with tf.Session() as sess:
    saver.export_meta_graph("model.ckpt.meda.json",as_text=True)
```

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/0DaEdv.png" alt="0DaEdv" style="zoom:33%;" />

- meta_info_def属性

meta_info_def属性是通过MetalnfoDef定义的，它记录了TensorFlow计算图中的元数据以及TensorFlow程序中所有使用到的运算方法的信息。

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/WK8B42.png" alt="WK8B42" style="zoom:33%;" />

- 其他信息

```json
meta_info_def {
  stripped_op_list {
    op {
      name: "Add"
      input_arg {
        name: "x"
        type_attr: "T"
      }
      ...
  }
  tensorflow_version: "1.8.0"
  tensorflow_git_version: "v1.8.0-0-g93bc2e2072"
}

graph_def {
  node {
    name: "random_normal/shape"
    op: "Const"
    attr {
      key: "_output_shapes"
      value {
        list {
          shape {
            dim {
              size: 1
            }
          }
        }
      }
    }
	...
  versions {
    producer: 26
  }
}

saver_def {
  filename_tensor_name: "save/Const:0"
  save_tensor_name: "save/control_dependency:0"
  restore_op_name: "save/restore_all"
  max_to_keep: 5
  keep_checkpoint_every_n_hours: 10000.0
  version: V2
}

collection_def {
  key: "trainable_variables"
  value {
    bytes_list {
      value: "\n\nVariable:0\022\017Variable/Assign\032\017Variable/read:02\017random_normal:0"
      value: "\n\014Variable_1:0\022\021Variable_1/Assign\032\021Variable_1/read:02\021random_normal_1:0"
    }
  }
}

collection_def {
  key: "variables"
  value {
    bytes_list {
      value: "\n\nVariable:0\022\017Variable/Assign\032\017Variable/read:02\017random_normal:0"
      value: "\n\014Variable_1:0\022\021Variable_1/Assign\032\021Variable_1/read:02\021random_normal_1:0"
    }
  }
}

```

