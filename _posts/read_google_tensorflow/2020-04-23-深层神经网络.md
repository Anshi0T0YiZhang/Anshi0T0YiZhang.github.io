---
title: "深层神经网络"
subtitle: "TensorFlow：实战Google深度学习框架「04」"
layout: post
author: "echisenyang"
header-style: text
hidden: false
catalog: true
tags:
  - TensorFlow
---



## 深层神经网络

### 深度学习与深层神经网络

维基百科对深度学习的精确定义为“一类通过**多层非线性**变换对高复杂性数据建模算法的合集”。

- 线性模型的局限性

**只通过线性变换，任意层的全连接神经网络和单层神经网络模型的表达能力没有任何区别**，而且它们都是线性模型。然而*线性模型能够解决的问题是有限的，这就是线性模型最大的局限性*，也是为什么深度学习要强调非线性。

![lGPf8D](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/lGPf8D.png)

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/z2BfIi.png" alt="z2BfIi" style="zoom: 33%;" />

![un37Ie](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/un37Ie.png)

- 激活函数实现去线性化

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/pssyWB.png" alt="pssyWB" style="zoom:33%;" />

每个节点的取值不再是单纯的加权和。每个节点的输出在加权和的基础上还做了一个非线性变换激活函数。通过这些激活函数，**每一个节点不再是线性变换**，于是**整个神经网络模型也就不再是线性的**了。

- 多层网络解决异或运算

将隐藏层的层数设置为0，这样就**模拟了感知机的模型**。通过500轮训练之后，可以看到这个感知机模型并不能将两种不同颜色的点分开，也就是说**感知机无法模拟异或运算的功能**。

![ppEDcM](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ppEDcM.png)

**当加入隐藏层之后，异或问题就可以得到很好地解决**。图4-9显示了一个有4个节点隐藏层的神经网络在训练500轮之后的效果。在图4-9中，除了可以看到最右边的输出节点可以很好地区分不同颜色的点，更加有意思的是，隐藏层的4个节点中，每个节点都有一个角是黑色的。**这4个隐藏节点可以被认为代表了从输入特征中抽取的更高维的特征**。比如第一个节点可以大致代表两个输入的逻辑与操作的结果（当两个输入都为正数时该节点输出为正数〉。从这个例子中可以看到，**深层神经网络实际上有组合特征提取的功能**。***这个特性对于解决不易提取特征向量的问题（比如图片识别、语音识别等）有很大帮助。这也是深度学习在这些问题上更加容易取得突破性进展的原因。***

![V0ZDf9](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/V0ZDf9.png)



### 损失函数定义

- 经典损失函数 (Softmax+cross entropy)

分类问题和回归问题是监督学习的两大种类。这一节将分别介绍分类问题和回归问题中使用到的经典损失函数。**分类问题希望解决的是将不同的样本分到事先定义好的类别中**。

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/eAp2ls.png" alt="eAp2ls" style="zoom:33%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/kpwmsn.png" alt="kpwmsn" style="zoom:33%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/WjcIb0.png" alt="WjcIb0" style="zoom:33%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/S8snYd.png" alt="S8snYd" style="zoom:33%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/fzbKxZ.png" alt="fzbKxZ" style="zoom:33%;" />

- 自定义损失函数

```python
# 定义损失函数使得预测少了的损失大，于是模型应该偏向多的方向预测。
loss_less = 10
loss_more = 1
loss = tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_) * loss_more, (y_ - y) * loss_less))
train_step = tf.train.AdamOptimizer(0.001).minimize(loss)

rdm = RandomState(1)
X = rdm.rand(128,2)
Y = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]

with tf.Session() as sess:
    init_op = tf.global_variables_initializer()
    sess.run(init_op)
    STEPS = 5000
    for i in range(STEPS):
        start = (i*batch_size) % 128
        end = (i*batch_size) % 128 + batch_size
        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})
        if i % 1000 == 0:
            print("After %d training step(s), w1 is: " % (i))
            print sess.run(w1), "\n"
    print "Final w1 is: \n", sess.run(w1)
    
>>> Final w1 is: 
[[ 1.01934695]
 [ 1.04280889]]

# 重新定义损失函数，使得预测多了的损失大，于是模型应该偏向少的方向预测。
loss_less = 1
loss_more = 10
>>> Final w1 is: 
[[ 0.95525807]
 [ 0.9813394 ]]

# 定义损失函数为MSE
>>> Final w1 is: 
[[ 0.97437561]
 [ 1.0243336 ]]
```

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/9DTOgl.png" alt="9DTOgl" style="zoom:33%;" />

 

### 神经网络优化算法

**梯度下降算法主要用于优化单个参数的取值**，而**反向传播算法给出了一个高效的方式**在**所有参数上使用梯度下降算法，从而使神经网络模型在训练数据上的损失函数尽可能小**。反向传播算法是训练神经网络的核心算法，它可以根据定义好的损失函数优化神经网络中参数的取值，从而使神经网络模型在训练数据集上的损失函数达到一个较小值。**神经网络模型中参数的优化过程直接决定了模型的质量**，是使用神经网络时非常重要的一步。

- **batch_sgd 的优点**

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/pt3JGy.png" alt="pt3JGy" style="zoom:33%;" />

### 神经网络进一步优化

- 学习率的设置

```python
# 1. 学习率为1的时候，x在5和-5之间震荡。
import tensorflow as tf

TRAINING_STEPS = 10
LEARNING_RATE = 1
x = tf.Variable(tf.constant(5, dtype=tf.float32), name="x")
y = tf.square(x)

train_op = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(y)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(TRAINING_STEPS):
        sess.run(train_op)
        x_value = sess.run(x)
        print "After %s iteration(s): x%s is %f."% (i+1, i+1, x_value) 
        
>>> After 1 iteration(s): x1 is -5.000000.
After 2 iteration(s): x2 is 5.000000.
After 3 iteration(s): x3 is -5.000000.
After 4 iteration(s): x4 is 5.000000.
After 5 iteration(s): x5 is -5.000000.
After 6 iteration(s): x6 is 5.000000.
After 7 iteration(s): x7 is -5.000000.
After 8 iteration(s): x8 is 5.000000.
After 9 iteration(s): x9 is -5.000000.
After 10 iteration(s): x10 is 5.000000.
    
# 2. 学习率为0.001的时候，下降速度过慢，在901轮时才收敛到0.823355。
TRAINING_STEPS = 1000
LEARNING_RATE = 0.001

>>> After 1 iteration(s): x1 is 4.990000.
After 101 iteration(s): x101 is 4.084646.
After 201 iteration(s): x201 is 3.343555.
After 301 iteration(s): x301 is 2.736923.
After 401 iteration(s): x401 is 2.240355.
After 501 iteration(s): x501 is 1.833880.
After 601 iteration(s): x601 is 1.501153.
After 701 iteration(s): x701 is 1.228794.
After 801 iteration(s): x801 is 1.005850.
After 901 iteration(s): x901 is 0.823355.
    
# 3. 使用指数衰减的学习率，在迭代初期得到较高的下降速度，可以在较小的训练轮数下取得不错的收敛程度
TRAINING_STEPS = 100
global_step = tf.Variable(0)
LEARNING_RATE = tf.train.exponential_decay(0.1, global_step, 1, 0.96, staircase=True)
>>> After 1 iteration(s): x1 is 4.000000, learning rate is 0.096000.
After 11 iteration(s): x11 is 0.690561, learning rate is 0.063824.
After 21 iteration(s): x21 is 0.222583, learning rate is 0.042432.
After 31 iteration(s): x31 is 0.106405, learning rate is 0.028210.
After 41 iteration(s): x41 is 0.065548, learning rate is 0.018755.
After 51 iteration(s): x51 is 0.047625, learning rate is 0.012469.
After 61 iteration(s): x61 is 0.038558, learning rate is 0.008290.
After 71 iteration(s): x71 is 0.033523, learning rate is 0.005511.
After 81 iteration(s): x81 is 0.030553, learning rate is 0.003664.
After 91 iteration(s): x91 is 0.028727, learning rate is 0.002436.
```

- 过拟合问题

然而在真实的应用中想要的**并不是让模型尽量模拟训练数据的行为**，而是**希望通过训练出来的模型对未知的数据给出判断**。***模型在训练数据上的表现并不一定代表了它在未知数据上的表现***。本节将介绍的过拟合问题就是可以导致这个差距的一个很重要因素。所谓**过拟合**，指的是**当一个模型过为复杂之后**，它**可以很好地“记忆”每一个训练数据中随机噪音的部分而忘记了要去“学习”训练数据中通用的趋势**。

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Wtqa79.png" alt="Wtqa79" style="zoom:50%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/8K816Q.png" alt="8K816Q" style="zoom: 33%;" />

```python
def get_weight(shape, lambda1):
    var = tf.Variable(tf.random_normal(shape), dtype=tf.float32)
    tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(lambda1)(var))
    return var
```

训练不带正则项的损失函数mse_loss

![RW64rf](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/RW64rf.jpg)

- 滑动平均模型

在采用随机梯度下降算法训练神经网络时，使用滑动平均模型在很多应用中都可以在一定程度提高最终模型在测试数据上的表现。

```python
import tensorflow as tf

v1 = tf.Variable(0, dtype=tf.float32)
step = tf.Variable(0, trainable=False)
ema = tf.train.ExponentialMovingAverage(0.99, step)
maintain_averages_op = ema.apply([v1]) 

with tf.Session() as sess:
    
    # 初始化
    init_op = tf.global_variables_initializer()
    sess.run(init_op)
    print sess.run([v1, ema.average(v1)])
    
    # 更新变量v1的取值
    sess.run(tf.assign(v1, 5))
    sess.run(maintain_averages_op)
    print sess.run([v1, ema.average(v1)]) 
    
    # 更新step和v1的取值
    sess.run(tf.assign(step, 10000))  
    sess.run(tf.assign(v1, 10))
    sess.run(maintain_averages_op)
    print sess.run([v1, ema.average(v1)])       
    
    # 更新一次v1的滑动平均值
    sess.run(maintain_averages_op)
    print sess.run([v1, ema.average(v1)])  
    
>>> [0.0, 0.0]
[5.0, 4.5]
[10.0, 4.5549998]
[10.0, 4.6094499]
```

