---
title: "nmt工程「basic」"
subtitle: "nmt"
layout: post
author: "echisenyang"
header-style: text
hidden: true
catalog: true
tags:
  - nmt
---



## 复现 Neural Machine Translation (seq2seq)

- reference

  NMT https://github.com/tensorflow/nmt/tree/tf-1.2

  GNMT https://github.com/google/sentencepiece

  OpenNMT https://github.com/OpenNMT/OpenNMT-tf

- 目标：

  1.  first build up some basic knowledge about seq2seq models for NMT.
  2. go into details of building a competitive NMT model with attention mechanism.
  3. then discuss tips and tricks to build the best possible NMT models (both in speed and translation quality) such as TensorFlow best practices (batching, bucketing), bidirectional RNNs, beam search, as well as scaling up to multiple GPUs using GNMT attention.


## Background on Neural Machine Translation

- **encoder-decoder architecture**

![015hPr](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/015hPr.jpg)

Specifically, an NMT system first reads the source sentence using an *encoder* to build a ["thought" vector](https://www.theguardian.com/science/2015/may/21/google-a-step-closer-to-developing-machines-with-human-like-intelligence), a sequence of numbers that represents the sentence meaning; a *decoder*, then, processes the sentence vector to emit a translation, as illustrated in Figure 1. This is often referred to as the *encoder-decoder architecture*. 

- **exact architectures: recurrent neural network (RNN)**

  NMT models vary in terms of their exact architectures. A natural choice for sequential data is the recurrent neural network (RNN), used by most NMT models. Usually an RNN is used for both the encoder and decoder. The RNN models, however, differ in terms of: 

  - (a) *directionality* – unidirectional or bidirectional;
  - (b) *depth* – single- or multi-layer; and 
  - (c) *type* – often either a vanilla RNN, a Long Short-term Memory (LSTM), or a gated recurrent unit (GRU). RNNs and LSTM on this [blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).

- implementation **refers** to [Luong (2016)](https://github.com/lmthang/thesis)
  - a *deep multi-layer RNN* which is unidirectional and uses LSTM as a recurrent unit.
  - **example** of a deep recurrent architecture for translating a source sentence "I am a student" into a target sentence "Je suis étudiant". Here, $\text{<s>}$ marks the start of the decoding process while  $\text{</s>}$tells the decoder to stop. 

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/wp14RZ.jpg" alt="wp14RZ" style="zoom: 50%;" />

---

## How to build our first NMT system

- **training data prepare**: *source sentence + boundary marker $\text{<s>}$ + target sentence*

  At the bottom layer, the encoder and decoder RNNs receive as input the following: first, the source sentence, then a boundary marker $\text{<s>}$ which indicates the transition from the encoding to the decoding mode, and the target sentence. For *training*, we will feed the system with the following tensors, which are **in time-major format** and contain word indices:

1. **encoder_inputs** [max_encoder_time, batch_size]: source input words.
2. **decoder_inputs** [max_decoder_time, batch_size]: target input words.
3. **decoder_outputs** [max_decoder_time, batch_size]: target output words, these are decoder_inputs shifted to the left by one time step with an end-of-sentence tag appended on the right.

> This part refers to file [**model.py**](https://github.com/tensorflow/nmt/blob/tf-1.2/nmt/model.py) and **[helper.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/helper.py)**



- **Embedding**: *look up vocabulary for word representation(embedding)*

  Given the categorical nature of words, the model must first **look up the source and target embeddings to retrieve the corresponding word representations**. For this *embedding layer* to work, a **vocabulary** is first chosen for each language. Usually, a vocabulary size V is selected, and **only the most frequent V words are treated as unique**. All other words are converted to an "**unknown**" token and **all get the same embedding**. The embedding weights, one set per language, are usually learned during training.

```python
# Embedding
embedding_encoder = variable_scope.get_variable(
    "embedding_encoder", [src_vocab_size, embedding_size], ...)
# Look up embedding:
#   encoder_inputs: [max_time, batch_size]
#   encoder_emb_inp: [max_time, batch_size, embedding_size]
encoder_emb_inp = embedding_ops.embedding_lookup(
    embedding_encoder, encoder_inputs)
```

​		Similarly, we can build *embedding_decoder* and *decoder_emb_inp*. **Note that one can choose to initialize embedding weights with pretrained word representations such as word2vec or Glove vectors**. In general, given a large amount of training data we can learn these embeddings from scratch.



- **Encoder**: *fed word embeddings into network*

  Once retrieved, the **word embeddings are then fed as input into the main network**, which consists of two multi-layer RNNs – an **encoder** for the **source** language and a **decoder** for the **target** language. These two RNNs, in principle, can share the same weights; however, **in practice, we often use two different RNN parameters (such models do a better job when fitting large training datasets)**. The *encoder* RNN uses zero vectors as its starting states and is built as follows:

```python
# Build RNN cell
encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)

# Run Dynamic RNN
#   encoder_outpus: [max_time, batch_size, num_units]
#   encoder_state: [batch_size, num_units]
encoder_outputs, encoder_state = tf.nn.dynamic_rnn(
    encoder_cell, encoder_emb_inp,
    sequence_length=source_sequence_length, time_major=True)
```

​		Note that sentences have different lengths to avoid wasting computation, we tell *dynamic_rnn* the exact source sentence lengths through *source_sequence_length*. Since our input is time major, we set *time_major=True*. Here, we build only **a single layer LSTM**, *encoder_cell*. 



- **Decoder**: *initialize with the last hidden state of the encoder*

  The *decoder* also needs to have access to the source information, and one simple way to achieve that is to **initialize it with the last hidden state of the encoder**, *encoder_state*. In Figure 2, we pass the hidden state at the source word "student" to the decoder side.

```python
# Build RNN cell
decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)

# Helper
helper = tf.contrib.seq2seq.TrainingHelper(
    decoder_emb_inp, decoder_lengths, time_major=True)
# Decoder
decoder = tf.contrib.seq2seq.BasicDecoder(
    decoder_cell, helper, encoder_state,
    output_layer=projection_layer)
# Dynamic decoding
outputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)
logits = outputs.rnn_output
```

> Here, the core part of this code is the *BasicDecoder* object, *decoder*, which receives *decoder_cell* (similar to encoder_cell), a *helper*, and the previous *encoder_state* as inputs. By separating out decoders and helpers, we can reuse different codebases, e.g., *TrainingHelper* can be substituted with *GreedyEmbeddingHelper* to do greedy decoding. See more in **[helper.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/helper.py)**



- **projection_layer**: *turn the top hidden states to logit vectors of dimension V*

  Lastly, we haven't mentioned *projection_layer* which is a dense matrix to turn the top hidden states to logit vectors of dimension V. We illustrate this process at the top of Figure 2.

```python
projection_layer = layers_core.Dense(
    tgt_vocab_size, use_bias=False)
```



- **Loss**: *sparse_softmax_cross_entropy_with_logits*

  Given the *logits* above, we are now ready to compute our training loss:

```python
crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(
    labels=decoder_outputs, logits=logits)
train_loss = (tf.reduce_sum(crossent * target_weights) /
    batch_size)
```

​		Here, *target_weights* is a zero-one matrix of the same size as *decoder_outputs*. It masks padding positions outside of the target sequence lengths with values 0.

***Important note***: It's worth pointing out that we divide the loss by *batch_size*, so our hyperparameters are "invariant" to batch_size. Some people divide the loss by (*batch_size* * *num_time_steps*), which plays down the errors made on short sentences. More subtly, our hyperparameters (applied to the former way) can't be used for the latter way. For example, if both approaches use SGD with a learning of 1.0, the latter approach effectively uses a much smaller learning rate of 1 / *num_time_steps*.



- **Gradient computation & optimization**: *clip_by_global_norm + AdamOptimizer*

  We have now defined the forward pass of our NMT model. Computing the backpropagation pass is just a matter of a few lines of code:

```python
# Calculate and clip gradients
params = tf.trainable_variables()
gradients = tf.gradients(train_loss, params)
clipped_gradients, _ = tf.clip_by_global_norm(
    gradients, max_gradient_norm)
```

​		One of the important steps in training RNNs is gradient clipping. Here, we clip by the global norm. The max value, *max_gradient_norm*, is often set to a value like 5 or 1. The last step is selecting the optimizer. The Adam optimizer is a common choice. We also select a learning rate. The value of *learning_rate* can is usually in the range 0.0001 to 0.001; and can be set to decrease as training progresses.

```python
# Optimization
optimizer = tf.train.AdamOptimizer(learning_rate)
update_step = optimizer.apply_gradients(
    zip(clipped_gradients, params))
```



## Hands-on – Let's train an NMT model

> This part refers to file [**nmt.py**](https://github.com/tensorflow/nmt/blob/tf-1.2/nmt/nmt.py). See [**train.py**](https://github.com/tensorflow/nmt/blob/tf-1.2/nmt/train.py) for more details.

We will use a *small-scale parallel corpus of TED talks* (133K training examples) for this exercise. All data we used here can be found at: https://nlp.stanford.edu/projects/nmt/. We will use tst2012 as our dev dataset, and tst2013 as our test dataset.

Run the following command to download the data for training NMT model:

```python
nmt/scripts/download_iwslt15.sh ~/tmp/nmt_data
```

![G0byaw](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/G0byaw.png)

Run the following command to start the training:

```python
mkdir /home/jiale/tmp/nmt_model
```

```python
CUDA_VISIBLE_DEVICES=0 python -m nmt.nmt \
    --src=vi --tgt=en \
    --vocab_prefix=/home/jiale/tmp/nmt_data/vocab  \
    --train_prefix=/home/jiale/tmp/nmt_data/train \
    --dev_prefix=/home/jiale/tmp/nmt_data/tst2012  \
    --test_prefix=/home/jiale/tmp/nmt_data/tst2013 \
    --out_dir=/home/jiale/tmp/nmt_model \
    --num_train_steps=12000 \
    --steps_per_stats=100 \
    --num_layers=2 \
    --num_units=128 \
    --dropout=0.2 \
    --metrics=bleu
```

![mIDzay](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/mIDzay.png)

The above command trains a 2-layer LSTM seq2seq model with 128-dim hidden units and embeddings for 12 epochs. We use a dropout value of 0.2 (keep probability 0.8). If no error, we should see logs similar to the below with decreasing perplexity values as we train.

We can start Tensorboard to view the summary of the model during training:

```python
tensorboard --port 22222 --logdir /home/jiale/tmp/nmt_model/
```

Training the reverse direction from English and Vietnamese can be done simply by changing:

```python
--src=en --tgt=vi
```

## Inference – How to generate translations

> Note the above commands can also be run while the model is still being trained as long as there exists a training checkpoint. See [**inference.py**](https://github.com/tensorflow/nmt/blob/tf-1.2/nmt/inference.py) for more details.

While you're training your NMT models (and once you have trained models), you can obtain translations given previously unseen source sentences. This process is called inference. There is a clear distinction between training and inference (*testing*): **at inference time, we only have access to the source sentence, i.e., *encoder_inputs*.** There are many ways to perform decoding. Decoding methods include greedy, sampling, and beam-search decoding. **Here, we will discuss the greedy decoding strategy**.

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Exhjfb.jpg" alt="Exhjfb" style="zoom: 67%;" />

1. We still **encode the source sentence in the same way as during training to obtain an *encoder_state***, and this *encoder_state* is used to initialize the decoder.
2. The decoding (translation) process is started as soon as the decoder receives a **starting symbol "\<s\>"**(refer as *tgt_sos_id* in our code);
3. For each timestep on the decoder side, we treat the RNN's output as a set of logits. We choose the most likely word, **the id associated with the maximum logit value**, as the emitted word (this is the "greedy" behavior). For example, the word "moi" has the highest translation probability in the first decoding step. We then feed this word as input to the next timestep.
4. The process continues until the **end-of-sentence marker "\</s\>"** is produced as an output symbol (refer as *tgt_eos_id* in our code).

![0R6deu](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/0R6deu.png)

- **greedy decoding**

  Step 3 is what makes inference different from training. Instead of always feeding the correct target words as an input, **inference uses words predicted by the model**. Here's the code to achieve greedy decoding. It is very similar to the training decoder.

```python
# Helper
helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(
    embedding_decoder,
    tf.fill([batch_size], tgt_sos_id), tgt_eos_id)

# Decoder
decoder = tf.contrib.seq2seq.BasicDecoder(
    decoder_cell, helper, encoder_state,
    output_layer=projection_layer)
# Dynamic decoding
outputs, _ = tf.contrib.seq2seq.dynamic_decode(
    decoder, maximum_iterations=maximum_iterations)
translations = outputs.sample_id
```

​		Here, we use *GreedyEmbeddingHelper* instead of *TrainingHelper*. Since **we do not know the target sequence lengths in advance**, we use *maximum_iterations* to limit the translation lengths. One heuristic is to decode up to two times the source sentence lengths.

```python
maximum_iterations = tf.round(tf.reduce_max(source_sequence_length) * 2)
```

​		Having trained a model, we can now create an inference file and translate some sentences:

```python
cat > /tmp/my_infer_file.vi
# (copy and paste some sentences from /tmp/nmt_data/tst2013.vi)

CUDA_VISIBLE_DEVICES=0 python -m nmt.nmt \
    --out_dir=/home/jiale/tmp/nmt_model \
    --inference_input_file=/home/jiale/tmp/my_infer_file.vi \
    --inference_output_file=/home/jiale/tmp/nmt_model/output_infer

cat /home/jiale/tmp/nmt_model/output_infer # To view the inference as output
```

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/HLJUVJ.png" alt="HLJUVJ" style="zoom: 50%;" />




