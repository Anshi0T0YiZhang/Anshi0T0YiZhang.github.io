---
title: "新闻标题改写初版idea"
subtitle: "nmt"
layout: post
author: "echisenyang"
header-style: text
hidden: true
catalog: true
tags:
  - nmt
---



topics to be checked:

- vocab build from sentences:

  - 分词 level：kd_jiebacut
  - wordpiece level: 以单个字为单位，character-based tokenization for Chinese

  > 分词处理时，要把 \n \t \r 去掉，不然会报 empty line

- sentence source from:

  - already have：业务员对上市公司重要新闻标题手动改写，100 sentences
  - possible plan：对之前用做事件聚类的金融新闻标题进行DBSCAN，代码自动匹配sentence title pair
  - there should be more...

  > 中文的需要在词或者句子之间加空格

- models to be build:

  - baseline:

    ```python
    # Misc
    # network
    num_units=128
    num_layers=2
    encoder_type=uni
    time_major=true
    
    # optimizer
    optimizer=sgd
    num_train_steps=12000
    
    # Vocab
    share_vocab=false
    
    # Sequence lengths
    src_max_len=50
    tgt_max_len=50
    
    # Default settings works well (rarely need to change)
    parser.add_argument("--unit_type", type=str, default="lstm",
                        help="lstm | gru | layer_norm_lstm")
    parser.add_argument("--forget_bias", type=float, default=1.0,
                        help="Forget bias for BasicLSTMCell.")
    parser.add_argument("--dropout", type=float, default=0.2,
                        help="Dropout rate (not keep_prob)")
    parser.add_argument("--max_gradient_norm", type=float, default=5.0,
                        help="Clip gradients to this norm.")
    
    # Misc
    metrics=bleu
    ```

  - advanced:

    1. attention=scaled_luong
    2. attention_architecture=gnmt
    3. encoder_type=bi/gnmt
    4. optimizer=adam

---



```python
mkdir /home/jiale/tmp/nmt_attention_model

--attention=scaled_luong \

CUDA_VISIBLE_DEVICES=0 python -m nmt.nmt \
	--attention=scaled_luong \
    --src=szh --tgt=tzh \
    --vocab_prefix=/home/jiale/tmp/nmt_chinese_100_news_title_data/vocab  \
    --train_prefix=/home/jiale/tmp/nmt_chinese_100_news_title_data/train_st_100 \
    --dev_prefix=/home/jiale/tmp/nmt_chinese_100_news_title_data/val_st_100  \
    --test_prefix=/home/jiale/tmp/nmt_chinese_100_news_title_data/test_st_100 \
    --out_dir=/home/jiale/tmp/nmt_chinese_100_news_title_model \
    --num_train_steps=1200 \
    --steps_per_stats=100 \
    --num_layers=2 \
    --num_units=128 \
    --dropout=0.2 \
    --metrics=bleu
```

```python
CUDA_VISIBLE_DEVICES=0 python -m nmt.nmt \
    --out_dir=/home/jiale/tmp/nmt_chinese_100_news_title_model \
    --inference_input_file=/home/jiale/tmp/my_infer_file_100.szh \
    --inference_output_file=/home/jiale/tmp/nmt_chinese_100_news_title_model/output_infer
    
cat /home/jiale/tmp/nmt_chinese_100_news_title_model/output_infer # To view the inference as output
```

```python
CUDA_VISIBLE_DEVICES=0 python -m nmt.nmt \
	--attention=scaled_luong \
    --src=szh --tgt=tzh \
    --vocab_prefix=/home/jiale/tmp/nmt_zh_65701_data/vocab  \
    --train_prefix=/home/jiale/tmp/nmt_zh_65701_data/train_st_65701 \
    --dev_prefix=/home/jiale/tmp/nmt_zh_65701_data/val_st_65701  \
    --test_prefix=/home/jiale/tmp/nmt_zh_65701_data/test_st_65701 \
    --out_dir=/home/jiale/tmp/nmt_zh_65701_model \
    --num_train_steps=12000 \
    --steps_per_stats=100 \
    --num_layers=2 \
    --num_units=128 \
    --dropout=0.2 \
    --metrics=bleu
```

```python
CUDA_VISIBLE_DEVICES=0 python -m nmt.nmt \
    --out_dir=/home/jiale/tmp/nmt_zh_65701_model \
    --inference_input_file=/home/jiale/tmp/my_infer_file.szh \
    --inference_output_file=/home/jiale/tmp/nmt_zh_65701_model/output_infer

cat /home/jiale/tmp/nmt_zh_65701_model/output_infer
```



