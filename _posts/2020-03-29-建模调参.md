---
layout:     post
title:      建模调参
subtitle:   天池比赛二手车价格预测
date:       2020-03-29
author:     Young
header-img: img/bg-post/1*EbpKczfURCvAF6uUp_Hhew.jpeg
catalog: true
tags:
    - tianchi

---



> 本节内容：
>
> 1. 线性回归模型：
>    - 线性回归对于特征的要求；
>    - 处理长尾分布；
>    - 理解线性回归模型；
> 2. 模型性能验证：
>    - 评价函数与目标函数；
>    - 交叉验证方法；
>    - 留一验证方法；
>    - 针对时间序列问题的验证；
>    - 绘制学习率曲线；
>    - 绘制验证曲线；
> 3. 嵌入式特征选择：
>    - Lasso回归；
>    - Ridge回归；
>    - 决策树；
> 4. 模型对比：
>    - 常用线性模型；
>    - 常用非线性模型；
> 5. 模型调参：
>    - 贪心调参方法；
>    - 网格调参方法；
>    - 贝叶斯调参方法；



## 1. 线性回归模型

### 读取数据

> ❗️reduce_mem_usage 函数通过调整数据类型，帮助我们减少数据在内存中占用的空间

```python
def reduce_mem_usage(df):
    """ iterate through all the columns of a dataframe and modify the data type
        to reduce memory usage.        
    """
    start_mem = df.memory_usage().sum() 
    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))
    
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type != object:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        else:
            df[col] = df[col].astype('category')

    end_mem = df.memory_usage().sum() 
    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))
    return df
  
sample_feature = reduce_mem_usage(pd.read_csv('data_for_tree.csv'))
>>> Memory usage of dataframe is 46499672.00 MB
    Memory usage after optimization is: 12370303.00 MB
    Decreased by 73.4%   
```

### 简单建模

> 用LinearRegression简单建模，数据集采用上一篇特征工程处理后的结果。
>
> ❓ 绘制特征v_9的值与标签的散点图，图片发现模型的预测结果（蓝色点）与真实标签（黑色点）的分布差异较大，且**部分预测值出现了小于0的情况，说明我们的模型存在一些问题**

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/QI3Puu.png" style="zoom:50%;" />

### 发现问题

- 训练的线性回归模型的截距（intercept）与权重(coef)异常

```python
'intercept:'+ str(model.intercept_)
>>> 'intercept:-110670.68277227799'

sorted(dict(zip(continuous_feature_names, model.coef_)).items(), key=lambda x:x[1], reverse=True)
>>> [('v_6', 3367064.3416418713),
     ('v_8', 700675.5609398792),
     ('v_9', 170630.27723221315),
     ('v_7', 32322.66193202968),
     ('v_12', 20473.670796924365),
     ('v_3', 17868.079541476392),
     ('v_11', 11474.938996675868),
     ('v_13', 11261.76456000783),
     ('v_10', 2683.920090599314),
     ('gearbox', 881.8225039248413),
     ('fuelType', 363.9042507215821),
     ('bodyType', 189.6027101207262),
     ('city', 44.94975120522563),
     ('power', 28.55390161675297),
     ('brand_price_median', 0.5103728134079052),
     ('brand_price_std', 0.4503634709263436),
     ('brand_amount', 0.1488112039506626),
     ('brand_price_max', 0.003191018670313464),
     ('SaleID', 5.3559899198566125e-05),
     ('train', 3.725290298461914e-09),
     ('offerType', -6.032059900462627e-07),
     ('seller', -1.3578101061284542e-06),
     ('brand_price_sum', -2.1750068681877097e-05),
     ('name', -0.00029800127130881054),
     ('used_time', -0.0025158943328642993),
     ('brand_price_average', -0.40490484510116304),
     ('brand_price_min', -2.2467753486890696),
     ('power_bin', -34.42064411727317),
     ('v_14', -274.78411807719135),
     ('kilometer', -372.8975266607234),
     ('notRepairedDamage', -495.1903844628754),
     ('v_0', -2045.054957354832),
     ('v_5', -11022.986240511687),
     ('v_4', -15121.73110985152),
     ('v_2', -26098.29992048568),
     ('v_1', -45556.18929726266)]
```

- 数据的标签（price）呈现长尾分布

> ‼️ 通过作图我们发现数据的标签（price）呈现长尾分布，不利于我们的建模预测。原因是很多模型都假设数据误差项符合正态分布，而长尾分布的数据违背了这一假设。

```python
import seaborn as sns
print('It is clear to see the price shows a typical exponential distribution')
plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
sns.distplot(train_y)
plt.subplot(1,2,2)
sns.distplot(train_y[train_y < np.quantile(train_y, 0.9)])
```

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/d3IWz7.png)

> 在这里我们对标签进行了 $log(x+1)$  变换，使标签贴近于正态分布

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/GrOynW.png)

> 再次训练模型，线性回归模型的截距（intercept）与权重(coef) 恢复正常

```python
>>> intercept:18.75074946556707

>>> [('v_9', 8.052409900567516),
     ('v_5', 5.764236596651678),
     ('v_12', 1.618208123678382),
     ('v_1', 1.4798310582981786),
     ('v_11', 1.166901656359816),
     ('v_13', 0.9404711296032502),
     ('v_7', 0.713727308356225),
     ('v_3', 0.6837875771079552),
     ('v_0', 0.00850051801011538),
     ('power_bin', 0.008497969302891761),
     ('gearbox', 0.007922377278329297),
     ('fuelType', 0.006684769706831137),
     ('bodyType', 0.004523520092703335),
     ('power', 0.0007161894205358977),
     ('brand_price_min', 3.334351114748513e-05),
     ('brand_amount', 2.897879704277427e-06),
     ('brand_price_median', 1.2571172873003311e-06),
     ('brand_price_std', 6.659176363424454e-07),
     ('brand_price_max', 6.194956307516873e-07),
     ('brand_price_average', 5.999345965068067e-07),
     ('SaleID', 2.1194170039651024e-08),
     ('seller', 1.234941038319448e-10),
     ('train', 4.831690603168681e-12),
     ('offerType', -9.57047774363673e-11),
     ('brand_price_sum', -1.5126504215919774e-10),
     ('name', -7.015512588879555e-08),
     ('used_time', -4.1224793723524284e-06),
     ('city', -0.002218782481042428),
     ('v_14', -0.004234223418111909),
     ('kilometer', -0.013835866226883112),
     ('notRepairedDamage', -0.27027942349846507),
     ('v_4', -0.8315701200993834),
     ('v_2', -0.9470842241612037),
     ('v_10', -1.6261466689767436),
     ('v_8', -40.34300748761642),
     ('v_6', -238.79036385506728)]
```

> 再次进行可视化，发现预测结果与真实值较为接近，且未出现异常状况

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/l4Ld8Q.png" style="zoom:50%;" />

## 2. 模型性能验证

### 五折交叉验证

> 在使用训练集对参数进行训练的时候，经常会发现人们通常会将一整个训练集分为三个部分（比如mnist手写训练集）。一般分为：训练集（train_set），评估集（valid_set），测试集（test_set）这三个部分。这其实是为了保证训练效果而特意设置的。其中**测试集很好理解，其实就是完全不参与训练的数据，仅仅用来观测测试效果的数据**。因为在实际的训练中，训练的结果对于训练集的拟合程度通常还是挺好的（初始条件敏感），但是对于训练集之外的数据的拟合程度通常就不那么令人满意了。因此我们通常并不会把所有的数据集都拿来训练，而是**分出一部分来（这一部分不参加训练）对训练集生成的参数进行测试，相对客观的判断这些参数对训练集之外的数据的符合程度**。这种思想就称为交叉验证（Cross Validation）

> ‼️ 但在事实上，由于我们并不具有预知未来的能力，**五折交叉验证在某些与时间相关的数据集上反而反映了不真实的情况**。通过2018年的二手车价格预测2017年的二手车价格，这显然是不合理的，因此我们还可以采**用时间顺序对数据集进行分隔**。在本例中，我们选**用靠前时间的4/5样本当作训练集，靠后时间的1/5当作验证集**，最终结果与五折交叉验证差距不大

 ### 绘制学习率曲线与验证曲线

```python
def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,n_jobs=1, train_size=np.linspace(.1, 1.0, 5 )):  
    plt.figure()  
    plt.title(title)  
    if ylim is not None:  
        plt.ylim(*ylim)  
    plt.xlabel('Training example')  
    plt.ylabel('score')  
    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_size, scoring = make_scorer(mean_absolute_error))  
    train_scores_mean = np.mean(train_scores, axis=1)  
    train_scores_std = np.std(train_scores, axis=1)  
    test_scores_mean = np.mean(test_scores, axis=1)  
    test_scores_std = np.std(test_scores, axis=1)  
    plt.grid()#区域  
    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,  
                     train_scores_mean + train_scores_std, alpha=0.1,  
                     color="r")  
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,  
                     test_scores_mean + test_scores_std, alpha=0.1,  
                     color="g")  
    plt.plot(train_sizes, train_scores_mean, 'o-', color='r',  
             label="Training score")  
    plt.plot(train_sizes, test_scores_mean,'o-',color="g",  
             label="Cross-validation score")  
    plt.legend(loc="best")  
    return plt  
```

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ds1aqa.png" style="zoom:50%;" />

## 3. 多种模型对比

### 嵌入式特征选择

> 在过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明显的分别。而嵌入式特征选择在学习器训练过程中自动地进行特征选择。嵌入式选择最常用的是L1正则化与L2正则化。在对线性回归模型加入两种正则化方法后，他们分别变成了岭回归与Lasso回归。

> ⭕️ **L2正则化(岭回归)**在拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』

> ⭕️ **L1正则化(Lasso回归)**有助于生成一个稀疏权值矩阵，进而可以用于特征选择。如下图，我们发现power与userd_time特征非常重要。

> ⭕️ 除此之外，**决策树通过信息熵或GINI指数选择分裂节点时，优先选择的分裂特征也更加重要，这同样是一种特征选择的方法**。XGBoost与LightGBM模型中的model_importance指标正是基于此计算的

## 4. 模型对比

> 除了线性模型以外，还有许多我们常用的**非线性模型**。我们选择了部分常用模型与线性模型进行效果比对。

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/STAzek.png)

> 可以看到随机森林模型在每一个fold中均取得了更好的效果

## 5. 模型调参

- #### 贪心调参

> 把求解的问题分成若干个子问题；对每个子问题求解，得到子问题的局部最优解；把子问题的解局部最优解合成原来问题的一个解。

```python
## LGB的参数集合：

objective = ['regression', 'regression_l1', 'mape', 'huber', 'fair']

num_leaves = [3,5,10,15,20,40, 55]
max_depth = [3,5,10,15,20,40, 55]
bagging_fraction = []
feature_fraction = []
drop_rate = []

# 1_turning_obj
best_obj = dict()
for obj in objective:
    model = LGBMRegressor(objective=obj)
    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))
    best_obj[obj] = score
    
# 2_turning_leaves
best_leaves = dict()
for leaves in num_leaves:
    model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0], num_leaves=leaves)
    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))
    best_leaves[leaves] = score
    
# 3_turning_depth
best_depth = dict()
for depth in max_depth:
    model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0],
                          num_leaves=min(best_leaves.items(), key=lambda x:x[1])[0],
                          max_depth=depth)
    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))
    best_depth[depth] = score
    
sns.lineplot(x=['0_initial','1_turning_obj','2_turning_leaves','3_turning_depth'], y=[0.143 ,min(best_obj.values()), min(best_leaves.values()), min(best_depth.values())])
```

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/xTm1tj.png" style="zoom:50%;" />

- #### Grid Search 调参

`from sklearn.model_selection import GridSearchCV`

- #### 贝叶斯调参

```python
from bayes_opt import BayesianOptimization

def rf_cv(num_leaves, max_depth, subsample, min_child_samples):
    val = cross_val_score(
        LGBMRegressor(objective = 'regression_l1',
            num_leaves=int(num_leaves),
            max_depth=int(max_depth),
            subsample = subsample,
            min_child_samples = int(min_child_samples)
        ),
        X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)
    ).mean()
    return 1 - val
    
rf_bo = BayesianOptimization(
    rf_cv,
    {
    'num_leaves': (2, 100),
    'max_depth': (2, 100),
    'subsample': (0.1, 1),
    'min_child_samples' : (2, 100)
    }
)
```

### [回归分析的五个基本假设](https://www.analyticsvidhya.com/blog/2016/07/deeper-regression-analysis-assumptions-plots-solutions/)

> 回归分析是一种统计学上分析数据的方法，目的在于了解两个或多个变量间是否相关、相关方向与强度，并建立数学模型。以便通过观察特定变量（自变量），来预测研究者感兴趣的变量（因变量）。
>
> 总的来说，回归分析是一种参数化方法，即为了达到分析目的，需要设定一些“自然的”假设。**如果目标数据集不满足这些假设，回归分析的结果就会出现偏差。因此想要进行成功的回归分析，我们就必须先证实这些假设。**

#### 5 Assumptions in Regression

1. There should be a linear and additive relationship between dependent (response) variable and independent (predictor) variable(s). 「**线性性 & 可加性**」

   > A linear relationship suggests that a change in response Y due to one unit change in X¹ is constant, regardless of the value of X¹. An additive relationship suggests that the effect of X¹ on Y is independent of other variables.

2. There should be no correlation between the residual (error) terms. 「**各项residual相互独立**」

   > *Residual = Observed value - Predicted value*
   >
   > Absence of this phenomenon is known as Autocorrelation「**自相关性**」.

3. The independent variables should not be correlated.「各项自变量 x 相互独立」

   > Absence of this phenomenon is known as multicollinearity.「**多重共线性性**」

4. The error terms must have constant variance. 

   > This phenomenon is known as homoskedasticity.「**同方差性**」The presence of non-constant variance is referred to heteroskedasticity.「**异方差性**」
   >
   > <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/AN6TzG.jpg" style="zoom:70%;" />

5.  The error terms must be normally distributed.「**residual符合正态分布**」

   > ❓ The error term can be thought of as the composite of a number of minor influences or errors. As the number of these minor influences gets larger, the distribution of the error term tends to approach the normal distribution. This tendency is called the Central Limit Theorem. The t-test and F- test are not applicable unless the error term is normal distributed.

#### What if these assumptions get violated ?

**1. Linear and Additive:** If you fit a linear model to a non-linear, non-additive data set, the regression algorithm would **fail to capture the trend mathematically, thus resulting in an inefficient model**. Also, this will result in erroneous predictions on an unseen data set.

> **How to check:** Look for **residual vs fitted value plots** (explained below). Also, you can include polynomial terms (X, X², X³) in your model to capture the non-linear effect. 

**2. Autocorrelation:** The presence of correlation in error terms drastically reduces model’s accuracy. This **usually occurs in time series models where the next instant is dependent on previous instant**. If the error terms are correlated, the estimated standard errors tend to underestimate the true standard error.

> If this happens, it **causes confidence intervals and prediction intervals to be narrower**「**导致置信区间变窄**」. Narrower confidence interval means that a 95% confidence interval would have lesser probability than 0.95 that it would contain the actual value of coefficients. Let’s understand narrow prediction intervals with an example:
>
> For example, the least square coefficient of X¹ is 15.02 and its standard error is 2.08 (without autocorrelation). But in presence of autocorrelation, the standard error reduces to 1.20. As a result, the prediction interval narrows down to (13.82, 16.22) from (12.94, 17.10).
>
> Also, lower standard errors would cause the associated p-values to be lower than actual. This will make us incorrectly conclude a parameter to be statistically significant.

> **How to check:** Look for **Durbin – Watson (DW) statistic**. It must lie between 0 and 4. If DW = 2, implies no autocorrelation, 0 < DW < 2 implies positive autocorrelation while 2 < DW < 4 indicates negative autocorrelation. Also, you can see residual vs time plot and look for the seasonal or correlated pattern in residual values.

**3. Multicollinearity:** This phenomenon exists when the independent variables are found to be **moderately or highly correlated**. In a model with correlated variables, it becomes **a tough task to figure out the true relationship of a predictors with response variable**. In other words, it becomes difficult to find out which variable is actually contributing to predict the response variable.

> Another point, with presence of correlated predictors, the standard errors tend to increase. And, with large standard errors, the confidence interval becomes wider leading to less precise estimates of slope parameters.
>
> Also, when predictors are correlated, the estimated regression coefficient of a correlated variable depends on which other predictors are available in the model. If this happens, you’ll end up with an incorrect conclusion that a variable strongly / weakly affects target variable. Since, even if you drop one correlated variable from the model, its estimated regression coefficients would change. That’s not good!

> **How to check:** You can **use scatter plot to visualize correlation effect among variables**. Also, you can also use **VIF factor**. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity. Above all, a correlation table should also solve the purpose.

**4. Heteroskedasticity:** The presence of non-constant variance in the error terms results in heteroskedasticity. Generally, **non-constant variance arises in presence of outliers or extreme leverage values**. Look like, these values get too much weight, thereby disproportionately influences the model’s performance. When this phenomenon occurs, **the confidence interval for out of sample prediction tends to be unrealistically wide or narrow**.

> **How to check**: You can look at **residual vs fitted values plot**. If heteroskedasticity exists, the plot would exhibit a funnel shape pattern (shown in next section). Also, you can use **Breusch-Pagan / Cook – Weisberg test** or **White general test** to detect this phenomenon.

**5. Normal Distribution of error terms:** If the error terms are non-normally distributed, **confidence intervals may become too wide or narrow**. Once confidence interval becomes unstable, it leads to difficulty in estimating coefficients based on minimization of least squares. Presence of non – normal distribution suggests that there are a few unusual data points which must be studied closely to make a better model.

> **How to check:** You can look at **QQ plot** (shown below). You can also perform statistical tests of normality such as **Kolmogorov-Smirnov test**, **Shapiro-Wilk test**.

#### Interpretation of Regression Plots

- **Residual vs Fitted Values - 检测「线性&可加性&同方差」**

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/K9SyBH.jpg)

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/1TqKeh.jpg)

This scatter plot shows **the distribution of residuals (errors) vs fitted values (predicted values)** . It is one of the most important plot which everyone must learn. It reveals various useful insights including outliers. The outliers in this plot are labeled by their observation number which make them easy to detect.

> There are two major things which you should learn:
>
> ❗️ If there exist any pattern (may be, a **parabolic shape**「**抛物线**」) in this plot, consider it as signs of **non-linearity**「**非线性**」 in the data. *It means that the model doesn’t capture non-linear effects.*
>
> ‼️ If a **funnel shape**「**漏斗**」 is evident in the plot, consider it as the signs of non constant variance i.e. **heteroskedasticity**.「**异方差**」

> **Solution:** To overcome the issue of non-linearity, you can **do a non linear transformation**「**做非线性变换**」 of predictors such as $log (X)$, $\sqrt X$ or $X^²$ transform the dependent variable. To overcome heteroskedasticity, a possible way is to transform the response variable such as $log(Y)$ or $\sqrt Y$. Also, you can use weighted least square method to tackle heteroskedasticity.

- **Normal Q-Q Plot - 检测「residual符合正态分布」**

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/fWqKkF.jpg)

This q-q or quantile-quantile is a scatter plot which helps us validate the assumption of normal distribution in a data set. Using this plot we can infer if the data comes from a normal distribution. **If yes, the plot would show fairly straight line「直线说明符合正态分布」**. Absence of normality in the errors can be seen with deviation in the straight line.

> If you are wondering what is a ‘**quantile「分位数」**’, here’s a simple definition: Think of quantiles as points in your data below which a certain proportion of data falls. Quantile is often referred to as percentiles. For example: when we say the value of 50th percentile is 120, it means half of the data lies below 120.

> **Solution:** If the errors are not normally distributed, **non – linear transformation of the variables (response or predictors) can bring improvement in the model**.「**做非线性变换**」

- **Scale Location Plot - 检测「同方差」 **

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ieFXA8.jpg)

This plot is also used to detect homoskedasticity (assumption of equal variance). It **shows how the residual are spread along the range of predictors.** It’s **similar to residual vs fitted value plot** except it uses **standardized residual values**. 

> **Ideally, there should be no discernible pattern in the plot.「理想状态，看不出任何模式则为正态分布」** This would imply that errors are normally distributed. But, in case, if the plot shows any discernible pattern (probably a funnel shape), it would imply non-normal distribution of errors.

> **Solution:** Follow the solution for heteroskedasticity given in plot 1.

- **Residuals vs Leverage Plot**

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/wboQm1.jpg)

It is also known as **Cook’s Distance plot**. Cook’s distance attempts to **identify the points which have more influence than other points.** Such influential points tends to have a sizable impact of the regression line. In other words, adding or removing such points from the model can completely change the model statistics.

> ❓ But, can these influential observations be treated as outliers? This question can only be answered after looking at the data. Therefore, in this plot, the large values marked by cook’s distance might require further investigation.

> **Solution:** For influential observations which are nothing but outliers, if not many, you can remove those rows. Alternatively, you can scale down the outlier observation with maximum value in data or else treat those values as missing values.



