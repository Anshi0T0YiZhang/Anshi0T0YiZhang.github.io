---
layout:     post
title:      特征工程
subtitle:   天池比赛二手车价格预测
date:       2020-03-26
author:     Young
header-img: img/bg-post/1*EbpKczfURCvAF6uUp_Hhew.jpeg
catalog: true
tags:
    - tianchi
---

### 常见的特征工程包括
> 1. 异常处理：
>    - 通过箱线图（或 3-Sigma）分析删除异常值；
>    - BOX-COX 转换（处理有偏分布）；
>    - 长尾截断；
> 2. 特征归一化/标准化：
>    - 标准化（转换为标准正态分布）；
>    - 归一化（抓换到 [0,1] 区间）；
>    - 针对幂律分布，可以采用公式： 𝑙𝑜𝑔(1+𝑥1+𝑚𝑒𝑑𝑖𝑎𝑛)log(1+x1+median)
> 3. 数据分桶：
>    - 等频分桶；
>    - 等距分桶；
>    - Best-KS 分桶（类似利用基尼指数进行二分类）；
>    - 卡方分桶；
> 4. 缺失值处理：
>    - 不处理（针对类似 XGBoost 等树模型）；
>    - 删除（缺失数据太多）；
>    - 插值补全，包括均值/中位数/众数/建模预测/多重插补/压缩感知补全/矩阵补全等；
>    - 分箱，缺失值一个箱；
> 5. 特征构造：
>    - 构造统计量特征，报告计数、求和、比例、标准差等；
>    - 时间特征，包括相对时间和绝对时间，节假日，双休日等；
>    - 地理信息，包括分箱，分布编码等方法；
>    - 非线性变换，包括 log/ 平方/ 根号等；
>    - 特征组合，特征交叉；
>    - 仁者见仁，智者见智。
> 6. 特征筛选
>    - 过滤式（filter）：先对数据进行特征选择，然后在训练学习器，常见的方法有 Relief/方差选择法/相关系数法/卡方检验法/互信息法；
>    - 包裹式（wrapper）：直接把最终将要使用的学习器的性能作为特征子集的评价准则，常见方法有 LVM（Las Vegas Wrapper） ；
>    - 嵌入式（embedding）：结合过滤式和包裹式，学习器训练过程中自动进行了特征选择，常见的有 lasso 回归；
> 7. 降维
>    - PCA/ LDA/ ICA；
>    - 特征选择也是一种降维。

### 0.EDA过程概要

> **1）数据概览**
>
> - `pd.describe()` 查看每列的统计量，个数count、平均值mean、方差std、最小 值min、中位数25% 50% 75% 、以及最大值
> - `pd.info()` 通过info来了解数据每列的type，有助于了解是否存在除了nan以 外的特殊符号异常
> 
> **2）判断缺失与异常**
>
> - 使用`pd.is_null()` 函数查看缺失
> - 使用 `pd.value_counts()` 查看数据情况
> - 可视化 ``msno.matrix()` `msno.bar()` 
> 
> **3）绘制数据分布 seaborn** ：Johnson SU、Normal、Log Normal等，一般用来了解预测值的分布
>
> **4）统计数值变量相关性** ：混淆矩阵
>
> **5）统计特征的偏度与峰度** 
>
> **6）绘制特征之间的关系图** ：数字特征相互之间的关系可视化、多变量互相回归关系可视化
>
> **7）绘制类别特征分布图** ：箱形图、小提琴图

### 1.异常值的识别与删除

> **箱线图**
>
> 箱线图技术实际上就是**利用数据的分位数识别其中的异常点**，该图形属于典型的统计图形，在学术界和工业界都得到广泛的应用。
>
> 图中的下四分位数指的是数据的25%分位点所对应的值（Q1）；中位数即为数据的50%分位点所对应的值（Q2）；上四分位数则为数据的75%分位点所对应的值（Q3）；上须的计算公式为Q3+1.5(Q3-Q1)；下须的计算公式为Q1-1.5(Q3-Q1)。其中，Q3-Q1表示四分位差。如果采用箱线图识别异常值，其判断标准是，当变量的数据值大于箱线图的上须或者小于箱线图的下须时，就可以认为这样的数据点为异常点（极端异常点）。

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/nLL9Ii.jpg)

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Br1Nzx.jpg)

```python
def outliers_proc(data, col_name, scale=3):
    """
    用于清洗异常值，默认用 box_plot（scale=3）进行清洗极端异常点
    :param data: 接收 pandas 数据格式
    :param col_name: pandas 列名
    :param scale: 尺度
    :return:
    """

    def box_plot_outliers(data_ser, box_scale):
        """
        利用箱线图去除异常值
        :param data_ser: 接收 pandas.Series 数据格式
        :param box_scale: 箱线图尺度，
        :return:
        """
        iqr = box_scale * (data_ser.quantile(0.75) - data_ser.quantile(0.25))
        val_low = data_ser.quantile(0.25) - iqr
        val_up = data_ser.quantile(0.75) + iqr
        rule_low = (data_ser < val_low)
        rule_up = (data_ser > val_up)
        return (rule_low, rule_up), (val_low, val_up)

    data_n = data.copy()
    data_series = data_n[col_name]
    rule, value = box_plot_outliers(data_series, box_scale=scale)
    index = np.arange(data_series.shape[0])[rule[0] | rule[1]]
    print("Delete number is: {}".format(len(index)))
    data_n = data_n.drop(index)
    data_n.reset_index(drop=True, inplace=True)
    print("Now column number is: {}".format(data_n.shape[0]))
    index_low = np.arange(data_series.shape[0])[rule[0]]
    outliers = data_series.iloc[index_low]
    print("Description of data less than the lower bound is:")
    print(pd.Series(outliers).describe())
    index_up = np.arange(data_series.shape[0])[rule[1]]
    outliers = data_series.iloc[index_up]
    print("Description of data larger than the upper bound is:")
    print(pd.Series(outliers).describe())
    
    fig, ax = plt.subplots(1, 2, figsize=(10, 7))
    sns.boxplot(y=data[col_name], data=data, palette="Set1", ax=ax[0])
    sns.boxplot(y=data_n[col_name], data=data_n, palette="Set1", ax=ax[1])
    return data_n
```

> 对数字特征 power 进行异常值清洗

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Lxt6S7.png" alt="清洗前 vs 清洗后" style="zoom:50%;" />

> **3∂原则**
>
> 这个原则有个条件：**数据需要服从正态分布**。在3∂原则下，异常值如超过3倍标准差，那么可以将其视为异常值。正负3∂的概率是99.7%，那么距离平均值3∂之外的值出现的概率为$P( \mid x-u \mid > 3∂) \leq 0.003$，属于极个别的小概率事件。如果数据不服从正态分布，也可以用远离平均值的多少倍标准差来描述。红色箭头所指就是异常值。

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/XfX9Ls.jpg"/>

### 2.特征构造

❌ 我们从EDA学到了啥（待完善）

> 现在我们已经探索了数据中的趋势和关系，我们可以为我们的模型设计一组函数。 我们可以使用EDA的结果来构建特征工程。 特别是，**我们从EDA学到了以下知识，可以帮助我们进行特征工程/选择**：
>
> - 分数分布因建筑类型而异，并且在较小程度上因行政区而异。 虽然我们将关注数字特征，但我们还应该在模型中包含这两个分类特征。
> - 对特征进行对数变换不会导致特征与分数之间的线性相关性显着增加

| 数据使用                      | **Field**         | **Description**                                              | 数据情况                                                     |
| :---------------------------- | :---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
|                               | SaleID            | 交易ID，唯一编码                                             | 唯一编码                                                     |
|                               | name              | 汽车交易名称，已脱敏                                         | 【无有效信息】                                               |
| 合并特征，构建新特征used_time | ~~regDate~~       | 汽车注册日期，例如20160101，2016年01月01日                   | 最早注册日期19910001、最晚20151212                           |
|                               | model             | 车型编码，已脱敏                                             | [0-247] 种车型【有缺失值】                                   |
| 计算统计量                    | brand             | 汽车品牌，已脱敏                                             | [0-39] 种品牌                                                |
|                               | bodyType          | 车身类型：豪华轿车：0，微型车：1，厢型车：2，大巴车：3，敞篷车：4，双门汽车：5，商务车：6，搅拌车：7 | [0-7] 种车身类型【有缺失值】                                 |
|                               | fuelType          | 燃油类型：汽油：0，柴油：1，液化石油气：2，天然气：3，混合动力：4，其他：5，电动：6 | [0-6] 种燃油类型【有缺失值】                                 |
|                               | gearbox           | 变速箱：手动：0，自动：1                                     | [0-1] 种变速箱【有缺失值】                                   |
| 数据分桶                      | power             | 发动机功率：范围 [ 0, 600 ]                                  | max值19312【该字段存在异常值】                               |
|                               | kilometer         | 汽车已行驶公里，单位万km                                     | [0-15] 万行驶里程                                            |
|                               | notRepairedDamage | 汽车有尚未修复的损坏：是：0，否：1                           | [0-1] 是否损坏                                               |
| 提取特征[:-3]                 | ~~regionCode~~    | 地区编码，已脱敏                                             |                                                              |
| 无效特征                      | ~~seller~~        | ~~销售方：个体：0，非个体：1~~                               | ~~[0-1] 种销售【无有效信息】【数据倾斜】~~                   |
| 无效特征                      | ~~offerType~~     | ~~报价类型：提供：0，请求：1~~                               | ~~最大值为0【无有效信息】【数据倾斜】~~                      |
| 合并特征，构建新特征used_time | ~~creatDate~~     | 汽车上线时间，即开始售卖时间                                 | 最早售卖日期20150618、最晚20160407                           |
| 计算统计量                    | price             | 二手车交易价格（预测目标）                                   | 最大值99999，与75%中位数7700差距较大【该字段存在异常值】     |
|                               | v系列特征         | 匿名特征，包含v0-14在内15个匿名特征                          | v_2,v_3,v_4,v_10,v_11,v_12,v_13,v_14【最大值与75%中位数和均值差距较大】 |

> **1）合并特征：**
>
> 使用时间：data['creatDate'] - data['regDate']，反应汽车使用时间，一般来说价格与使用时间成反比
>
> > 看一下空数据，有 15k 个样本的时间是有问题的，我们可以选择删除，也可以选择放着。
> > 但是这里不建议删除，因为**删除缺失数据占总样本量过大**，7.5%。我们可以先放着，因为如果我们 XGBoost 之类的决策树，其本身就能处理缺失值，所以可以不用管；
>
> **2）提取特征：**
>
> 从邮编中提取城市信息，相当于加入了先验知识
>
> **3）计算某一特征的统计量信息**
>
> 从汽车品牌与二手车交易价格计算某品牌的（销售，价格）统计量
>
> ⚠️ 二手车交易价格为测试集独有，这里的思路为将测试集关于（销售，价格）统计量特征通过`pd.merge(brand_fe, how='left', on='brand')`的方式运用到训练集上【这里存疑❓】
>
> **4）数据分桶**
> ```python
> bin = [i*10 for i in range(31)]
> data['power_bin'] = pd.cut(data['power'], bin, labels=False)
> ```
> 以 power 为例，这时候我们的缺失值也进桶了，为什么要做数据分桶呢，原因有很多，= =
> 1. 离散后稀疏向量内积乘法运算速度更快，计算结果也方便存储，容易扩展；
> 2. 离散后的特征对异常值更具鲁棒性，如 age>30 为 1 否则为 0，对于年龄为 200 的也不会对模型造成很大的干扰；
> 3. LR 属于广义线性模型，表达能力有限，经过离散化后，每个变量有单独的权重，这相当于引入了非线性，能够提升模型的表达能力，加大拟合；
> 4. 离散后特征可以进行特征交叉，提升表达能力，由 M+N 个变量编程 M*N 个变量，进一步引入非线形，提升了表达能力；
> 5. 特征离散后模型更稳定，如用户年龄区间，不会因为用户年龄长了一岁就变化
>
> **5）归一化**
>
> ```python
> def max_min(x):
>     return (x - np.min(x)) / (np.max(x) - np.min(x))
> ```
>
> **6）对类别特征进行 OneEncoder**
>
> `pd.get_dummies(data, columns=['model', 'brand', 'bodyType', 'fuelType','gearbox', 'notRepairedDamage', 'power_bin'])`
>
> 7）确保所有的特征都是数字特征（类别特征、数字特征、object）
>
> `pd.select_dtypes(include="object").columns`


### 特征筛选

- #### 包裹式：用 SequentialFeatureSelector

```python
# k_feature 太大会很难跑，没服务器，所以提前 interrupt 了
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from sklearn.linear_model import LinearRegression
sfs = SFS(LinearRegression(),
           k_features=10,
           forward=True,
           floating=False,
           scoring = 'r2',
           cv = 0)
data = data[data["train"]==1]
x = data.drop(['price'], axis=1)
x = x.fillna(0)
y = data['price']
sfs.fit(x, y)
sfs.k_feature_names_ 
>>> ('kilometer',
     'v_0',
     'v_3',
     'v_7',
     'used_time',
     'brand_price_std',
     'brand_price_average',
     'model_167.0',
     'brand_24',
     'gearbox_1.0')
```

```python
# 查看边际效益
from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs
import matplotlib.pyplot as plt
fig1 = plot_sfs(sfs.get_metric_dict(), kind='std_dev')
plt.grid()
plt.show()
```

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/F0F7q1.png" style="zoom:50%;" />

### 经验总结

> 特征工程是比赛中最至关重要的的一块，特别的传统的比赛，大家的模型可能都差不多，调参带来的效果增幅是非常有限的，但**特征工程的好坏往往会决定了最终的排名和成绩**。
>
> **特征工程的主要目的还是在于将数据转换为能更好地表示潜在问题的特征**，从而提高机器学习的性能。比如，异常值处理是为了去除噪声，填补缺失值可以加入先验知识等。
>
> **特征构造**也属于特征工程的一部分，其目的是**为了增强数据的表达**。
>
> 有些比赛的特征是**匿名特征**，这导致我们并不清楚特征相互直接的关联性，这时我们就只有单纯基于特征进行处理，比如**装箱，groupby，agg** 等这样一些操作进行一些特征统计，此外还可以对特征进行进一步的 **log，exp 等变换**，或者对多个特征进行四则运算（如上面我们算出的使用时长），多项式组合等然后进行筛选。由于特性的匿名性其实限制了很多对于特征的处理，当然有些时候用 NN 去提取一些特征也会达到意想不到的良好效果。
>
> 对于**知道特征含义（非匿名）的特征工程**，特别是在工业类型比赛中，会基于信号处理，频域提取，丰度，偏度等构建更为有实际意义的特征，这就是**结合背景的特征构**建，在推荐系统中也是这样的，各种类型点击率统计，各时段统计，加用户属性的统计等等，这样一种特征构建往往要深入分析背后的业务逻辑或者说物理原理，从而才能更好的找到 magic。
>
> 当然特征工程其实是和模型结合在一起的，这就是为什么要为 LR NN 做分桶和特征归一化的原因，而对于特征的处理效果和特征重要性等往往要通过模型来验证。
>
> 总的来说，特征工程是一个入门简单，但想精通非常难的一件事。

