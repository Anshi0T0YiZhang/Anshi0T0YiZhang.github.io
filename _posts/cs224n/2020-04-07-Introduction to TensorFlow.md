---
title: "Introduction to TensorFlow"
subtitle: "CS224N 284「7」"
layout: post
author: "echisenyang"
header-style: text
hidden: true
catalog: true
tags:
  - 笔记
  - 课程
---



## Introduction to TensorFlow

![UV71zW](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/UV71zW.png)

- why use tensorflow

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/xSzApC.png" alt="xSzApC" style="zoom:25%;" />

- What is TensorFlow?

  1. Open source software library for numerical computation using data flow graphs

  2. Originally developed by Google Brain Team to conduct machine learning research

  3. “Tensorflow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms”

### Programming model

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/MIk0Oa.png" alt="MIk0Oa" style="zoom:25%;" />

- **Variables**：

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/QSHiXX.png" alt="QSHiXX" style="zoom:25%;" />

- **Placeholders**：占位符，不给定任何初始化的值，只分配一个数据类型（分配一种大小的张量），这样我们的图就知道应该图和计算尽管当前没有存储任何值。

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/M4eU94.png" alt="M4eU94" style="zoom:25%;" />

- **Mathematical operations**

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/z5lMD5.png" alt="z5lMD5" style="zoom:25%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/XLRC9x.png" alt="XLRC9x" style="zoom:25%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/FwcM71.png" alt="FwcM71" style="zoom:25%;" />

> 到这，我们已经构建好了图，并准备接收数据让图流动起来。这时需要用会话（会话就像是硬件环境支持图中所有操作的执行，并且会话是deploy graph的方法）

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/tESkDN.png" alt="tESkDN" style="zoom:25%;" />

- **Fetch**：Fetch操作是指TensorFlow的session可以一次run多个op
  语法: 将多个op放入数组中然后传给run方法

```python
import tensorflow as tf

#Fetch
input1 = tf.constant(3.0)
input2 = tf.constant(2.0)
input3 = tf.constant(5.0)

#定义两个op
add = tf.add(input2, input3)
mu1 = tf.multiply(input1, add)

with tf.Session() as sess:
    #一次操作两个op, 按顺序返回结果
    result = sess.run([mu1, add])
    print(result)
    
>>> [21.0, 7.0]
```

- **Feed**：Feed操作是指首先建立占位符, 然后把占位符放入op中.
  在run op的时候, 再把要op的值传进去, 以达到使用时再传参数的目的

  语法: 首先创建placeholder 然后再在run的时候把值以字典的方式传入run

```python
#Feed
#创建占位符
input1 = tf.placeholder(tf.float32)
input2 = tf.placeholder(tf.float32)
#使用placeholder定义op
output = tf.multiply(input1, input2)

with tf.Session() as sess:
    #feed数据以字典的方式传入
    print(sess.run(output, feed_dict={input1: [7.], input2: [2.]}))
    
>>> [ 14.]
```

![Emafi6](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Emafi6.png)

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/hIUErN.png" alt="hIUErN" style="zoom:25%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Uh7K13.png" alt="Uh7K13" style="zoom:25%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/vFpv2w.png" alt="vFpv2w" style="zoom:25%;" />

### 实现 [linear regression & skip-gram](https://github.com/nishithbsk/tensorflow_tutorials)

```python
# linear regression
# Author: Nishith Khandwala (nishith@stanford.edu)
# Adapted from https://github.com/hans/ipython-notebooks/

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np
import tensorflow as tf
import matplotlib
matplotlib.use('TKAgg')
from matplotlib import pyplot as plt

'''
Good ole linear regression: find the best linear fit to our data
'''

def generate_dataset():
    # data is generated by y = 2x + e
    # where 'e' is sampled from a normal distribution
    x_batch = np.linspace(-1, 1, 101)
    y_batch = 2 * x_batch + np.random.randn(*x_batch.shape) * 0.3
    return x_batch, y_batch
    
def linear_regression():
    x = tf.placeholder(tf.float32, shape=(None,), name="x")
    y = tf.placeholder(tf.float32, shape=(None,), name="y")
    # Note: The second argument shape=(None,) indicates that 
    # these variables take on a 1-dimensional value of a dynamic 
    # size. We can use the None value in this case to allow for 
    # arbitrary batch sizes.
    
    with tf.variable_scope('lreg') as scope:
        w = tf.Variable(np.random.normal(), name="W")
        y_pred = tf.multiply(w, x)

        loss = tf.reduce_mean(tf.square(y_pred - y))
    return x, y, y_pred, loss

def run():
    x_batch, y_batch = generate_dataset()

    x, y, y_pred, loss = linear_regression()
    
    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)

    init = tf.global_variables_initializer()
    with tf.Session() as session:
        session.run(init)

        feed_dict = {x: x_batch, y: y_batch}
        for _ in range(30):
            loss_val, _ = session.run([loss, optimizer], feed_dict)
            print("loss:", loss_val)

        y_pred_batch = session.run(y_pred, {x: x_batch})

    plt.figure(1)
    plt.scatter(x_batch, y_batch)
    plt.plot(x_batch, y_pred_batch)
    plt.savefig('plot.png')
        
if __name__ == '__main__':
    run()
```



```python
# word2vec
# Author: Nishith Khandwala (nishith@stanford.edu)
# Adapted from https://www.tensorflow.org/tutorials/word2vec/

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import math
import numpy as np
import tensorflow as tf

from utils import *

'''
Consider the following sentence:
"the first cs224n homework was a lot of fun"
With a window size of 1, we have the dataset:
([the, cs224n], first), ([lot, fun], of) ...
Remember that Skipgram tries to predict each context word from 
its target word, and so the task becomes to predict 'the' and
'cs224n' from first, 'lot' and 'fun' from 'of' and so on.
Our dataset now becomes:
(first, the), (first, cs224n), (of, lot), (of, fun) ...
'''

# Let's define some constants first
batch_size = 128
vocabulary_size = 50000
embedding_size = 128  # Dimension of the embedding vector.
num_sampled = 64    # Number of negative examples to sample.

'''
load_data loads the already preprocessed training and val data.
train data is a list of (batch_input, batch_labels) pairs.
val data is a list of all validation inputs.
reverse_dictionary is a python dict from word index to word
'''
train_data, val_data, reverse_dictionary = load_data()
print("Number of training examples:", len(train_data)*batch_size)
print("Number of validation examples:", len(val_data))

def skipgram():
    batch_inputs = tf.placeholder(tf.int32, shape=[batch_size])
    batch_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
    val_dataset = tf.constant(val_data, dtype=tf.int32)

    with tf.variable_scope('word2vec') as scope:
        embeddings = tf.Variable(tf.random_uniform([vocabulary_size, 
                                                    embedding_size], 
                                                    -1.0, 1.0))
        batch_embeddings = tf.nn.embedding_lookup(embeddings, batch_inputs)

        weights = tf.Variable(tf.truncated_normal([vocabulary_size, 
                                                   embedding_size],
                                                   stddev=1.0/math.sqrt(embedding_size)))
        biases = tf.Variable(tf.zeros([vocabulary_size]))

        # This objective is maximized when the model assigns high probabilities
        # to the real words, and low probabilities to noise words.
        loss = tf.reduce_mean(tf.nn.nce_loss(weights=weights, 
                                             biases=biases,
                                             labels=batch_labels,
                                             inputs=batch_embeddings,
                                             num_sampled=num_sampled,
                                             num_classes=vocabulary_size))


        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
        normalized_embeddings = embeddings/norm
        
        val_embeddings = tf.nn.embedding_lookup(normalized_embeddings, 
                                                val_dataset)
        similarity = tf.matmul(val_embeddings, 
                               normalized_embeddings, transpose_b=True)

    return batch_inputs, batch_labels, normalized_embeddings, similarity, loss

def run():
    # load model
    batch_inputs, batch_labels, normalized_embeddings, similarity, loss = skipgram()
    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)

    init = tf.global_variables_initializer()
    with tf.Session() as session:
        session.run(init)

        average_loss = 0
        for step, batch_data in enumerate(train_data):
            inputs, labels = batch_data
            feed_dict = {batch_inputs: inputs, batch_labels: labels}

            _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)
            average_loss += loss_val
            
            if step % 1000 == 0:
                if step > 0:
                    average_loss /= 1000
                print("Average loss at step ", step, ": ", average_loss)
                average_loss = 0
                    
            if step % 5000 == 0:
                sim = similarity.eval()
                for i in xrange(len(val_data)):
                    top_k = 8  # number of nearest neighbors                       
                    nearest = (-sim[i, :]).argsort()[1:top_k + 1] 
                    print_closest_words(val_data[i], nearest, reverse_dictionary)
  
        final_embeddings = normalized_embeddings.eval()
        return final_embeddings

# Let's start training
final_embeddings = run()

# Visualize the embeddings.
visualize_embeddings(final_embeddings, reverse_dictionary)
```

## Lecture Notes: TensorFlow

**Nodes** in TensorFlow’s data ﬂow graph represent **mathematical operations**, while the **edges** represent the multidimensional **data** arrays (tensors) communicated between them.

- **step1：deﬁned a graph**

![u6e8ae](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/u6e8ae.png)

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/MiJS3k.png" alt="MiJS3k" style="zoom: 50%;" />

- step2：**deploy this graph** with a session and run the session to get our outputs

> **A session is an environment** that supports the execution of all operations to a particular execution context (e.g. CPU, GPU). A session can be easily built by doing sess = tf.Session(). **In order for a session to run, two arguments have to be fed: fetches and feeds**. We use feeds and fetches to get data into and out of arbitrary operations.

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/9UmUYG.png" alt="9UmUYG" style="zoom: 50%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/QhSMfg.png" alt="QhSMfg" style="zoom:50%;" />

- **step3：How to Train a Model in TensorFlow**

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/5ZllJ6.png" alt="5ZllJ6" style="zoom:50%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/pqEN6T.png" alt="pqEN6T" style="zoom:50%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ndMnsp.png" alt="ndMnsp" style="zoom:50%;" />

- **step4：Variable Sharing**

> One last important concept is variable sharing. When building complex models, **we often need to share large sets of variables and might want to initialize all of them in one place**. This can be done by using tf.variable _ scope() and tf.get _ variable().

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/3wjUy6.png" alt="3wjUy6" style="zoom:50%;" />