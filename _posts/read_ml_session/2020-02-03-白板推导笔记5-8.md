---
title: "「白板推导笔记」5-8"
subtitle: "Machine Learning Session"
layout: post
author: "echisenyang"
header-style: text
hidden: true
tags:
  - 笔记
  - 白板推导笔记
---

# 机器学习-白板推导系列

## 系列五 降维

### 背景

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Xie8fp.jpg)

- 防止过拟合

  - 增加样本数（更多的数据）
  - 正则化（限制参数空间，增加约束）
  - 降维
    - 直接降维：特征选择
    - 线性降维：PCA，MDS
    - 非线性降维：流形（ISOMAP, LLE）

- 维度灾难

  - 几何角度1：

    超立方体体积为1，超球体体积趋向于0 -》从而造成了**数据的稀疏性**，很难做分类把数据分开

  - 几何角度2:

    超球体，数据都几乎分布在球壳壁上（环形带）

### 样本均值&样本方差矩阵

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/tC08di.jpg)

### PCA最大投影方差

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Hw5XMG.jpg)

- 核心思想
  - 一个中心：**原始特征空间的重构**
    - 把相关的特征转换为无关的特征
  - 两个基本点：**最大投影方差、最小重构距离**
    - 最大投影方差：投影的点**覆盖距离大**
    - 最小重构距离：投影的点**分布散**代价小，若密集分布则重构的代价大
    - 这两个实际上是同一个意思
- 1.中心化
  - 对数据作平移（减去均值）
  - 所以过坐标轴原点
- 2.投影
  -  $J=\sum_{i=1}^N((x_i-\overline x)^T u_1)^2 =u_1^T \cdot S \cdot u_1$
  - 用**拉格朗日乘子法 求解带约束的不等式优化问题**
  - 看出**特征值与特征向量**，要求的便是**协方差矩阵 S 的特征向量 u**
  - PCA降维做的便是 **取 p维特征向量 u 的前 q 维当作主成分**

### PCA最小代价重构

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/3Ut7qg.jpg)

- **降维前**的 x 向量：$x_i=\sum_{k=1}^p(x_i^Tu_k) \cdot u_k$
- **降维后**的 x 向量：$\hat x_i=\sum_{k=1}^q(x_i^Tu_k) \cdot u_k$
- **最小重构代价为：降维后与降维前的模差**
- q+1到p 一个一个求解
- <font color=red>**思想 -》数学表达 -〉演变为最优化问题**</font>

### SVD角度看PCA与PCoA

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/NH0ZeD.jpg)

- 对中心化 HX 的数据矩阵进行奇异值分解 $HX=U\Sigma V^T$
- 再带到方差矩阵中去 $S=X^T H X=V\Sigma^2V^T$
- S 特征分解（主成分分析）
  - 先得到主成分方向 V
  - 然后 $HX\cdot V$ 得到坐标矩阵 $U\Sigma$
  - S 为 p*p维
- T 特征分解（主坐标分析）
  - 直接得到坐标
  - $T=U\Sigma^2U^T$ -> $TU\Sigma=U\Sigma^2U^TU\Sigma=U\Sigma\Sigma^2$
  - $U\Sigma$：特征向量
  - T 为 N*N维
- 一般看哪个维度小用哪个分解

### 概率角度P-PCA看PCA

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/uzs8A3.jpg)

- 模型
  - x是原始数据，z是重构降维后数据
- 求解
  - 假设 先验 Z
  - 求解 $X|Z$
  - 观测得数据 $X$
  - **求解后验** $Z|X$ -》相当于求**后验分布**
- 随着采样点 z 数量的增多，会形成更多的 x 各向同性的高斯分布，最终形成一个 各向异性的高斯分布
- 条件概率 $E(X|Z)$：这里把 $Z$ 看作已知常数
- $E(X)$：这里的 $Z$ 不再是常数，而应该视作变量

## 系列六 支持向量机

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/5UQmCV.jpg)

### 硬间隔SVM 模型定义

- **SVM有三宝：间隔、对偶、核技巧**
- SVM分类：
  - hard-margin SVM（最大间隔分类器）
  - soft-margin SVM
  - Kernel SVM 
- SVM 要做的就是找到最合适的超平面 $w^Tx+b$ 使其离两边样本点的距离足够大，模型 $f(w)=sign(w^Tx+b)$ 相当于是线性分类中的判别模型。
- **Min-distance：表示间隔为支持向量到超平面的距离**
- **<font color=red>几何意义的表达映射成数学语言(凸优化问题)</font>**

### 硬间隔SVM 模型求解 引出对偶问题

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/2vt4yQ.jpg)

- 当样本数不多，数据维度不高的时候，可以直接使用套件求解
- **原问题（primal problem）转换为 无约束的原问题**
  - **<font color=red>引入拉格朗日乘子法：将带约束的问题转为无约束问题（关于 w 与 b）</font>**
  - 无约束问题实际上**隐含 w,b 的取值范围**（$\Delta \leq 0$）
  - ![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ruZi3U.jpg)
- **无约束的原问题 转换为 强对偶问题（dual problem）**
  - **弱对偶关系**：$min \ max \ L \geq max \ min \ L$
    - 宁为鸡头不为凤尾（自我安慰的话）实际上还是凤强
  - **强对偶关系**：$min \ max \ L = max \ min \ L$
    - **凸优化二次问题天生满足强对偶**

### 硬间隔SVM 模型求解 引出KKT条件

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/3QR88i.jpg)

- KKT条件
  - 什么是KKT条件？
    - 原问题与对偶问题具有强对偶关系 $\iff$ 满足KKT条件
  - 松弛互补条件 slackless complementary
  - 这里没有对 $\lambda$ 的偏导
- 则**决策平面**为：${w^*}^Tx+b^*$, **决策函数**为：$f(x)=sign({w^*}^Tx+b^*)$

### 软间隔SVM 模型定义

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/qBNnsi.jpg)

- **思想：允许一点点错误**
- loss表达错误
  - 定义为犯错误点的个数：0-1损失
    - 指示函数 不连续不可导
  - 定义为距离：hinge loss
    - 合并定义max函数 连续可导
    - z=0：超平面
    - z=1：支持向量
    - 0～1之间为loss考虑的范围，即噪声
- 求解过程略，同硬间隔

### 约束优化问题 弱对偶性证明【补充】

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/xqiF3v.jpg)

- 约束优化问题
  - 目标函数
  - 不等式约束
  - 等式约束
- 拉格朗日函数
  - 对不等式约束与等式约束添加拉格朗日乘子
  - **隐含只用 好的x，即优化问题本身蕴涵了 x 属于好的x 的集合 **
  - **引入拉格朗日函数，把原问题转为原问题的无约束形式**
- 对偶性
  - 原问题是关于 x 的函数
  - 对偶问题是关于 参数 $\lambda$ 的函数
  - **弱对偶性：对偶问题的优化值 $\leq$ 原问题**
  - $A \leq B$ 恒成立，故 -》证毕

### 约束优化问题 对偶关系之几何解释

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/7ozBtA.jpg)

- 核心问题：$p^*,g(\lambda)$ 如何在图上表示
- $p^*$：是集合的下确界
  - 限制条件：必须在区域 $G$  内取值，$u \leq 0$ 
  - 表示：区域 $G$  左半区域在 $t$ 轴上的投影线段的下确界
- $g(\lambda)$：当 $\lambda$ 固定时，与区域 $G$  相交的直线 $t+\lambda u$，**从下切点到上切点的集合**的下确界
  - $d^*$：变动$\lambda$ 值，当同时与两点相切时，为 $max \ g(\lambda)$
- 非凸优化 -》弱对偶关系
  - 有 $d^* \leq p^*$
- 凸优化 + 限制条件（slater条件是一种限制条件） -〉强对偶关系
  - 有 $d^* = p^*$
  - slater条件为充分非必要条件
  - SVM是二次规划问题，天生满足 slater条件，默认强对偶关系成立

### 约束优化问题 对偶关系之slater condition

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/mzWWNo.jpg)

- relint  D：相对内部
  - **凸二次规划问题：f是凸的，$m_i$是仿射的**
  - 天然满足slater条件
  - **理论上保证 SVM 天然 满足强对偶关系**
  - **<font color=red>满足强对偶关系，而且是凸优化问题，则可以用KKT条件</font>**

### 约束优化问题 对偶关系之KKT条件

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/QS5LnO.jpg)

- KKT条件给出了一种解法
  - 给出了关于 $x^*,\lambda^*,\eta^*$ 的关系
  - KKT条件是强对偶关系的充要条件
- KKT条件满足三组条件
  - 可行条件：关于 $x^*$ 
  - 互补松弛：
  - 梯度为0

## 系列七 核方法

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/P9y0tA.jpg)

### 背景介绍

- **<font color=red>非线性带来高位转换</font>**
  - $\phi (x)$：非线性转换
  - 把非线性可分转换为线性可分
  - 异或问题：最著名的非线性问题
  - **二维空间映射成三维空间**：$X=(x_1,x_2) \overset{\phi(x)}{\longmapsto}Z=(x_1,x_2,(x_1-x_2)^2)$
  - **<font color=red>cover theorem：高维比低维更易线性可分</font>**
- **<font color=red>对偶表示带来内积</font>**
  - 内积 $x_i^Tx_j \mapsto \phi (x_i)^T \phi (x_j)$
  - 想直接求内积，而不必先求出 $\phi (x)$ ，再求出 $\phi (x_i)^T \phi (x_j)$
  - 引入 kernel function
- **<font color=red>直接一步到位</font>**：**可以直接根据输入空间的样本 直接求出内积，从而避免先求出满足形式的 $\phi (x)$** 
- **<font color=red>Kernel SVM在模型上没有变化，只是计算上引入了 kernel function</font>**

### 正定核 两个定义

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/fQKNdp.jpg)

- 通常指的核函数是正定核函数
- 定义一：希尔伯特空间
- 定义二：另一种判别方式 gram矩阵半正定

### 正定核 必要性证明

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/nSIYoc.jpg)

## 系列八 指数族分布

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/75YkN0.jpg)

### 背景介绍

- 指数族分布形式

  - $P(x|\eta)=h(x)exp(\eta^T\phi(x)-A(\eta))$

  - $\eta$：参数向量

  - $A(\eta)$：log partition-function 

    - 配分函数：相当于归一化因子

  - 性质1-》充分统计量：

    - 不需要把样本的详细信息记录下来，比如只需要记录均值与平方值（从而可以推导均值与方差），从而只需记录统计信息，相当于压缩数据

  - 性质2-》共轭：

    ![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ScrRh1.jpg)

  - 性质3-〉最大熵

  - 应用1-》广义线性模型

  - 应用2-〉概率图模型

  - 应用3-》变分推断

  

### 高斯分布的指数族形式

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/u7UZ9u.jpg)

- 一维高斯分布的例子
  - $\phi(x)$：只与 x 相关
  - $A(\eta)$：只与 $\eta$ 相关

### 对数配分函数与充分量统计

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/H4n8T1.jpg)

- **挖掘充分统计量与对数分配函数的关联**
  - step1: 对 $p(x|\eta)$ 求积分，积分为1，则可推导出 $A(\eta)$ 与 $\phi(x)$ 的关系
  - Step2: 对等式两边同步对 $\eta$ 求导
  - 结论
    - $A^{'}(\eta)$ ：**一次导为概率密度函数关于充分统计量的期望**
    - $A^{''}(\eta)$： **二次导为概率密度函数关于充分统计量的方差**
    - $A(\eta)$：为**凸函数**（二次导大于0）

### 极大似然估计与充分量统计

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/OyYoXY.jpg)

- $A^{'}(\eta_{MLE})=\frac{1}{N}\sum_{i=1}^N\phi(x_i)$

### 最大熵角度

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/0bRLGF.jpg)

- 熵
  - 信息量 $-log \ p$：
    - 假设随机事件发生的概率为 p，则信息量为 $log \ \frac{1}{p}$，即成反比的关系
  - 熵为信息量关于分布本身的期望
    - 连续用积分
    - 离散用 $\sum$
  - **熵是对可能性的衡量**
  - **最大熵：等可能** 
- 用拉格朗日乘子法求解
  - 先转为最小化问题
  - **没有任何已知的情况下，均匀分布可以使熵达到最大**

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/eOxuTr.jpg)

- **满足已经事实（约束）：事实-》经验分布-〉统计量表达**
  - 事实就是数据
  - 经验分布是对数据的表达
  - 约束转而用经验分布的统计量表达（均值期望、方差期望）

- 最大熵原理
  - 假设 $f(x)$ 为函数向量
  - **最大熵最终推出 $p(x)$ 为指数族分布**