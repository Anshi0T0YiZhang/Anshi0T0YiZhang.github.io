---
title: "「白板推导笔记」1-4"
subtitle: "Machine Learning Session"
layout: post
author: "echisenyang"
header-style: text
hidden: true
tags:
  - 笔记
  - 白板推导笔记
---

# 机器学习-白板推导系列

## 系列一 序论 

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/NHB9IA.jpg)

###  学习资料推荐


|                   书籍推荐                   |                视频                |
| :------------------------------------------: | :--------------------------------: |
|       【pdf】李航-统计学习方法(第二版)       |            吴恩达-cs229            |
|            【book】周志华-西瓜书             | 徐亦达-概率模型（github配套notes） |
|        【pdf】PRML-模式识别与机器学习        |         台大李宏毅-ML2017          |
| 【网页链接】MLAPP-从概率学的角度阐释机器学习 |        台大李宏毅-MLDS2018         |
|        【github/md】ESL-统计学习基础         |                                    |
|                 【book】花书                 |                                    |
|          【pdf笔记】台大林轩田-基石          |                                    |
|          【pdf笔记】台大林轩田-技法          |                                    |
|                                              |                                    |

- 机器学习学习顺序
  - 白板推导23讲(bilibili视频)
  - 李航-统计学习方法【十二章 十个算法 感K朴决逻 支提E隐条】
  - PRML【回分神核稀 图混近采连 顺组】
  - 基石【VC Theory 正则化等基础理论讲的特别好】
  - 技法【SVM 决策树 随机森林等机器学习讲的特别好 深度学习涉及少】
  - 西瓜书
  - ng-cs229
  - 徐亦达/台大李宏毅
- 深度学习学习顺序
  - cs224n(bilibili视频)
  - 动手学深度学习(github配套dive into deep learning with tensorflow)
  - 花书

###  频率派 vs. 贝叶斯派
  - 把数据当作概率模型 $x \sim p(x|\theta)$
    
    - $x_i \overset{i.i.d.}{\sim} p(x|\theta)$ 因 $x_i$ 独立同分布，故写成 $p(x|\theta)=\prod_{i=1}^{N} p\left(\mathbf{x}^{(i)} | \boldsymbol{\theta}\right)$
    
  - 频**率派认为： 参数 $\theta$ 是一个未知的常量，数据 X 是随机变量 r.v ，关心的是数据，需要把参数 $\theta$ 估计出来**
    
    - 常用 **极大似然估计MLE** 

    $$
    \boldsymbol{\theta}_{\mathrm{MLE}}=\underset{\boldsymbol{\theta}}{\operatorname{argmax}} \prod_{i=1}^{N} p\left(\mathbf{x}^{(i)} | \boldsymbol{\theta}\right)=\underset{\boldsymbol{\theta}}{\operatorname{argmax}} \sum_{i=1}^{N} \ln p\left(\mathbf{x}^{(i)} | \boldsymbol{\theta}\right)
    $$
    
    - 频率派 -》统计机器学习 -〉**最终演变为 “优化” 问题**
      - 模型
      - loss function（最小二乘）
      - algorithm（梯度下降、牛顿法）
    
  - **贝叶斯派认为：参数 $\theta$ 也是一个随机变量 r.v，服从概率分布 $\theta \sim p(\theta)$，其中  $p(\theta)$ 为先验概率**
  
    - **借助贝叶斯定理，用似然把先验与后验联系起来** $P(\theta | X)=\frac{p(X | \theta) p(\theta)}{p(X)} \propto p(X | \theta) p(\theta)$
    
    - 常用 **最大后验估计MAP** 
    
    $$
    \boldsymbol{\theta}_{\mathrm{MAP}}=\underset{\boldsymbol{\theta}}{\operatorname{argmax}} \prod_{i=1}^{N} p\left(\mathbf{x}^{(i)} | \boldsymbol{\theta}\right)p(\boldsymbol{\theta})=\underset{\boldsymbol{\theta}}{\operatorname{argmax}} \sum_{i=1}^{N} \ln p\left(\mathbf{x}^{(i)} | \boldsymbol{\theta}\right)p(\boldsymbol{\theta})
    $$
    
    - 贝叶斯派 -》概率图模型 -〉**最终演变为 “求积分” 问题**
      - MCMC
      - 蒙特卡洛采样

## 系列二 数学基础

### 高斯分布（MLE与无偏vs有偏）

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/F66uwv.jpg)


- 高斯分布在**统计机器学习中占据重要地位**

- 高斯分布-极大似然估计

  - 考虑一维独立同分布向量 $X_i$

    ![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/9BeFFD.jpg)

    ![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/L3VyU5.jpg)

  - **<font color=red >解释为什么 均值为无偏估计，方差为有偏估计</font>**
    Suppose $X_1, ..., X_n$ are independent and identically distributed (i.i.d.) random variables with expectation $μ$ and variance $σ^2$. If the sample mean and uncorrected sample variance are defined as
    $$
    \mu_{MLE}=\bar{X}=\frac{1}{n} \sum_{i=1}^{n} X_{i} \\
    \sigma^2_{MLE}=S^{2}=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}
    $$
    
    $\bar{X}$ is an unbiased estimator of $\mu$ , because
    
    $$
    E\left(\bar{X} \right)=E\left(\frac{1}{N} \sum_{n=1}^{N} X_{i}\right)=\frac{1}{N} E\left(\sum_{n=1}^{N} X_{i}\right)=\frac{1}{N} \sum_{n=1}^{N} E\left(X_{i}\right)=\frac{1}{N} \cdot(N \cdot \mu)=\mu
    $$
    
    $S^2$ is a biased estimator of $σ^2$, because
    
    $$
    \begin{aligned} \mathrm{E}\left[S^{2}\right] &=\mathrm{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}\right]=\mathrm{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left(\left(X_{i}-\mu\right)-(\bar{X}-\mu)\right)^{2}\right] \\ &=\mathrm{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left(\left(X_{i}-\mu\right)^{2}-2(\bar{X}-\mu)\left(X_{i}-\mu\right)+(\bar{X}-\mu)^{2}\right)\right] \\ &=\mathrm{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}-\frac{2}{n}(\bar{X}-\mu) \sum_{i=1}^{n}\left(X_{i}-\mu\right)+\frac{1}{n}(\bar{X}-\mu)^{2} \cdot \sum_{i=1}^{n} 1\right] \\ &=\mathrm{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}-\frac{2}{n}(\bar{X}-\mu) \sum_{i=1}^{n}\left(X_{i}-\mu\right)+(\bar{X}-\mu)^{2}\right] \\ &=\mathrm{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}-\frac{2}{n}(\bar{X}-\mu) \sum_{i=1}^{n}\left(X_{i}-\mu\right)+(\bar{X}-\mu)^{2}\right] \end{aligned}
    $$
    
    To continue, we note that by subtracting $\mu$ from both sides of $\overline{X}={\frac {1}{n}}\sum _{i=1}^{n}X_{i}$ , we get
    
    $$
    \bar{X}-\mu=\frac{1}{n} \sum_{i=1}^{n} X_{i}-\mu=\frac{1}{n} \sum_{i=1}^{n} X_{i}-\frac{1}{n} \sum_{i=1}^{n} \mu=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\mu\right)
    $$
    
    Meaning, (by cross-multiplication)  $n\cdot ({\overline {X}}-\mu )=\sum _{i=1}^{n}(X_{i}-\mu )$. Then, the previous becomes:
    
    $$
    \begin{aligned} \mathrm{E}\left[S^{2}\right] &=\mathrm{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}-\frac{2}{n}(\bar{X}-\mu) \sum_{i=1}^{n}\left(X_{i}-\mu\right)+(\bar{X}-\mu)^{2}\right] \\ &=\mathrm{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}-\frac{2}{n}(\bar{X}-\mu) \cdot n \cdot(\bar{X}-\mu)+(\bar{X}-\mu)^{2}\right] \\ &=\mathrm{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}-2(\bar{X}-\mu)^{2}+(\bar{X}-\mu)+(\bar{X}-\mu)^{2}\right] \\ &=\mathrm{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}\right]-\mathrm{E}\left[(\bar{X}-\mu)^{2}\right] \\ &=\mathrm{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}\right]-\mathrm{E}\left[(\bar{X}-\mu)^{2}\right] \\ &=\sigma^{2}-\mathrm{E}\left[(\bar{X}-\mu)^{2}\right]=\left(1-\frac{1}{n}\right) \sigma^{2} < \sigma^{2} \end{aligned}
    $$
    
  - **<font color=red >Note that the usual definition of sample variance is </font>** $S^{2}={\frac {1}{n-1}}\sum _{i=1}^{n}(X_{i}-{\overline {X}}\,)^{2}$, and this is an unbiased estimator of the population variance.
  
    Algebraically speaking, ![{\displaystyle \operatorname {E} [S^{2}]}](https://wikimedia.org/api/rest_v1/media/math/render/svg/270b0ddfeee20bb27511ac518d98ef767c813bf6) is unbiased because:
    
    $$
    \begin{aligned} \mathrm{E}\left[S^{2}\right] &=\mathrm{E}\left[\frac{1}{n-1} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}\right]=\frac{n}{n-1} \mathrm{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}\right] \\ &=\frac{n}{n-1}\left(1-\frac{1}{n}\right) \sigma^{2}=\sigma^{2} \end{aligned}
    $$
  
### 高斯分布（从概率密度角度观察与其局限性）

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/c6RqZC.jpg)

- **从高纬高斯分布概率密度角度观察**
  - 马氏距离（ x 与 $\mu$ 之间）与欧式距离
  - $y_i=(x_i-\mu)^T u_i$ 的含义是：$y_i$ 是 $x_i -\mu$ 在 $u_i$ **轴上的投影**
  - 当 p=2 时，**二维的高斯分布（概率密度）可以看作是 等高线的切面（椭圆）**
  - 概率密度函数写为 $p(x)=\frac{1}{(2\pi)^{\frac{p}{2}} \cdot |\Sigma|^{\frac{1}{2}}}exp(-\frac{1}{2}\Delta)$ ，故取定一个 $p(x)$ 就相当于取定了一个等高截面
- **高斯分布的局限性**
  - **第一个局限性：参数个数为上三角矩阵元素个数 $\frac{p^2+p}{2}$，参数个数太多**
    - 因此往往假设 $\Sigma$ 为**对角矩阵**（即只有对角线有非零值，参数个数为 p 个）， $u_i$ 即为 $x_i$ ，椭圆不再倾斜
    - 当 $\Sigma$ 为**各向同性矩阵**（对角矩阵且 $\lambda_i$ 均相同），则变为正的圆（各向同性）
    - factor analysis -》对角矩阵，p-pca-〉各向同性
  - **第二个局限性：有时候用一个高斯分布不足以表达数据分布**
    - gmm中**混合模型**（多个高斯分布）

### 高斯分布（求边缘概率以及条件概率）

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/k6Ue5m.jpg)

- **求边缘概率分布 $p(X_a)$ 以及条件概率 $p(X_b|X_a)$ **
  - 对 $X,\mu,\Sigma$ **分块**，分为 a,b 两块，从而将 $X$ 看作为 a,b 的**联合概率分布**
  - 配方法（PRML）
  - **引入定理**：$X \sim N(\mu,\Sigma)$，若有$y=A \cdot X+ B$，则**结论** $y \sim N(A\mu+B,A\Sigma A^T)$
  - 构造 $X_a = AX+0(B=0)$， 得 $E(X_a)=\mu_a,Var[X_a]=\Sigma_{aa}$
  - 同理构造 $X_{b \cdot a}$ (**schur complementary**) ，得 $E(X_{b \cdot a})=\mu_{b \cdot a},Var[X_{b \cdot a}]=\Sigma_{b \cdot a}$
  
- **勘误**
- 由上图中 $x_b=x_{b \cdot a}+\Sigma_{ba} \Sigma_{aa}^{-1} x_a$ 得 $x_b｜x_a=x_{b \cdot a}|x_a+\Sigma_{ba} \Sigma_{aa}^{-1} x_a|x_a$ ，由 $x_{b \cdot a}$与 $x_a$ 为 x 的倍数同时 $\Sigma_{ba} \Sigma_{aa}^{-1} x_a$ 与 $x_a$ 为 x 的倍数，故 $x_{b \cdot a} \perp x_a,\Sigma_{ba} \Sigma_{aa}^{-1} x_a \perp x_a$ ，则 $x_b｜x_a=x_{b \cdot a}|x_a+\Sigma_{ba} \Sigma_{aa}^{-1} x_a|x_a=x_{b \cdot a}+\Sigma_{ba} \Sigma_{aa}^{-1} x_a=x_b$ 

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/VgMLcl.jpg)


### 高斯分布（求联合概率分布）[续上节]

- 求联合概率分布 $p(y),p(x|y)$ 
  - **线性高斯模型**
  - $\varepsilon$ 为高斯噪音
  - 第一步：令 $y=Ax+b+\varepsilon$ -》y的概率分布
  - 第二步：构造z，求 $\Delta$ ，从而得到**联合概率分布**，从而**套上节条件概率公式**得到 $p(x|y)$


## 系列三 线性回归

### 最小二乘法矩阵表达及其几何意义

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/gmjmAD.jpg)

- 线性回归图示（画出一条直线去拟合样本点 $D$）
  - $X$ ：数据矩阵，$Y$：值矩阵，$W$：拟合矩阵，$L$：loss function
  - 矩阵求导
  - 得到 $W$ 的解析解 $W=(X^TX)^{-1}X^TY$ 
- **几何解释**
  - **Goal: to find the line (or hyperplane) that minimizes the vertical offsets**
    - **把总误差分散在每一个样本点上，共 $N$ 段**
- **另一种解释**
  - $Y$ 在 $p$ 维空间的投影是 $x_1,x_2,...,x_p$ 的线性组合 $X\beta$
  - $Y-X\beta$ : 法向量 , 且该法向量垂直于 $p$ 维空间中每一维向量
  - **把总误差分散在每一个维度上，共 $P$ 段**



### 概率角度看最小二乘

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Ri0fKJ.jpg)

- <font color=red >**最小二乘估计 LSE**</font> $\iff$ <font color=red >**极大似然估计 MLE（nosie 为 Guassian分布）**</font>
  - 引入高斯噪声 $\varepsilon \sim N(0,\sigma^2)$ , $y=w^Tx+\varepsilon$ , $y \sim N(w^Tx,\sigma^2)$ ,其中 $w,x$ 视作参数 
  - **从概率的视角用 MLE 求出 w 的估计值 -》与最小二乘估计的形式相同**
  - 从而得到 **最小二乘估计 隐含 噪声为高斯分布的事实**

### 正则化

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/fSEg6R.jpg)

- **正则化的框架**
  -  $\underset{w}{argmax}[L(w)+\lambda P(w)]$ 
- L1(Lasso)
- **L2(Ridge) 常用**
  - 权值衰减 
  - $\hat{w}=(X^TX+\lambda I)^{-1}X^TY$ 
  - $X^TX+\lambda I$ 半正定矩阵+对角矩阵一定可逆（变为正定矩阵了）

### 贝叶斯角度看正则化

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ashD2c.jpg)

- 假设先验概率 $w \sim N(0,\sigma^2)$
- 求出 MAP
- <font color=red >**统计角度：最小二乘估计 LSE**</font> $\iff$ <font color=red >**极大似然估计 MLE（nosie 为 Guassian分布）**</font>
- <font color=red >**贝叶斯角度：带正则化的最小二乘法估计 LSE **</font>$\iff$ <font color=red >**极大后验估计 MAP（nosie 为 Guassian分布，先验也是高斯分布）**</font>

## 系列四 线性分类

### 背景

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/F2XDtr.jpg)

- **线性回归的三个特征**

  - **线性回归是统计机器学习中最重要的算法之一，是基石**

  - 线性

    - 属性线性：p维特征 $x_1,...,x_p$ 线性的，未做过特征转换（如多项式回归）

    - 全局线性：只是一个特征组合，没有再经过非线性的激活函数（如线性分类）

    - 系数线性：w，b也是线性的

  - 全局性

    - 在整个特征空间上只有一个模型，未对特征空间进行划分，再对每一段进行拟合（如线性样条回归，决策树）

  - 数据未加工

    - 用原始的数据，并未经过 PCA 降维处理

- **线性分类（打破了全局线性的特点）**
  
  - 用激活函数将线性回归的输出作映射: $f(w^T+b) \mapsto \lbrace 0,1 \rbrace$ 
  
  - **硬分类** $y \in \lbrace 0,1 \rbrace$
    - fisher 线性判别模型
    - 感知机
  - **软分类**  $y \in [0,1]$ 
    - 生成式模型：高斯判别分析 GDA
    - 判别式模型：逻辑回归

### 感知机PLA（线性分类硬分类）

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Gg2rwt.jpg)

- 古老的算法 1957年，也是深度学习中多层感知机的基础
- **思想：错误驱动**
- **模型**： $sign(a) = \left\{\begin{matrix}
  +1, a\geq 0\\ 
  -1, a < 0
  \end{matrix}\right.$
  - $f(x)=sign(w^Tx)$
  - $D$ 被错误分类的样本集合
- **策略**：loss func -》**被错误分类的点的个数**
  - 指示函数 $I: y_iw^Tx_i < 0$
  - 指示函数取值0或1，非连续函数，loss function（$L(w)=\sum I \lbrace y_iw^Tx_i \rbrace <0$）不可导
  - 故转换为 $L(w)=\sum -y_iw^Tx_i$，连续函数，可导
  - 再对其梯度用 梯度下降算法求解
- pocket algorithm
  - 如果允许犯一点错误，正负样本参杂（线性不可分）

### Fisher线性判别分析LDA 模型定义

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/PVquYm.jpg)

- **思想：类内小，类间大**
  - 把数据降维投影到一维，再分类
  - 类内小：同一个类的数据足够近，方差小
  - 类间大：不同的类间隔要大
  - 相当于 **松耦合高内聚**
- **模型转化：用数学表达式把类内类间的概念表达出来**
  - 任意样本点 $x_{i}$ 到 $w$ 轴的投影为 $z_i = w^Tx_i$
  - 得到均值与方差
  - 类间：均值差的平方 $(\overline{Z_1}-\overline{Z_2})^2$ 
  - 类内：方差的和 $S_1+S_2$
  - 从而定义**目标函数** $J(w)=\frac{(\overline{Z_1}-\overline{Z_2})^2}{S_1+S_2}$

### Fisher线性判别分析LDA 模型求解

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/wDU53W.jpg)

- 其中 $w^T S_b w, w^T S_w w$ 为实数，$(\overline{x_{c1}}-\overline{Z_{c2}})w$为两个样本均值到 w 的投影，也是一个实数
- 若 w 为对角矩阵且各向同性，则 $S_w^{-1}$ 为对角矩阵，可以进一步化简
- **找到的 w 相当于 分隔平面**

### 逻辑回归

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/TjZX0V.jpg)

- 软分类 概率判别模型 逻辑回归
  - 线性回归 $w^Tx \overset{sigmoid}{\mapsto}$ 逻辑回归 $\sigma(w^Tx)$ 
  - $p(y|x)=p_1^y p_0^{1-y} $

### 高斯判别分析GDA 模型定义

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/egqJzs.jpg)

- **软输出 高斯判别分析** 
  - 假设先验 $y \sim$ 伯努利分布
  - 假设似然 $x｜y \sim$ 高斯分布
  - 估计参数 $\theta$ 

### 高斯判别分析GDA 模型求解

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/X5Pw71.jpg)

- 求 $\varphi$
- 求 $\mu_1$

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/x5SasX.jpg)

- 运用迹的性质
  - $\sum_{i=1}^N tr[(x_i-\mu)^T \Sigma^{-1} (x_i-\mu)] =\sum_{i=1}^N tr[(x_i-\mu)^T (x_i-\mu) \Sigma^{-1}]  =N \cdot tr(S \cdot \Sigma^{-1})$

### 朴素贝叶斯分类器

- **思想：朴素贝叶斯假设（条件独立性假设）**
- 朴素贝叶斯是最简单的（有向）概率图

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/3xfVKS.jpg)

