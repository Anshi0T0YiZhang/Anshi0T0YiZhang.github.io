---
title: "「白板推导笔记」9-12"
subtitle: "Machine Learning Session"
layout: post
author: "echisenyang"
header-style: text
hidden: true
tags:
  - 笔记
  - 白板推导笔记
---



# 白板推导笔记

## 系列九 概率图模型

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/xX6jE7.jpg)

### 背景介绍 从概率角度与图角度归纳

- 法则
  - 加法法则 sum rule
  - 乘法法则 product rule
  - 链式法则 chain rule
  - 贝叶斯法则 beyesian rule
- 困境
  - 高维时，计算复杂，联合概率计算量太大
- 简化
  - Step1: 假设每一个维度之间都相对独立
    - $p(x_1,x_2,...,x_p)=\Pi_{i=1}^P p(x_i)$
    - 应用朴素贝叶斯：$p(x|y)=\Pi_{i=1}^P p(x_i|y_i)$
  - Step2: 马尔科夫性质
    - 一阶马尔可夫：在给定当前时刻的情况下，将来与过去相对独立
    - HMM中有齐次马尔可夫假设
  - **<font color=red>Step3: 条件独立性（核心性质）</font>**
    - 马尔科夫链独立条件太强
    - 大大简化运算

### 贝叶斯网络 条件独立性

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/qN7uld.jpg)



- 概率图模型可以有效的表达条件独立性
  - 找到了条件独立的性质：c 独立于 b 在 a 的条件下
  - Tail-to-tail（观测a）：
    - 若 a 被观测，则路径被 a 阻塞，b 与 c 独立
  - Head-to-tail（观测b）：
    - 若 b 被观测，则路径被 b 阻塞，a 与 c 独立
  - head-to-head（观测c）：
    - 默认情况下，a 独立于 b，路径阻塞，若 c 被观测，则路径是通的，a 与 b 不独立
    - a b 相当于陌生的男女，默认情况下无关系，但是有了孩子后，他们就有了关系
- **因子分解**
  - **$x_{\mu_i}$ 是 $x_i$ 的父亲节点的集合**



### 贝叶斯网络 D-Seperation

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/wlYb9y.jpg)

- D划分的三个规则
  - 三个集合（互无交集）
  - 三个规则分别看三种情况条件下元素是否在集合中
- 马尔可夫毯

### 贝叶斯网络 具体模型举例

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/4ieVR1.jpg)

- **<font color=red>贝叶斯网络（有向图模型）</font>**
  - **从单一到混合**
    - 单一：朴素贝叶斯
    - 混合：高斯混合模型（一般用来做聚类）
  - **从有限到无限**
    - **时间**
      - 马尔科夫链
      - 高斯过程（无限维高斯分布）
    - **空间**
      - 离散
      - 连续（高斯贝叶斯网络）

### 马尔可夫随机场 Representation 条件独立性

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/7zm6QE.jpg)

- 条件独立性
  - 全局马尔科夫性 Global Markov
  - 局部马尔可夫性
  - 成对马尔可夫性（不能相邻）
  - 条件独立性的三个方面相互等价
- 因子分解
  - 从成对马尔可夫性出发，$x_i,x_j$ 因子分解后不能在同一个因子中
  - 引入团的概念
    - 团：一个关于节点的集合，集合中的节点之间相互都是连通的
    - 最大团：在一个团里，无法再添加更多的节点
  - **<font color=red>要证明：在引入最大团后的概率分布满足条件独立性</font>**
    - $x_{c_i}$ 为最大团变量集合

### 马尔可夫随机场 Representation 因子分解

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/uXrfgG.jpg)

- **引入 energy function 将概率函数转为 Gibbs Distribution**

  **从而 马尔可夫随机场 等价于 Gibbs Distribution**

### 推断 Inference 总体介绍

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/CVqxaK.jpg)

### 推断 Inference Variable Elimination

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/EAEr9e.jpg)

- 变量消除法 VE
  - **思想：乘法对加法的分配律**
  - 先求和再相乘
- **变量消除法的缺点**
  - **没有中间过程的存储**
    - **重复计算**：即计算出 $p(d)$ 后，如果要算 $p(c)$  需要重新计算一遍
  - 找到**最优次序**是 NP-hard 问题

- 变量消除法**局限性**
  - 节点数不多，连接不复杂适用

### 推断 Inference VE to BP

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/DgHZY0.jpg)

- 引入 信念传播
  - 原因：解决重复计算问题
  - 思想：记录下 m 值，然后组合起来求概率

- **BP算法本质**
  - 对树进行遍历，让每条边各走一次，记录下 $m_{i\to j}$

### 推断 Inference Belief Propagation

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/terR4c.jpg)

- BP算法
  - Sequential Implementation
    - 递归的收集消息：递推式
  - Parallel Implementation
    - 每个节点收集消息，然后分散给临近的节点
    - 临近的节点同样收集消息，然后把消息分散给临近的节点
    - 收敛条件
- belief(b)
  - $m_{b\to a}$：从 $b\to a$ 这条路径上能送给 a 多少信息量
  - belief(b): b 点能传出多少信息量
  - 信息的传递与分发

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/m0mWKx.jpg" alt="WX20200128-103653@2x" style="zoom:50%;" /> 

### 推断 Inference Max Product

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/khyDg8.jpg)

- Max Product
  - BP算法的改进
  - 模型上看是viterbi的推广
  - 变量 x 取最好的状态，使 m 值达到最大

### 概念补充 因子图 Factor Graph

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/CoqAnl.jpg)

- 因子图
  - 引入因子节点
  - 图一、图二：因子图表达形式可能不唯一
  - 图一：因子分解本身对应一个特殊的因子图（最大团）
  - 图二：因子图看作是对因式分解的进一步分解
  - 图三：因子图是一个两层结构

### 概念补充 道德图 Moral Graph

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/2ix3Yt.jpg)

- 还有很多概念没有讲

- 道德图
  - 把有向图转为无向图
  - v 结构特殊
  - 不是一个团原因：a、b、c 不是一个团，因为 a、b 不连通
  - **作法：**
    - Step1: 将 $parent(x_i)$ 两两连接
      - **本质：找到所有的 v 结构**
    - Step2: 将有向边替换成无向边
  - **性质：如果判断有向图中节点的条件独立性困难，可以先转为无向道德图，再判断**

## 系列十 EM算法

### 算法收敛性证明

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/gpz7xX.jpg)

- 期望最大算法
  - **<font color=red>主要用来解决具有隐变量的混合模型的参数估计，也就是解决它的极大似然问题</font>**
  - **简单的**极大似然一般求出**解析解**即可
  - 但是对含有隐变量的混合模型直接求出解析解是非常困难的
  - 迭代的公式 -》 EM算法为迭代的算法
- 具体求解
  - Step1: 等式两边同时用 $\int_{z}p(z|x,\theta^{(t)})dz$ 积分
  - Step2: 考虑右边式子，记 $Q(\theta,\theta^{(t)}),H(\theta,\theta^{(t)})$
  - Step3:  关于 $Q(\theta^{(t+1)},\theta^{(t)}) \geq Q(\theta^{(t)},\theta^{(t)})$ 的不等式显然成立，因为 $\theta^{(t+1)}$ 为最大值
  - Step4: 又由于不等式 $log\ p(x|\theta^{(t)}) \leq log\ p(x|\theta^{(t+1)})$ ，故  $H(\theta^{(t+1)},\theta^{(t)}) \leq H(\theta^{(t)},\theta^{(t)})$ 
  - Step5: 求解 $H(\theta^{(t+1)},\theta^{(t)}) - H(\theta^{(t)},\theta^{(t)})$
  - 根据 log x 的性质，有 $E[log \ x] \leq log \ E[x]$ ，先 log 再期望比先期望再 log 小

### 公式导出之ELBO+KL Divergence

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Qm72J4.jpg)

- 等式成立原因

  - $p(x,z)=p(z|x)\cdot p(x)$
  - $log \ p(x)=log \ \frac{p(x,z)}{p(z|x)}$

  - 再引入 $q(z)$

- 等式两边同时关于 $q(z)$ 求积分

- 后面为 相对熵：$KL(q(z)||p(z|x,\theta))$

  - 当 $q(z)=p(z|x,\theta)$ 时取等

- 前面为 ELBO：证明下界

- EM算法思路：

  - 每次先求出 ELBO（log-likelihood 的下界，即 $log \ p(x|\theta)$），可以看作是关于后验 $p(z|x,\theta^{(t)})$ 的期望
  - 然后每次先去求这个期望
    - E step: 先固定 $\theta$ 值，然后求出后验，然后表达出期望
    - M step: 期望也是一个函数，然后针对这个期望去滑动这个 $\theta$ ，得到最大值

### 公式导出之ELBO+Jensen Inequlity

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/xrkd7A.jpg)

- step1: 引入 z，然后引入 $q(z)$
- step2: 积分可以想象为求期望，即关于分布  $q(z)$ 的期望（这个期望是关于 $\theta$ 的函数）
- Step3: 利用 Jensen Inequlity
- Step4: 左右两边同时关于 $q(z)$ 求期望
- 推出 $q(z)$ 为后验 $p(z|x,\theta)$ 

### EM算法 再回首

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/FUKaOf.jpg)

- 梯度下降GD 是解决非凸问题的迭代算法
  - EM 也是用来解决优化问题的迭代算法
- 提纲
  - 从狭义的 EM 推广到广义的 EM 
  - 狭义的 EM 是广义 EM 的一个特例
  - 介绍真正的 EM 
- **EM 是生成模型**
  - **$\theta$ 不能用 MLE 直接解，因为 根本不知道 $p(x)$ 是什么**
  - **看到的样本 x 是非常复杂的，所以引入自己的归纳偏置，也就是说假定服从某一模型分布**
  - **生成模型就是假定存在一个隐变量 z ，能够生成模型 x，因此 $p(x)$ 简化为联合概率分布 $p(x,z)$ ，再把 z 通过积分积掉即可 $p(x)=\int_zp(x,z)dz=\frac{p(x,z)}{p(z|x)}$ ，从而简化 $p(x)$ 。**

### EM算法 广义EM

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/elNFfa.jpg)

- q 可能取不到 p

### EM算法 变种

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/uTrcN8.jpg)

- 坐标上升法（SMO）角度看 EM
- **兄弟齐心其利断金 -》难兄难弟**

## 系列十一 高斯混合模型

### 模型介绍

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/M63cYI.jpg)

- 从**几何角度**看（上图）
  - **概率密度函数是由多个高斯分布叠加而成**
- 从**混合模型角度**看（下图）
  - **隐变量 z 的含义：对应的样本 x 属于哪一个高斯分布**
    - 离散随机变量
  - 高斯混合模型的生成模型
    - 概率图
    - 生成过程
      - 随机选择 p，选中一个高斯分布
      - 在这个高斯分布中随机采样，选中一个样本
      - 重复 N 次，即获得生成模型

### 高斯混合模型 极大似然

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/papfVs.jpg)

- **权重对应概率值**，从而几何角度与混合模型角度统一
- **极大似然估计无法求解高斯混合模型的解析解**
  - log 内部是连加，计算异常复杂
  - **转而求助** EM 算法
  - 注意：单一高斯模型可以用 MLE 求解

### 高斯混合模型 EM求解-E step

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/gb5DqX.jpg)

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/sNDVfP.jpg" alt="WX20200128-103653@2x" style="zoom:50%;" />

### 高斯混合模型 EM求解-M step

![WX20200128-103653@2x](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/8KASTr.jpg)

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/a6DSBk.jpg" alt="WX20200128-103653@2x" style="zoom:50%;" />

- $p_k$ 求出来了，$\mu_k, \Sigma_k$ 同理求

## 系列十二 变分推断

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/1oCmX6.jpg" />

### 背景介绍

- 变分推断是确定性近似，即可以精确计算出后验

### 变分推断 公式推导

- **基于平均场理论**

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/8NSocZ.jpg" />

- 中间推导过程

  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/UXfkGP.jpg" alt="WX20200128-103653@2x" style="zoom:50%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/VD5NWE.jpg" alt="WX20200128-103653@2x" style="zoom:50%;" />

### 变分推断 再回首

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Oa3ke5.jpg" />

- **<font color=red>变分推断问题转为最优化问题</font>**

  目标缩小到一个小样本 $x^{(i)}$ 上

  **Coordinate ascend 坐标上升法的思想**

- $Classical\ VI$ 问题

  - 假设太强
  - 对很大一部分的复杂积分 intractable

### 变分推断 SGVI

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/je04DF.jpg" />

- 随机变分推断

  - 求梯度

  - 将 1 式积分写成期望的形式，从而借助蒙特卡洛采样法求解

    - 问题：可能造成 high variance

  - 转而用 **<font color=red>重参数化技巧 Repatameterization Trick</font>**

    - 将随机性转移

    - 最后再用 蒙特卡洛采样法求解

      <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/bAdNr0.jpg" />

    - 再用 SGVI 更新参数（梯度上升方法）

      <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/5p4OsT.jpg" />

