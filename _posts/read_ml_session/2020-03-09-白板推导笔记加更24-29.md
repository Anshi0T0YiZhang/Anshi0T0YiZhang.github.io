---
title: "「白板推导笔记」24-29"
subtitle: "Machine Learning Session"
layout: post
author: "echisenyang"
header-style: text
hidden: true
catalog: true
tags:
  - 笔记
  - 白板推导笔记
---




# 白板推导笔记

## 24.Confronting Partition function（圣经18章）

### 1.The log-likelihood gradient

- 核心：把某个分布的积分写成**期望**的形式，保证至少可以计算出来（比如采用蒙特卡洛采样的方法）

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/iflU7b.jpg" style="zoom:100%" />
</p>

### 2.Stochastic Maximum Likelihood

- 对negative phase采用gibbs采样，然后用随机梯度上升的方法（ Gradient Ascent based on MCMC）
- pdata是已成事实，无法改变，目的是让pmodel接近pdata，即使 $l(\theta)$ 最大

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ooVmND.jpg" style="zoom:100%" />
</p>

### 3.What is Contrastive Divergence?

- gibbs采样，一条链经过燃烧期后达到平稳分布再采m个样本（当然也可以多条链）
- cd想法：**给定一个比较好的初始值**，从而缩小mixing time（即直接从pdata中取数进行初始化）

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/XBIAW8.jpg" style="zoom:100%" />
</p>

### 4.Name of Contrastive Divergence

- 分母添加pdata，凑出 KL 散度的形式（可以看出**极大似然估计做的就是让pdata与pmodel的KL 散度最小**）
- cd正好可以写成

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/L9yEXj.jpg" style="zoom:100%" />
</p>

### 5.RBM learning

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/CttwX9.jpg" style="zoom:100%" />
</p>

### 6.Log-likelihood gradient of RBM

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/n6A4xl.jpg" style="zoom:100%" />
</p>

### 7.CDK for RBM

- 上一步解的形式第一项close-form，第二项untractable，所以需要用cdk采样的方法解决
- 画的图为块gibbs

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Du75ZQ.jpg" style="zoom:100%" />
</p>

## 25.近似推断

### 1.介绍

- 以深度学习中的推断为侧重点
- 推断的动机是什么？
  - 推断本身：**算后验概率是对原因的一种追溯** 
  - learning问题：学习的本身会用到推断，为学习参数提供帮助
- 精确推断为什么是困难的？
  - a.玻尔兹曼机：剪不断理还乱，实践中还没有好的解法
  - b.RBM：受限降低了复杂度，后验是可解的
  - c.DBM：无向图至少两两之间相互连结，又提升了复杂度，不好解
  - d.Sigmoid Belief Network：有向图，但是由于explained away（相消原理，v型），导致h不再独立

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/eovvwv.jpg" style="zoom:100%" />
</p>

### 2.推断即优化

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/TQeaBQ.jpg" style="zoom:100%" />
</p>

## 26. Sigmoid Belief Network

### 1.背景介绍

- 贝叶斯网络网络

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/bMY4N6.jpg" style="zoom:100%" />
</p>

### 2-3.log-likelihood

- mcmc只适合小规模的网络

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/99h4Lh.jpg" style="zoom:100%" />
</p>

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Z7urwK.jpg" style="zoom:100%" />
</p>

### 4.醒眠算法（启发式迭代算法）

- wake phase：
  - bottom-up：激活神经元（获得各层样本，训练样本驱动）
  - learning generative connnection：求w
- sleep phase：
  - top-down：激活神经元（获得样本，是model中假设的样本，不是真实样本）
  - learning recognization connnection：求r

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/mBwDWf.jpg" style="zoom:100%" />
</p>

### 5-6.醒眠算法 KL Divergence

- 无法一定保证收敛，只是个启发式的算法

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/nY3hRS.jpg" style="zoom:100%" />
</p>

## 27.深度信念网络

### 1.背景介绍

- 又重新打开了连结主义的大门，干翻了svm，深度学习重新开始流行

- 上面两层是RBM，下面是Sigmoid Belief Network，所以属于hybrid model
- 为什么设计成这样啊？

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/VKLxnB.jpg" style="zoom:100%" />
</p>

### 2.叠加RBM的动机

- DBN又叫stacking RBM
- 为了提高先验 $p(h)$ 的准确度，接上一个新的 RBM 作为  $p(h)$ 的输入模型
- 另外注意，把 v->h 的方向去掉，这样的话 v 不会对 h 产生影响

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/AEyR2n.jpg" style="zoom:100%" />
</p>

### 3.叠加RBM可以提高ELBO

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/f7mrIZ.jpg" style="zoom:100%" />
</p>

### 4.贪心逐层预训练

- fine tuning-【hinton a fast learning algorithm for deep belief network】
- 逐层训练

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ofmx4K.jpg" style="zoom:100%" />
</p>

## 28.玻尔兹曼机

### 1.介绍

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/xTqOD0.jpg" style="zoom:100%" />
</p>

### 2.log似然的梯度

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/r31l4W.jpg" style="zoom:100%" />
</p>

### 3.基于MCMC的随机梯度上升

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/TBj6Mj.jpg" style="zoom:100%" />
</p>

### 4.条件概率推导

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/wvtLwF.jpg" style="zoom:100%" />
</p>

### 5-7.平均场推断

- 变分推断优化了mcmc采样，从而优化了玻尔兹曼机

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ExgapI.jpg" style="zoom:100%" />
</p>

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/56fTll.jpg" style="zoom:100%" />
</p>

## 29.深度玻尔兹曼机

### 1.背景

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/0C6Yj6.jpg" style="zoom:100%" />
</p>

### 2-4.预训练介绍

- 两层如何融合？**intuation**：做几何平均

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/2BeVGD.jpg" style="zoom:100%" />
</p>

- **double counting** 问题：$h^{(2)}$ 把 v 用了两次，会使得偏差增大（理解成double variance），使得表达出来出来的分布过于 sharp 

- 无向图：**任督二脉都打通** 

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/m8mBmK.jpg" style="zoom:100%" />
</p>

- pretraining再训练dbm的方法几乎没有人在用了，只是讲一下历史

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/PIa1Y3.jpg" style="zoom:100%" />
</p>

