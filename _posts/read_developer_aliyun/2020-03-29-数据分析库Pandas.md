---
title: "「人工智能学习路线」6"
subtitle: "数据分析库Pandas"
layout: post
author: "echisenyang"
header-style: text
hidden: true
catalog: true
tags:
  - 笔记
  - 人工智能学习路线
---

### 一、Pandas介绍

#### 1.Pandas介绍

> Pandas数据处理工具「panel+data+analysis」
>
> - 2008年WesMcKinney开发出的库
> - 专门用于数据挖掘的开源python库
> - **以Numpy为基础，借力Numpy模块在计算方面性能高的优势**
> - **基于matplotlib，能够简便的画图**
> - **独特的数据结构**

#### 2.为什么使用Pandas

> Numpy已经能够帮助我们处理数据，能够结合matplotlib解决部分数据展示等问题，那么pandas学习的目的在什么地方呢？
>
> - **便捷的数据处理能力**
> - 读取文件方便
> - 封装了**matplotlib**、**Numpy(只负责运算，但是可视化不好)**

> Pandas的重要性
>
> - 对于数据的处理，如果没有pandas，那么可能python就在数据挖掘/机器学习领域领域落后于R
>
>   **那么在说大数据可能我们可能会听过Hadoop和Spark，它们的有时是基于集群的云端处理数据，如果数据只有几GB，甚至1~2TB，那么pandas也是处理数据的最好选择**

#### 3.Pandas的数据结构

**结构：DataFrame对象既有行索引，又有列索引**

- 行索引，表明不同行，横向索引，叫index，0轴，axis=0
- 列索引，表名不同列，纵向索引，叫columns，1轴，axis=1

```python
# 增加行索引
		#构造行索引索引序列
stock_code = ['股票' + str(i) for i in range(stock_day_rise.shape[0])]
data = pd.DataFrame(stock_day_rise, index=stock_code)

# 增加列索引
		# 生成一个时间的序列，略过周末非交易日
date = pd.date_range('2017-01-01', periods=stock_day_rise.shape[1], freq='B')
		# index代表行索引，columns代表列索引
data = pd.DataFrame(stock_day_rise, index=stock_index, columns=date)
```

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/2Io6Vw.jpg)

**属性：**

- shape
- dtypes
- ndim
- **index**
- **columns**
- **values** :刨除行列索引的值，即ndarray
- **T**

#### 4.DatatFrame索引的设置

- 修改行列索引值

```python
# 修改行列索引值
data.index[499] = "0000001.SH" # 无法修改

# 通过整体修改，不能单个赋值
data.index = [i for i in range(500)]
```

> DatatFrame中要修改索引的值，只能整体修改，不能单独修改

- 重设索引

```python
data.reset_index(drop=True)
```

- 以某列值设置为新的索引

```python
df = pd.DataFrame({'month':[1,4,7,10], 'year':[1, 1, 2, 2], 'sale':[55, 40, 84, 31]})
# df.set_index(['month'])# 设置新的索引值，但是返回一个新的dataframe
df = df.set_index(['month'])
# 设置多重索引 MultiIndex的结构
df.set_index(['year', df.index])

>>> MultiIndex([(1,  1),
                (1,  4),
                (2,  7),
                (2, 10)],
               names=['year', 'month'])
```

> 注：通过刚才的设置，这样DataFrame就变成了一个具有MutiIndex的DataFrame。可以用来表示3维数据。

#### 5.Series结构

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/WVYg6b.jpg" style="zoom:50%;" />



> series结构只有行索引，是一个一维的ndarray

series获取属性和值

- index
- values

> 总结：
>
> series是带索引的一维数组
>
> dataframe是带索引的二维数组，dataframe是series的容器
>
> panel是dataframe的容器

### 二、DatatFrame基本数据操作

#### 1.索引操作

- **直接使用行列索引(必须先列后行)**

```python
data['name'][['2018-02-27']]
```

- **结合loc或者iloc使用索引(按名字索引)**

```python
# 使用loc
# loc:只能指定行列索引的名字
data.loc['2018-02-27':'2018-02-22', 'open']

# 使用iloc
# 使用iloc可以通过索引的下标去获取
data.iloc[0:100, 0:2].head()
```

- **使用ix组合索引(数字与名字混用)**

```python
# 使用ix进行下表和名称组合做引
data.ix[0:10, ['open', 'close']]
# 相当于
data[['open','close']][0:10]
```

#### 2.赋值操作

```python
# 直接修改原来的值
data['close'] = 1
# 或者
data.close = 1
```

#### 3.排序

- 使用df.sort_values对内容进行排序（默认是从小到大，升序ascending=True)
  - 单个键进行排序
  - 多个键进行排序

```python
# 按照涨跌幅大小进行排序 , 使用ascending指定按照大小排序
data = data.sort_values(by='p_change', ascending=False)

# 按照过个键进行排序
data = data.sort_values(by=['open', 'high'])
```

- 使用df.sort_index给索引进行排序

```python
# 对索引进行排序
data.sort_index()
```

### 三、DatatFrame算术运算&统计运算

- 数学运算

```python
# 进行数学运算 加上具体的一个数字
data['open'].add(1)

# 自己求出每天 close- open价格差
# 筛选两列数据
close = data['close']
open1 = data['open']
# 默认按照索引对齐
data['m_price_change'] = close.sub(open1)
```

- 使用逻辑运算符号（<、>等进行筛选，|、&完成复合的逻辑）

```python
# 进行逻辑判断
# 用true false进行标记，逻辑判断的结果可以作为筛选的依据
data[data['p_change'] > 2]

# 完成一个符合逻辑判断， p_change > 2, open > 15
data[(data['p_change'] > 2) & (data['open'] > 15)]
```

- 逻辑运算函数（`query()`,`isin()`）

```python
# 强烈推荐使用，与符合逻辑功能一样，但是更简便
data.query("p_change > 2 & open > 15")

# 可以指定值进行一个判断，从而进行筛选操作
data[data['turnover'].isin([4.19,8.88])]
data.head(10)
```

- 自定义运算函数

```python
# 进行apply函数运算
data[['open', 'close']].apply(lambda x: x.max() - x.min(), axis=0)
data[['open', 'close']].apply(lambda x: x.max() - x.min(), axis=1)
```

- 统计运算

> 综合分析: describe()

> 单个函数分析
>
> | `count`  | Number of non-NA observations                                |
> | -------- | ------------------------------------------------------------ |
> | `sum`    | **Sum of values**                                            |
> | `mean`   | **Mean of values**                                           |
> | `mad`    | Mean absolute deviation                                      |
> | `median` | Arithmetic median of values                                  |
> | `min`    | **Minimum**                                                  |
> | `max`    | **Maximum**                                                  |
> | `mode`   | Mode                                                         |
> | `abs`    | Absolute Value                                               |
> | `prod`   | Product of values                                            |
> | `std`    | **Bessel-corrected sample standard deviation**               |
> | `var`    | **Unbiased variance**                                        |
> | `idxmax` | compute the index labels with the maximum相当于`np.argmax()` |
> | `idxmin` | compute the index labels with the minimum相当于`np.argmin()` |

> 累计统计分析函数
>
> | 函数      | 作用                        |
> | --------- | --------------------------- |
> | `cumsum`  | **计算前1/2/3/…/n个数的和** |
> | `cummax`  | 计算前1/2/3/…/n个数的最大值 |
> | `cummin`  | 计算前1/2/3/…/n个数的最小值 |
> | `cumprod` | 计算前1/2/3/…/n个数的积     |

### 四、Pandas画图

#### pandas.DataFrame.plot[¶](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html#pandas-dataframe-plot)

> Parameters
>
> - **data**: Series or DataFrame
>
>   The object for which the method is called.
>
> - **x**: label or position, default None
>
>   Only used if data is a DataFrame.
>
> - **y**: label, position or list of label, positions, default None
>
>   Allows plotting of one column versus another. Only used if data is a DataFrame.
>
> - **kind**: str
>
>   The kind of plot to produce:
>
>   ‘line’ : line plot (default)
>
>   ‘bar’ : vertical bar plot
>
>   ‘barh’ : horizontal bar plot
>
>   ‘hist’ : histogram
>
>   ‘box’ : boxplot

```python
import matplotlib.pyplot as plt
# 对dataframe中的series可以直接用matplotlib画图
stock_rise.sort_index().cumsum().plot()
plt.show()

# 对dataframe也可以直接画图
stock_rise.plot(x='open', y='close',kind="scatter")
```

### 五、文件的读取与存储

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ctqAii.jpg)

> 我们的数据大部分存在于文件当中，所以pandas会支持复杂的IO操作，pandas的API支持众多的文件格式，如CSV、SQL、XLS、JSON、HDF5。

- read_csv与to_csv

```python
# 读取文件
data = pd.read_csv("./data/stock_day/stock_day.csv", usecols=['open', 'close'], names=["col1","col2",...], engine="python")

data[:10].to_csv("./test.csv", columns=['open'], index=False, mode='a', header=False)
```

- read_hdf与to_hdf

```python
close = pd.read_hdf("./data/stock_plot/day_close.h5",key=None)

a = close[['000001.SZ', '000002.SZ']]

a.to_hdf("./test.h5", key="close")
# 当有多个key时，用read_hdf必须指定key，因为此时的hdf5文件相当于是一个三维数据库
b = pd.read_hdf("./test.h5", key="x")
```

> **优先选择使用hdf文件存储**
>
> - hdf在存储的是支持压缩，**使用的方式是blosc，这个是速度最快**的也是pandas默认支持的
> - 使用压缩可以**提磁盘利用率，节省空间**
> - **hdf还是跨平台的**，可以轻松迁移到hadoop 上面

- read_json与to_json

```python
pd.read_json(path=,orient="records",lines="是否按行读取,默认false")

pd.to_json(path,orient="records",lines=True)
```

### 6.高阶-缺失值处理

> 如何处理nan?
>
> 1）判断数据是否为NaN：`pd.isnull(df)`, `pd.notnull(df)`
>
> 2）存在缺失值nan,并且是np.nan:
>
> - 1、删除存在缺失值的:`pd.dropna(axis='rows',inplace=False)`
> - 2、替换缺失值:`pd.fillna(df[].mean(), inplace=True)`
>
> 不是缺失值nan，有默认标记的
>
> ```python
> # 把一些其它值标记的缺失值，替换成np.nan
> wis = wis.replace(to_replace='?', value=np.nan)
> wis.dropna()
> ```

### 7.高阶-数据离散化

> 连续属性离散化的目的是为了简化数据结构，**数据离散化技术可以用来减少给定连续属性值的个数**。离散化方法经常作为数据挖掘的工具。

> **连续属性的离散化就是将连续属性的值域上，将值域划分为若干个离散的区间，最后用不同的符号或整数** **值代表落在每个子区间中的属性值。**
>
> *离散化有很多种方法，这使用一种最简单的方式去操作*
>
> - *原始人的身高数据：165，174，160，180，159，163，192，184*
> - *假设按照身高分几个区间段：150~165, 165~180,180~195*
>
> *这样我们将数据分到了三个区间段，我可以对应的标记为矮、中、高三个类别，最终要处理成一个"哑变量"矩阵*

> 如何实现离散化？
>
> 1）分组
>
> ```python
> p_change= data['p_change']
> # 自动分组
> qcut = pd.qcut(np.abs(p_change), 10)
> qcut.value_counts()
> 
> # 自己指定分组区间
> bins = [-100, -7, -5, -3, 0, 3, 5, 7, 100]
> p_counts = pd.cut(p_change, bins)
> ```
>
> 2）将分组好的结果转换为one-hot编码
>
> ```python
> dummaries = pd.get_dummies(p_counts, prefix="身高")
> ```

### 8.高阶-合并

- `pd.concat()`

  行索引一致，按照行进行合并：`pd.concat([data, dummaries], axis=1)`

- `pd.merge()`

> pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None,left_index=False, right_index=False, sort=True,suffixes=('_x', '_y'), copy=True, indicator=False,validate=None)
>
> - 可以指定按照两组数据的共同键值对合并或者左右各自
> - `left`: A DataFrame object
> - `right`: Another DataFrame object
> - `on`: Columns (names) to join on. Must be found in both the left and right DataFrame objects.
> - left_on=None, right_on=None：指定左右键
>
> | Merge method | SQL Join Name      | Description                               |
> | ------------ | ------------------ | ----------------------------------------- |
> | `left`       | `LEFT OUTER JOIN`  | Use keys from left frame only             |
> | `right`      | `RIGHT OUTER JOIN` | Use keys from right frame only            |
> | `outer`      | `FULL OUTER JOIN`  | Use union of keys from both frames        |
> | `inner`      | `INNER JOIN`       | Use intersection of keys from both frames |

```python
# 默认内连接：只保留共有主键
result = pd.merge(left, right, on=['key1', 'key2'])
```

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/jqdB0S.jpg)

```python
# 左连接：左表指定主键都保留,右表配合左表
result = pd.merge(left, right, how='left', on=['key1', 'key2'])
```

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/978YbT.jpg)

```python
# 右连接：右表指定主键都保留,左表配合右表
result = pd.merge(left, right, how='right', on=['key1', 'key2'])
```

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Q5PZaQ.jpg)

```python
# 外链接：所有主键都保留
result = pd.merge(left, right, how='outer', on=['key1', 'key2'])
```

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/NR22if.jpg)

### 9.高阶-交叉表与透视表

> 交叉表：交叉表用于计算一列数据对于另外一列数据的分组个数(寻找两个列之间的关系)
>
> - pd.crosstab(星期数据列, 涨跌幅数据列)

```python
# 寻找星期几跟股票张得的关系
# 1、先把对应的日期找到星期几
date = pd.to_datetime(data.index).weekday
data['week'] = date
# 2、假如把p_change按照大小去分个类0为界限
data['posi_neg'] = np.where(data['p_change'] > 0, 1, 0)

# 通过交叉表找寻两列数据的关系
count = pd.crosstab(data['week'], data['posi_neg'])

# 转换为比例
# 算数运算，先求和
count.sum(axis=1).astype(np.float32)
pro = count.div(count.sum(axis=1).astype(np.float32), axis=0)

# 使用plot画出这个比例，使用一直stacked的柱状图
pro.plot(kind='bar', stacked=True)
plt.show()
```

> 使用透视表，刚才的过程更加简单
>
> - DataFrame.pivot_table([], index=[])

```python
# 通过透视表，将整个过程变成更简单一些
data.pivot_table(['posi_neg'], index=['week'])
```

### 10.高阶-分组与聚合

> **分组与聚合通常是分析数据的一种方式，通常与一些统计函数一起使用，查看数据的分组情况,聚合是分组后的统计结果**
>
> 想一想其实刚才的交叉表与透视表也有分组的功能，所以算是分组的一种形式，只不过他们主要是计算次数或者计算比例！！

```python
# 对dataframe进行分组，求平均值
col.groupby(['color'])['price1'].mean()

# 对series进行分组，求平均值
col['price1'].groupby(col['color']).mean()

# 分组，数据的结构不变
col.groupby(['color'], as_index=False)['price1'].mean()
```

```python
# 设置多个索引，set_index()
starbucks.groupby(['Country', 'State/Province']).count()
```

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/srbvsT.jpg)



