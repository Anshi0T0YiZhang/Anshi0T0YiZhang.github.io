---
title: "Word2vec"
subtitle: "TF EXAMPLES「2.7」"
layout: post
author: "echisenyang"
header-style: text
hidden: true
catalog: true
tags:
  - TensorFlow
---

#### step1: word2id、id2word

- step1: 按词频构建count字典
- step2: count字典中对词频小于10的词直接剔出，次数从 50000->47135
- step3: 为每个词创建字典 **word2id** {word:id}
- step4: 将text_words根据word2id转为id的序列 **data**
- step5: 创建字典 **id2word** {id:word}

```python
data_path = '/home/jiale/codes/2020gogogo/text8.zip'
with zipfile.ZipFile(data_path) as f:
    text_words = f.read(f.namelist()[0]).lower().split()

# step1: 按词频构建count字典
count = [('UNK', -1)]
# Retrieve the most common words
count.extend(collections.Counter(text_words).most_common(max_vocabulary_size - 1))
"""
collections.Counter().most_common() 词频最高的前50000个词
"""
# Remove samples with less than 'min_occurrence' occurrences
# step2: 对词频小于10的词直接剔出，次数从 50000->47135
for i in range(len(count) - 1, -1, -1):
    if count[i][1] < min_occurrence:
        count.pop(i)
    else:
        # The collection is ordered, so stop when 'min_occurrence' is reached
        # i从50000逆序到47134，退出，从后往前遍历可以省不少时间
        break
        
# Compute the vocabulary size
vocabulary_size = len(count) # 47135
# Assign an id to each word
# step3: 为每个词创建 字典{id:word}
word2id = dict()
for i, (word, _)in enumerate(count):
    word2id[word] = i
"""    
Python 字典(Dictionary) get() 函数返回指定键的值，如果值不在字典中返回默认值
"""
# step4: 将text_words根据word2id转为id的序列
for word in tqdm(text_words):
    # Retrieve a word id, or assign it index 0 ('UNK') if not in dictionary
    index = word2id.get(word, 0)
    if index == 0:
        unk_count += 1
    data.append(index)
# step5: 创建字典id2word
id2word = dict(zip(word2id.values(), word2id.keys()))
```

![Tensorflow-10](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Tensorflow-10.jpg)

#### step2: next_batch

生成batch[128,1]与labels[128,1]

- 其中bacth为以data_index开头的滑动窗口 [0,1,2,3,4,5,6] 中中心词 [3]，每个batch_size中均为同一个词
- 其中labels为以data_index开头的滑动窗口 [0,1,2,3,4,5,6] 中环境词  [0,1,2,4,5,6] 的随机采样，每个batch_size中为不一定相同的环境词

```python
data_index = 0
def next_batch(batch_size=128, num_skips=2, skip_window=3):
    global data_index
    assert batch_size % num_skips == 0
    assert num_skips <= 2 * skip_window
    batch = np.ndarray(shape=(batch_size), dtype=np.int32)
    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)
    # get window size (words left and right + current one)
    span = 2 * skip_window + 1 #7
    # FIFO：buffer的大小恒定为7，相当于一个滑动窗口
    buffer = collections.deque(maxlen=span)
    if data_index + span > len(data):
        data_index = 0
    buffer.extend(data[data_index:data_index + span])
    data_index += span
    for i in range(batch_size // num_skips):
        # [0,1,2,4,5,6]
        context_words = [w for w in range(span) if w != skip_window]
        # 从context_words中采样num_skips次
        words_to_use = random.sample(context_words, num_skips)
        for j, context_word in enumerate(words_to_use):
            # 对同一个data_index，bacth中存储的是同一个中心词
            batch[i * num_skips + j] = buffer[skip_window]
            # 对同一个data_index，labels中存储的是随机采样的环境词（6选2）
            labels[i * num_skips + j, 0] = buffer[context_word]
        if data_index == len(data):
            # 如果用完data_index，则重头开始，保证next_batch一直可以生成数据
            buffer.extend(data[0:span])
            data_index = span
        else:
            # 否则，滑动窗口取下一批数据，偏移一位
            buffer.append(data[data_index])
            data_index += 1
    # Backtrack a little bit to avoid skipping words in the end of a batch
    data_index = (data_index + len(data) - span) % len(data)
    return batch, labels
```

![Tensorflow-11](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Tensorflow-11.jpg)

#### step3: 构图

```python
# Input data
X = tf.placeholder(tf.int32, shape=[None])
# Input label
Y = tf.placeholder(tf.int32, shape=[None, 1])

# Ensure the following ops & var are assigned on CPU
# (some ops are not compatible on GPU)
with tf.device('/cpu:0'):
    # Create the embedding variable (each row represent a word embedding vector)
    embedding = tf.Variable(tf.random_normal([vocabulary_size, embedding_size]))
    # Lookup the corresponding embedding vectors for each sample in X
    X_embed = tf.nn.embedding_lookup(embedding, X)

    # Construct the variables for the NCE loss
    nce_weights = tf.Variable(tf.random_normal([vocabulary_size, embedding_size]))
    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))
```

![Tensorflow-12](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Tensorflow-12.jpg)

#### step4: nce损失函数

```python
# Compute the average NCE loss for the batch
loss_op = tf.reduce_mean(
    tf.nn.nce_loss(weights=nce_weights,
                   biases=nce_biases,
                   labels=Y,
                   inputs=X_embed,
                   num_sampled=num_sampled,
                   num_classes=vocabulary_size))
```

![sqmUYP](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/sqmUYP.png)

![Tensorflow-14](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Tensorflow-14.jpg)



Tensorflow实现了两种常用与word2vec的loss，sampled softmax和NCE，这两种loss本身可以用于任意分类问题。

**两个方法的目标是在分类的数量太大时，简化softmax计算，采取估算的方法得到softmax的值。**

- 问题：

通常训练例如word2vec的时候，**我们最后用full softmax预测出下一个词，真实的值通常是一小部分context words,也就是一小部分target classes，在非常大的语料库中(通常维度为百万），softmax需要对每一个class ![[公式]](https://www.zhihu.com/equation?tex=y%5Cin+L+) 预测出probability，那么|L|非常大的时候，这个计算量就非常大**。

我们可不可以针对这么昂贵的计算 进行优化，不计算所有class的probability，但同时给我们一个合理的loss？ 这里就引入了NCE（Noise-contrastive estimation)：

- 对于每一个训练样本（x, T)，我们训练**binary classification（1:是neighbour，0:不是neighbour是噪声）**，而不是multiclass classification。具体一点，我们**对于每个样本，拆分成一个真实的（x,y)pair,另外我们随机产生k个Noise的（x,y）pair,这样我们就可以用来训练处这样的binary classifier**。
- **将给定当前词，预测下一个词的问题转换为给定（当前词，环境词/噪声）判断这两个词是否为neighbours**

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/DCSLFg.png" alt="DCSLFg" style="zoom:50%;" />

![hPGMEV](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/hPGMEV.png)

NCE的实现就由这三部分组成，从名字上看，

- \_compute_sampled_logits 负责采样
  - 该函数最后返回两个结果，out_logits *shape是[batch_*size, num_*true+num_*sampled]，num_*true 默认是1(后面都当1进行说明)，num_sampled 是*我们每个 batch 需要采样的负样本的个数，out*labels shape 和 out_logits 一样，只是前 num_true 列值是1(正样本)，后面列的值都是0(负样本).
- sigmoid_cross_entropy_with_logits 负责做 logistic regression
- 然后计算用cross entropy loss，_sum_rows求和
  - 计算完sigmoid_cross_entropy_with_logits后，接tf.reduce_sum(x, 1) ，相当于只保留batchsize维度的信息

#### step5: optimizer & Evaluation

```python
# Define the optimizer
optimizer = tf.train.GradientDescentOptimizer(learning_rate)
train_op = optimizer.minimize(loss_op)

# Evaluation
# Compute the cosine similarity between input data embedding and every embedding vectors
X_embed_norm = X_embed / tf.sqrt(tf.reduce_sum(tf.square(X_embed)))
embedding_norm = embedding / tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keepdims=True))
cosine_sim_op = tf.matmul(X_embed_norm, embedding_norm, transpose_b=True)

"""
normonization详解
假设X_embed = np.array([[0.5, 0.3, 0.2],
                   [0.1, 0.8, 0.1]], dtype=np.float32)
tf.square(X_embed) = [[0.25       0.09       0.04      ]
 					  [0.01       0.64000005 0.01      ]]
tf.reduce_sum(tf.square(X_embed)) = 1.04
tf.sqrt(tf.reduce_sum(tf.square(X_embed))) = 1.0198039
X_embed_norm = [[0.49029034 0.29417422 0.19611615]
			    [0.09805807 0.7844646  0.09805807]]
"""
```

#### step6: 初始化并传值运行

```python
# Initialize the variables (i.e. assign their default value)
init = tf.global_variables_initializer()

with tf.Session() as sess:

    # Run the initializer
    sess.run(init)

    # Testing data
    x_test = np.array([word2id[w] for w in eval_words])

    average_loss = 0
    for step in range(1, num_steps + 1):
        # Get a new batch of data
        batch_x, batch_y = next_batch(batch_size, num_skips, skip_window)
        # Run training op
        _, loss = sess.run([train_op, loss_op], feed_dict={X: batch_x, Y: batch_y})
        average_loss += loss

        if step % display_step == 0 or step == 1:
            if step > 1:
                average_loss /= display_step
            print("Step " + str(step) + ", Average Loss= " + \
                  "{:.4f}".format(average_loss))
            average_loss = 0

        # Evaluation
        if step % eval_step == 0 or step == 1:
            print("Evaluation...")
            sim = sess.run(cosine_sim_op, feed_dict={X: x_test})
            for i in range(len(eval_words)):
                top_k = 8  # number of nearest neighbors
                nearest = (-sim[i, :]).argsort()[1:top_k + 1]
                log_str = '"%s" nearest neighbors:' % eval_words[i]
                for k in range(top_k):
                    log_str = '%s %s,' % (log_str, id2word[nearest[k]])
                print(log_str)
```



