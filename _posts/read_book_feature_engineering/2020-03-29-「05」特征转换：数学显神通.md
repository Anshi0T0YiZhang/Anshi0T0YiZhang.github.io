---
title: "「05」特征转换：数学显神通"
subtitle: "《特征工程入门与实践》"
layout: post
author: "echisenyang"
header-style: text
hidden: false
catalog: true
tags:
  - book
  - 《特征工程入门与实践》
---



## 特征转换：数学显神通

> 到目前为止，我们似乎已经从数据的所有角度应用了特征工程工具。从通过分析表格数据以 确定数据的等级，到通过统计方法构建并选择列以优化机器学习流水线，我们为处理数据中的特 征做了很多了不起的事。

> 在前面的 5 章中，我们了解了所谓的经典特征工程。目前，我们已经讨论了特征工程的 5 个 主要类别/步骤。
>
> 1.**探索性数据分析**：在应用机器学习流水线，甚至在使用机器学习算法或特征工程工具之 前，我们理应对数据集进行一些基本的描述性统计，并进行可视化操作，以便更好地理解数据的性质。 
>
> 2.**特征理解**：在了解了数据的大小和形状后，应该进一步仔细观察数据集的每一列（如果 有可能的话）和大致特点，包括数据的等级，因为这会决定如何清洗数据。 
>
> 3.**特征增强**：这个阶段是关于改变数据值和列的，我们根据数据的等级填充缺失值，并按 需执行虚拟变量转换和缩放操作。 
>
> 4.**特征构建**：在拥有可以得到的最好数据集之后，可以考虑构建新的列，以便理解特征交互情况。 
>
> 5.**特征选择**：在选择阶段，用所有原始和新构建的列进行（通常是单变量）统计测试，选 取性能最佳的特征，以消除噪声影响、加速计算。

> ❗️ **本章会涉及特征转换， 这是一套改变数据内部结构的算法， 以产生数学上更优的超级列 （super-column）。**下一章则重点介绍使用非参数方法（不依赖数据的形状）的特征学习，以自动 学习新的特征。本书的最后一章是几个经过细致研究的案例，旨在展示特征工程的端到端过程， 以及特征工程对机器学习流水线的影响。

> ‼️ 这两个摄像头就是特征转换形成的新维度，以一种新的方式捕获数据，但是只需要两列而非三列 就可以提供足够的信息。在特征转换中，最棘手的部分是一开始就不认为原始特征空间是最好的。 **我们需要接受一个事实：可能有其他的数学坐标轴和系统能用更少的特征描述数据，甚至可以描述得更好。**

### 1）维度缩减：特征转换、特征选择与特征构建

> ‼️ 刚才，我们提到了如何压缩数据集，用全新的方法以更少的列描述数据。听起来和特征选择 的概念很类似：从原始数据集中删除列，通过消除噪声和增强信号列来创建一个不同而且更好的 数据集。**虽然特征选择和特征转换都是降维的好办法，但是它们的方法迥然不同。**
>
> - 特征选择仅限于从原始列中选择特征；特征转换算法则将原始列组合起来，从而创建可以更 好地描述数据的特征。因此，**特征选择的降维原理是隔离信号列和忽略噪声列**。
> - **特征转换方法使用原始数据集的隐藏结构创建新的列，生成一个全新的数据集，结构与之前不同**。这些算法创建的新特征非常强大，只需要几个就可以准确地解释整个数据集。
> - **特征转换的原理是生成可以捕获数据本质的新特征**，这一点和特征构造的本质类似：都是创 建新特征，捕捉数据的潜在结构。需要注意，这两个不同的过程方法截然不同，但是结果类似。
> - 特征转换算法可以选择最佳的列，将其与几个全新的列进行组合，从而构建新的特征。我们 可以认为，**特征转换在本书中最强大的算法之列**。

### 2）主成分分析

> 主成分分析（PCA，principal components analysis）是**将有多个相关特征的数据集投影到相 关特征较少的坐标系上**。这些**新的、不相关的特征（之前称为超级列）叫主成分**。主成分能替代原始特征空间的坐标系，需要的特征少、捕捉的变化多。在摄像头的例子中，主成分就是摄像头本身。

> ⚠️ PCA 本身是无监督任务，意思是 PCA 不使用响应列进行投影/转换。

- 手动计算 PCA

```python
# 1. 创建数据集的协方差矩阵
	# 导入 NumPy 
import numpy as np

	# 计算均值向量 
mean_vector = iris_X.mean(axis=0) print(mean_vector)

	# 计算协方差矩阵 
cov_mat = np.cov((iris_X).T) print(cov_mat.shape)

# 2. 计算协方差矩阵的特征值
	# 计算鸢尾花数据集的特征向量和特征值 
eig_val_cov, eig_vec_cov = np.linalg.eig(cov_mat)

	# 按降序打印特征向量和相应的特征值 
for i in range(len(eig_val_cov)):
		eigvec_cov = eig_vec_cov[:,i] 
    print('Eigenvector {}: \n{}'.format(i+1, eigvec_cov)) 
    print('Eigenvalue {} from covariance matrix: {}'.format(i+1, eig_val_cov[i])) print(30 * '-')
    
# 3. 按降序保留前 k 个特征值
	# 每个主成分解释的百分比是特征值除以特征值之和 
explained_variance_ratio = eig_val_cov/eig_val_cov.sum() 

# 碎石图可视化
plt.plot(np.cumsum(explained_variance_ratio)) 
plt.title('Scree Plot') 
plt.xlabel('Principal Component (k)') 
plt.ylabel('% of Variance Explained <= k')
```

> 我们有 4 个特征值，需要选择合适的数量进行保留。如果我们愿意，可以保留完整的 4 个， 但是一般希望选择的比原始特征数更少。多少合适呢？虽然可以进行暴力搜索，但是我们的工具 库中有一个新工具——**碎石图（scree plot）**。
>
> **碎石图是一种简单的折线图，显示每个主成分解释数据总方差的百分比**。要绘制碎石图，需 要对特征值进行降序排列，绘制每个主成分和之前所有主成分方差的和。在鸢尾花数据集上，我们的碎石图有 4 个点，每个点代表一个主成分。每个主成分解释了总方差的某个百分比，相加后， 所有主成分应该解释了数据集中总方差的 100%。

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/BHIvdQ.png)

```python
# 4. 使用保留的特征向量转换新的数据点
	# 保存两个特征向量 
top_2_eigenvectors = eig_vec_cov[:,:2].T

  # 将数据集从 150 × 4 转换到 150 × 2 
  # 将数据矩阵和特征向量相乘
np.dot(iris_X, top_2_eigenvectors.T)
```

> 我们将四维的鸢尾花数据集转换成了只有两列的新矩阵，而这个新矩阵可以 在机器学习流水线中代替原始数据集。

- scikit-learn 的 PCA

```python
# scikit-learn 的 PCA 
from sklearn.decomposition import PCA

# 和其他 scikit 模块一样，先实例化 
pca = PCA(n_components=2)

# 在数据上使用 
PCA pca.fit(iris_X)

# 用 PCA对象的 transform 方法，将数据投影到新的二维平面上
pca.transform(iris_X)

# 每个主成分解释的方差量 
# 和之前的一样 
pca.explained_variance_ratio_
>>> array([0.92461621, 0.05301557])
```

> 注意，这里投影后的数据和之前不同，因为 scikit-learn 的 PCA 会在预测阶段自 动将数据中心化，从而改变结果。

#### 中心化和缩放对 PCA 的影响

> 在实践和生产环境下，我们会建议进行缩放，但应该在缩放和未缩放的数据上都进行性能测试。

### 3）线性判别分析

> 线性判别分析（LDA，linear discriminant analysis）是特征变换算法，也是有监督分类器。LDA 一般用作分类流水线的预处理步骤。和 PCA 一样，LDA 的目标是提取一个新的坐标系，将原始 数据集投影到一个低维空间中。和 PCA 的主要区别在于，LDA 不会专注于数据的方差，而是优 化低维空间，以获得最佳的类别可分性。意思是，新的坐标系在为分类模型查找决策边界时更有用，非常适合用于构建分类流水线。

> LDA 极为有用的原因在于，**基于类别可分性的分类**有助于避免机器学习流水线 的过拟合，也叫防止维度诅咒。LDA 也会降低计算成本。

> ⭕️ LDA 等有监督特征转换的局限性在于，不能像 PCA 那样处理聚类任务。这是因 为聚类是无监督的任务，没有 LDA 需要的响应变量。

> ‼️ **最好的准确率（接近 99%）结合了缩放、PCA 和 LDA**。在流水线中结合使用这 3 种算法并 且用超参数进行微调是很常见的。因此，在生产环境下，最好的机器学习流水线实际上是多种特 征工程工具的组合。

#### 小结

> PCA 和 LDA 都是很强大的工具，但也有局限性。这两个工具都是线性转换，所以**只能创建 线性的边界，表达数值型数据**。它们也是静态转换。无论输入什么，LDA 和 PCA 的输出都是可 预期的，而且是数学的。如果数据不适合 PCA 或 LDA（**数据有非线性特征**，例如是圆形的）， 那么无论我们怎么进行网格搜索，这些算法都不会有什么帮助。

