---
title: "「06」特征学习：以 AI 促 AI "
subtitle: "《特征工程入门与实践》"
layout: post
author: "echisenyang"
header-style: text
hidden: true
catalog: true
tags:
  - book
  - 《特征工程入门与实践》
---



## 特征学习：以 AI 促 AI 

> 本章概要：
>
> 1）数据的参数假设； 
>
> 2）受限玻尔兹曼机（RBM，restricted Boltzmann machine）； 
>
> 3）伯努利受限玻尔兹曼机（BernoulliRBM）； 
>
> 4）在机器学习流水线中应用 RBM； 
>
> 5）学习文本特征——词向量。

### 1）数据的参数假设

> 🚫 参数假设是指算法对数据形状的基本假设。在上一章中当探索主成分分析（PCA）时，我们 发现可以利用算法的结果产生主成分，通过矩阵乘法来转换数据。我们的假设是，原始数据的形 状可以进行（特征值）分解，并且可以用单个线性变换（矩阵计算）表示。**但如果不是这样呢？ 如果 PCA 不能从原始数据集中提取有用的特征，那该怎么办呢？**PCA 和线性判别分析（LDA） 这样的算法肯定能找到特征，但找到的特征不一定有用。**此外，这些算法都基于预定的算式，每次肯定输出同样的特征。这也是我们将 PCA 和 LDA 都视为线性变换的原因。**

> **特征学习算法希望可以去除这个参数假设，从而解决该问题。**这些算法不会对输入数据的形状有任何假设，而是依赖于随机学习（stochastic learning）。意思是，这些算法并不是每次输出相 同的结果，而是一次次按轮（epoch）检查数据点以找到要提取的最佳特征，并且拟合到一个解 决方案（在运行时可能会有所不同）。

> ❗️这样，特征学习算法可以绕过 PCA 和 LDA 等算法的参数假设，解决比之前更难的问题。这 种复杂的想法（绕过参数假设）需要使用复杂的算法。很多数据科学家和机器学习流水线会**使用 深度学习算法，从原始数据中学习新特征。**

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ZP9xT2.png)

> 本章中我们重点关注以下两个特征学习领域
>
> - **受限玻尔兹曼机（RBM）**：一种简单的深度学习架构，根据数据的概率模型学习一定数量 的新特征。这些机器其实是一系列算法，但 scikit-learn 中只实现了一种。**BernoulliRBM** 可以作为非参数特征学习器，但是顾名思义，这个算法对单元格有一些假设。 
> - **词嵌入**：可以说是深度学习在自然语言处理/理解/生成领域最近进展的主要推动者之一。 词嵌入可以将字符串（单词或短语）投影到 n 维特征集中，以便理解上下文和措辞的细 节。我们用 Python 的 gensim 包准备词嵌入，然后借助预训练过的词嵌入来研究它能如 何增强我们与文本的交互能力。

### 2）受限玻尔兹曼机

> RBM 是一组**无监督**的特征学习算法，使用概率模型学习新特征。**与 PCA 和 LDA 一样，我 们可以使用 RBM 从原始数据中提取新的特征集，用于增强机器学习流水线**。在 RBM 提取特征 之后使用线性模型（线性回归、逻辑回归、感知机等）往往效果最佳。

- 不一定降维

> RBM 可以学习的特征数量只受限于计算机的计算能力，以及人为的解释。**RBM 可以学习到比初始输入更少或更多的特征**。具体要学习的特征数量取决于要解决的问题，可以进行网格 搜索。

- 玻尔兹曼机的限制

> 我们看见了层与层之间的连接（层间连接），**但是没有看见同一层内节点的连接 （层内连接）**。这是因为没有这种连接。**RBM 的限制是，不允许任何层内通信。这样，节点可以 独立地创造权重和偏差，最终成为（希望是）独立的特征**。

- 数据重建

> RBM 用这种方式进行自我评估。通过将激活信息进行后向传导并获取原始输入的近似值， 该网络可以调整权重，让近似值更接近原始输入。在训练开始时，由于权重是随机初始化的（标准做法），近似值有可能相差很大。然后，通过反向传播（和前向传导的方向相同，的确很绕） 调整权重，最小化原始输入和近似值的距离。我们重复这个过程，直到近似值尽可能接近原始的 输入。这个过程发生的次数叫作迭代次数。

### 3）伯努利受限玻尔兹曼机

> scikit-learn 中唯一的 RBM 实现是 BernoulliRBM，因为它对原始数据的范围进行了约束。伯 努利分布要求数据的值为 0～1。scikit-learn 的文档称，该模型假定输入是二进制的值，或者是 0~1 的数。这个限制是为了表示节点的值就是节点被激活的概率，从而可以更快地学习特征集。

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Vc43aJ.png" style="zoom:33%;" />

> 可以看见，图像中的模糊消失了，要分类的数字很清晰。现在我们开始从数字数据集中提取 特征。

- 从 MNIST 中提取 PCA 主成分

<img src="/Users/yangjiale/Library/Application Support/typora-user-images/image-20200329201807037.png" style="zoom:40%;" />

> ❌ 很明显，前 10 个主成分似乎保留了一点数字的形状，之后图像好像没什么意义了。最后， 我们看到的好像只是黑白像素随机混合的结果。这有可能是因为，**PCA（和 LDA）是参数变换， 从图像等复杂数据集（如图像）中提取信息的能力有限。**

### 4）在机器学习流水线中应用 RBM

> 逻辑回归本身的效果就很好，用原始数据就达到了 89.08%的交叉验证准确率。

> 结果是 88.77%的交叉验证准确率，稍差一点。如果进行了思考，那么 100 比 10 和 200 表现 更好并不令人惊讶。在上一节的碎石图中，30 个主成分解释了 64%的变化，所以 10 个主成分肯 定不足以解释图像。在 100 个主成分后，碎石图开始变平，代表在第 100 个主成分后，解释的方 差没有增加太多。因此 200 个主成分太多了，会导致过拟合。综上所述，100 个 PCA 主成分是最 佳数量。

> RBM模块的交叉验证准确率是 91.57%，能从数字中提取 200个新特征。除了引入 BernoulliRBM 模块外，不进行任何操作就可以增加近 2.5%的准确率。（很多了！）

### 5）学习文本特征：词向量

> 小结：
>
> 这两种工具都使用深度学习架构从原始数据中学习新的特征集。两种技术都用较浅的网络来 优化训练时间，并且用拟合阶段学到的权重和偏差来提取数据的潜在结构。



