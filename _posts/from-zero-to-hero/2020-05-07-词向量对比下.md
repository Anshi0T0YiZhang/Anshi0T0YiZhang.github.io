---
title: "词向量对比「下」"
subtitle: "word2vec/glove/fastText/elmo/GPT/bert"
layout: post
author: "echisenyang"
header-style: text
hidden: false
catalog: true
tags:
  - 输出计划
---





### 基于词向量的动态表征 (动态词向量)

#### ELMo

ELMo与word2vec最大的不同：

- Contextual: The representation for each word depends on the entire context in which it is used.　（即词向量不是一成不变的，而是根据上下文而随时变化，elmo属于动态词向量（上下文词向量）这与word2vec或者glove具有很大的区别）

- **什么是一个好的词向量**
  - 能够反映出语义和语法的复杂特征.
  - 能够准确的对不同上下文进行反应.

![1BYhYD](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/1BYhYD.jpg)

- **ELMO的本质思想是**：
  - **事先用语言模型学好一个单词的Word Embedding**，此时多义词无法区分，不过这没关系。在实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候可以**根据上下文单词的语义去调整单词的Word Embedding表示**，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。**所以ELMO本身是个根据当前上下文对Word Embedding动态调整的思路**。

ELMO采用了典型的两阶段过程，**第一个阶段是利用语言模型进行预训练**；**第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中。**

- 第一阶段：预训练过程
  - 上图展示的是其预训练过程，它的网络结构采用了双层双向LSTM，目前语言模型训练的任务目标是根据单词 $W_i$ 的上下文去正确预测单词 $W_i$. 
  - $W_i$ 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。图中左端的前向双层LSTM代表正方向编码器，输入的是从左到右顺序的除了预测单词外 $W_i$ 的上文Context-before；右端的逆向双层LSTM代表反方向编码器，输入的是从右到左的逆序的句子下文Context-after；每个编码器的深度都是两层LSTM叠加。
  - 这个网络结构其实在NLP中是很常用的。**使用这个网络结构利用大量语料做语言模型任务就能预先训练好这个网络**，如果训练好这个网络后，输入一个新句子 $S_{new}$ ，句子中每个单词都能得到对应的三个Embedding：**最底层是单词的Word Embedding**，往上走是**第一层双向LSTM**中对应单词位置的Embedding，这层编码单词的**句法信息**更多一些；再往上走是**第二层LSTM**中对应单词位置的Embedding，这层编码单词的**语义信息**更多一些。也就是说，**ELMO的预训练过程不仅仅学会单词的Word Embedding，还学会了一个双层双向的LSTM网络结构，而这两者后面都有用**。

![qFkwrK](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/qFkwrK.jpg)

![216bth](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/216bth.jpg)

- 第二阶段：下游任务使用预训练模型
  - 上图展示了下游任务的使用过程，比如我们的下游任务仍然是QA问题，此时对于问句X，我们可以**先将句子X作为预训练好的ELMO网络的输入，这样句子X中每个单词在ELMO网络中都能获得对应的三个Embedding**
  - **之后给予这三个Embedding中的每一个Embedding一个权重 a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个**。
  - 然后将**整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入**，**以此作为补充的新特征给下游任务使用（two representations pf the word）**。对于上图所示下游任务QA中的回答句子Y来说也是如此处理。因为ELMO给下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为“Feature-based Pre-Training”。



![a48jnW](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/a48jnW.png)

前面我们提到静态Word Embedding无法解决多义词的问题，那么ELMO引入上下文动态调整单词的embedding后多义词问题解决了吗？

- 解决了，而且比我们期待的解决得还要好。上图给了个例子，对于Glove训练出的Word Embedding来说，多义词比如play，根据它的embedding找出的最接近的其它单词大多数集中在体育领域，这很明显是因为训练数据中包含play的句子中体育领域的数量明显占优导致；
- 而使用ELMO，**根据上下文动态调整后的embedding不仅能够找出对应的“演出”的相同语义的句子，而且还可以保证找出的句子中的play对应的词性也是相同的，这是超出期待之处。**之所以会这样，是因为我们上面提到过，**第一层LSTM编码了很多句法信息，这在这里起到了重要作用**。

![LyzYTB](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/LyzYTB.png)

那么站在现在这个时间节点看，ELMO有什么值得改进的缺点呢？

- 首先，**一个非常明显的缺点在特征抽取器选择方面，ELMO使用了LSTM而不是新贵Transformer**，Transformer是谷歌在17年做机器翻译任务的“Attention is all you need”的论文中提出的，引起了相当大的反响，**很多研究已经证明了Transformer提取特征的能力是要远强于LSTM的**。如果ELMO采取Transformer作为特征提取器，那么估计Bert的反响远不如现在的这种火爆场面。

- 另外一点，**ELMO采取双向拼接这种融合特征的能力可能比Bert一体化的融合特征方式弱**，但是，这只是一种从道理推断产生的怀疑，目前并没有具体实验说明这一点。

预训练方法对比

- 除了**以ELMO为代表的这种基于特征融合的预训练方法外**
- NLP里还有一种典型做法，这种做法和图像领域的方式就是看上去一致的了，一般将这种方法称为“**基于Fine-tuning的模式**”，而GPT就是这一模式的典型开创者。



#### GPT

![RdKpLO](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/RdKpLO.png)

GPT是“Generative Pre-Training”的简称，从名字看其含义是指的生成式的预训练。**GPT也采用两阶段过程，第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务**。

- 上图展示了GPT的预训练过程，其实和ELMO是类似的，主要不同在于两点：首先，**特征抽取器不是用的RNN，而是用的Transformer**，上面提到过它的特征抽取能力要强于RNN，这个选择很明显是很明智的；其次，GPT的预训练虽然仍然是以语言模型作为目标任务，**但是采用的是单向的语言模型**，所谓“单向”的含义是指：语言模型训练的任务目标是根据 ![[公式]](https://www.zhihu.com/equation?tex=W_i) 单词的上下文去正确预测单词 ![[公式]](https://www.zhihu.com/equation?tex=W_i) ， ![[公式]](https://www.zhihu.com/equation?tex=W_i) 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。
- ELMO在做语言模型预训练的时候，预测单词 ![[公式]](https://www.zhihu.com/equation?tex=W_i) 同时使用了上文和下文，而GPT则**只采用Context-before这个单词的上文来进行预测，而抛开了下文**。**这个选择现在看不是个太好的选择**，原因很简单，它没有把单词的下文融合进来，这限制了其在更多应用场景的效果，比如阅读理解这种任务，在做任务的时候是可以允许同时看到上文和下文一起做决策的。如果预训练时候不把单词的下文嵌入到Word Embedding中，是很吃亏的，白白丢掉了很多信息。

![mJVvU1](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/mJVvU1.png)

上面讲的是GPT如何进行第一阶段的预训练，那么假设预训练好了网络模型，**后面下游任务怎么用？它有自己的个性，和ELMO的方式大有不同**。

- 上图展示了GPT在第二阶段如何使用。首先，对于不同的下游任务来说，本来你可以任意设计自己的网络结构，现在不行了，你**要向GPT的网络结构看齐，把任务的网络结构改造成和GPT的网络结构是一样的**。
- 然后，在做下游任务的时候，**利用第一步预训练好的参数初始化GPT的网络结构，这样通过预训练学到的语言学知识就被引入到你手头的任务里来了**，这是个非常好的事情。再次，**你可以用手头的任务去训练这个网络，对网络参数进行Fine-tuning，使得这个网络更适合解决手头的问题**。就是这样。看到了么？这有没有让你想起最开始提到的图像领域如何做预训练的过程（请参考上图那句非常容易暴露年龄的歌词）？对，这跟那个模式是一模一样的。

![NqQIBq](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/NqQIBq.png)

这里引入了一个新问题：对于NLP各种花样的不同任务，怎么改造才能靠近GPT的网络结构呢？

- GPT论文给了一个改造施工图如上，其实也很简单：
- 对于分类问题，不用怎么动，加上一个起始和终结符号即可；
- 对于句子关系判断问题，比如Entailment，两个句子中间再加个分隔符即可；
- 对文本相似性判断问题，把两个句子顺序颠倒下做出两个输入即可，这是为了告诉模型句子顺序不重要；
- 对于多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入即可。从上图可看出，这种改造还是很方便的，不同任务只需要在输入部分施工即可。

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/1NIiFH.png" alt="1NIiFH" style="zoom:33%;" />

#### Bert

![BxDasT](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/BxDasT.png)

Bert采用和GPT完全相同的两阶段模型，首先是语言模型预训练；其次是使用Fine-Tuning模式解决下游任务。**和GPT的最主要不同在于在预训练阶段采用了类似ELMO的双向语言模型**，当然另外一点是语言模型的数据规模要比GPT大。所以这里Bert的预训练过程不必多讲了。

![iciQZi](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/iciQZi.png)

第二阶段，Fine-Tuning阶段，这个阶段的做法和GPT是一样的。当然，它也面临着下游任务网络结构改造的问题，在改造任务方面Bert和GPT有些不同，下面简单介绍一下。

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/PSbs4u.png" alt="PSbs4u" style="zoom: 33%;" />

在介绍Bert如何改造下游任务之前，先大致说下NLP的几类问题，说这个是为了强调Bert的普适性有多强。通常而言，绝大部分NLP问题可以归入上图所示的四类任务中：

- 一类是序列标注，这是最典型的NLP任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。
- 第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。
- 第三类任务是句子关系判断，比如Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系；
- 第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。

![ph6ZiV](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ph6ZiV.jpg)

对于种类如此繁多而且各具特点的下游NLP任务，Bert如何改造输入输出部分使得大部分NLP任务都可以使用Bert预训练好的模型参数呢？上图给出示例，对于句子关系类任务，很简单，和GPT类似，加上一个起始和终结符号，句子之间加个分隔符即可。对于输出来说，把第一个起始符号对应的Transformer最后一层位置上面串接一个softmax分类层即可。对于分类问题，与GPT一样，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造；对于序列标注问题，输入部分和单句分类是一样的，只需要输出部分Transformer最后一层每个单词对应位置都进行分类即可。从这里可以看出，上面列出的NLP四大任务里面，除了生成类任务外，Bert其它都覆盖到了，而且改造起来很简单直观。尽管Bert论文没有提，但是稍微动动脑子就可以想到，其实对于机器翻译或者文本摘要，聊天机器人这种生成式任务，同样可以稍作改造即可引入Bert的预训练成果。只需要附着在S2S结构上，encoder部分是个深度Transformer结构，decoder部分也是个深度Transformer结构。根据任务选择不同的预训练数据初始化encoder和decoder即可。这是相当直观的一种改造方法。当然，也可以更简单一点，比如直接在单个Transformer结构上加装隐层产生输出也是可以的。不论如何，从这里可以看出，NLP四大类任务都可以比较方便地改造成Bert能够接受的方式。这其实是Bert的非常大的优点，这意味着它几乎可以做任何NLP的下游任务，具备普适性，这是很强的。



### **word2vec和NNLM对比有什么区别？（word2vec vs NNLM）**

1）其本质都可以看作是语言模型；

2）词向量只不过NNLM一个产物，word2vec虽然其本质也是语言模型，但是其专注于词向量本身，因此做了许多优化来提高计算效率：

- 与NNLM相比，词向量直接sum，不再拼接，并舍弃隐层；
- 考虑到sofmax归一化需要遍历整个词汇表，采用hierarchical softmax 和negative sampling进行优化，hierarchical softmax 实质上生成一颗带权路径最小的哈夫曼树，让高频词搜索路劲变小；negative sampling更为直接，实质上对每一个样本中每一个词都进行负例采样；



### **word2vec和fastText对比有什么区别？（word2vec vs fastText）**

1）都可以无监督学习词向量， fastText训练词向量时会考虑subword；

2） fastText还可以进行有监督学习进行文本分类，其主要特点：

- 结构与CBOW类似，但学习目标是人工标注的分类结果；
- 采用hierarchical softmax对输出的分类标签建立哈夫曼树，样本中标签多的类别被分配短的搜寻路径；
- 引入N-gram，考虑词序特征；
- 引入subword来处理长词，处理未登陆词问题；



### **glove和word2vec、 LSA对比有什么区别？（word2vec vs glove vs LSA）**

1）**glove vs LSA**

- LSA（Latent Semantic Analysis）可以基于co-occurance matrix构建词向量，实质上是基于全局语料采用SVD进行矩阵分解，然而SVD计算复杂度高；
- glove可看作是对LSA一种优化的高效矩阵分解算法，采用Adagrad对最小平方损失进行优化；

2）**word2vec vs glove**

- word2vec是局部语料库训练的，其特征提取是基于滑窗的；而glove的滑窗是为了构建co-occurance matrix，是基于全局语料的，可见glove需要事先统计共现概率；因此，word2vec可以进行在线学习，glove则需要统计固定语料信息。
- word2vec是无监督学习，同样由于不需要人工标注；glove通常被认为是无监督学习，但实际上glove还是有label的，即共现次数![[公式]](https://www.zhihu.com/equation?tex=log%28X_%7Bij%7D%29)。
- word2vec损失函数实质上是带权重的交叉熵，权重固定；glove的损失函数是最小平方损失函数，权重可以做映射变换。
- 总体来看，**glove可以被看作是更换了目标函数和权重函数的全局word2vec**。



### **elmo、GPT、bert三者之间有什么区别？（elmo vs GPT vs bert）**

之前介绍词向量均是静态的词向量，无法解决一次多义等问题。下面介绍三种elmo、GPT、bert词向量，它们都是基于语言模型的动态词向量。下面从几个方面对这三者进行对比：

（1）**特征提取器**：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。

（2）**单/双向语言模型**：

- GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。
- GPT和bert都采用Transformer，Transformer是encoder-decoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子。



### **深入解剖bert（与elmo和GPT比较）**

bert的全称是Bidirectional Encoder Representation from Transformers，bert的核心是双向**Transformer Encoder**，提出以下问题并进行解答：

**1、为什么bert采取的是双向Transformer Encoder，而不叫decoder？**

- BERT Transformer 使用双向self-attention，而GPT Transformer 使用受限制的self-attention，其中每个token只能处理其左侧的上下文。双向 Transformer 通常被称为“Transformer encoder”，而左侧上下文被称为“Transformer decoder”，decoder是不能获要预测的信息的。

**2、elmo、GPT和bert在单双向语言模型处理上的不同之处？**

- 在上述3个模型中，只有bert共同依赖于左右上下文。那elmo不是双向吗？实际上elmo使用的是经过独立训练的从左到右和从右到左LSTM的串联拼接起来的。而GPT使用从左到右的Transformer，实际就是“Transformer decoder”。

**3、bert构建双向语言模型不是很简单吗？不也可以直接像elmo拼接Transformer decoder吗？**

- BERT 的作者认为，这种拼接式的bi-directional 仍然不能完整地理解整个语句的语义。更好的办法是用上下文全向来预测[mask]，也就是用 “能/实现/语言/表征/../的/模型”，来预测[mask]。BERT 作者把上下文全向的预测方法，称之为 deep bi-directional。

**4、bert为什么要采取Marked LM，而不直接应用Transformer Encoder？**

- 我们知道向Transformer这样深度越深，学习效果会越好。可是为什么不直接应用双向模型呢？因为随着网络深度增加会导致标签泄露。如下图：

- 深度双向模型比left-to-right 模型或left-to-right and right-to-left模型的浅层连接更强大。遗憾的是，标准条件语言模型只能从左到右或从右到左进行训练，因为双向条件作用将允许每个单词在多层上下文中间接地“see itself”。

  为了训练一个深度双向表示（deep bidirectional representation），研究团队采用了一种简单的方法，即随机屏蔽（masking）部分输入token，然后只预测那些被屏蔽的token。论文将这个过程称为“masked LM”(MLM)。

![REpO3J](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/REpO3J.jpg)

**5、bert为什么并不总是用实际的[MASK]token替换被“masked”的词汇？**

> [NLP必读 | 十分钟读懂谷歌BERT模型](https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/4dbdb5ab959b)： 虽然这确实能让团队获得双向预训练模型，但这种方法有两个缺点。首先，预训练和finetuning之间不匹配，因为在finetuning期间从未看到[MASK]token。为了解决这个问题，团队并不总是用实际的[MASK]token替换被“masked”的词汇。相反，训练数据生成器随机选择15％的token。例如在这个句子“my dog is hairy”中，它选择的token是“hairy”。然后，执行以下过程：
> 数据生成器将执行以下操作，而不是始终用[MASK]替换所选单词：
> 80％的时间：用[MASK]标记替换单词，例如，my dog is hairy → my dog is [MASK]
> 10％的时间：用一个随机的单词替换该单词，例如，my dog is hairy → my dog is apple
> 10％的时间：保持单词不变，例如，my dog is hairy → my dog is hairy. 这样做的目的是将表示偏向于实际观察到的单词。
> Transformer encoder不知道它将被要求预测哪些单词或哪些单词已被随机单词替换，因此它被迫保持每个输入token的分布式上下文表示。此外，因为随机替换只发生在所有token的1.5％（即15％的10％），这似乎不会损害模型的语言理解能力。
> 使用MLM的第二个缺点是每个batch只预测了15％的token，这表明模型可能需要更多的预训练步骤才能收敛。团队证明MLM的收敛速度略慢于 left-to-right的模型（预测每个token），但MLM模型在实验上获得的提升远远超过增加的训练成本。

- bert模型的主要创新点都在pre-train方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。

