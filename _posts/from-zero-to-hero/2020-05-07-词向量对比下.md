---
title: "词向量对比「下」"
subtitle: "word2vec/glove/fastText/elmo/GPT/bert"
layout: post
author: "echisenyang"
header-style: text
hidden: true
catalog: true
tags:
  - 输出计划
---





### 基于词向量的动态表征 (动态词向量)

#### ELMo

ELMo与word2vec最大的不同：

- Contextual: The representation for each word depends on the entire context in which it is used.　（即词向量不是一成不变的，而是根据上下文而随时变化，elmo属于动态词向量（上下文词向量）这与word2vec或者glove具有很大的区别）

- **什么是一个好的词向量**
  - 能够反映出语义和语法的复杂特征.
  - 能够准确的对不同上下文进行反应.

![1BYhYD](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/1BYhYD.jpg)

- **ELMO的本质思想是**：
  - **事先用语言模型学好一个单词的Word Embedding**，此时多义词无法区分，不过这没关系。在实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候可以**根据上下文单词的语义去调整单词的Word Embedding表示**，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。**所以ELMO本身是个根据当前上下文对Word Embedding动态调整的思路**。

ELMO采用了典型的两阶段过程，**第一个阶段是利用语言模型进行预训练**；**第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中。**

- 第一阶段：预训练过程
  - 上图展示的是其预训练过程，它的网络结构采用了双层双向LSTM，目前语言模型训练的任务目标是根据单词 $W_i$ 的上下文去正确预测单词 $W_i$. 
  - $W_i$ 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。图中左端的前向双层LSTM代表正方向编码器，输入的是从左到右顺序的除了预测单词外 $W_i$ 的上文Context-before；右端的逆向双层LSTM代表反方向编码器，输入的是从右到左的逆序的句子下文Context-after；每个编码器的深度都是两层LSTM叠加。
  - 这个网络结构其实在NLP中是很常用的。**使用这个网络结构利用大量语料做语言模型任务就能预先训练好这个网络**，如果训练好这个网络后，输入一个新句子 $S_{new}$ ，句子中每个单词都能得到对应的三个Embedding：**最底层是单词的Word Embedding**，往上走是**第一层双向LSTM**中对应单词位置的Embedding，这层编码单词的**句法信息**更多一些；再往上走是**第二层LSTM**中对应单词位置的Embedding，这层编码单词的**语义信息**更多一些。也就是说，**ELMO的预训练过程不仅仅学会单词的Word Embedding，还学会了一个双层双向的LSTM网络结构，而这两者后面都有用**。

![qFkwrK](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/qFkwrK.jpg)

![216bth](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/216bth.jpg)

- 第二阶段：下游任务使用预训练模型
  - 上图展示了下游任务的使用过程，比如我们的下游任务仍然是QA问题，此时对于问句X，我们可以**先将句子X作为预训练好的ELMO网络的输入，这样句子X中每个单词在ELMO网络中都能获得对应的三个Embedding**
  - **之后给予这三个Embedding中的每一个Embedding一个权重 a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个**。
  - 然后将**整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入**，**以此作为补充的新特征给下游任务使用（two representations pf the word）**。对于上图所示下游任务QA中的回答句子Y来说也是如此处理。因为ELMO给下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为“Feature-based Pre-Training”。



![a48jnW](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/a48jnW.png)

前面我们提到静态Word Embedding无法解决多义词的问题，那么ELMO引入上下文动态调整单词的embedding后多义词问题解决了吗？

- 解决了，而且比我们期待的解决得还要好。上图给了个例子，对于Glove训练出的Word Embedding来说，多义词比如play，根据它的embedding找出的最接近的其它单词大多数集中在体育领域，这很明显是因为训练数据中包含play的句子中体育领域的数量明显占优导致；
- 而使用ELMO，**根据上下文动态调整后的embedding不仅能够找出对应的“演出”的相同语义的句子，而且还可以保证找出的句子中的play对应的词性也是相同的，这是超出期待之处。**之所以会这样，是因为我们上面提到过，**第一层LSTM编码了很多句法信息，这在这里起到了重要作用**。

![LyzYTB](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/LyzYTB.png)

那么站在现在这个时间节点看，ELMO有什么值得改进的缺点呢？

- 首先，**一个非常明显的缺点在特征抽取器选择方面，ELMO使用了LSTM而不是新贵Transformer**，Transformer是谷歌在17年做机器翻译任务的“Attention is all you need”的论文中提出的，引起了相当大的反响，**很多研究已经证明了Transformer提取特征的能力是要远强于LSTM的**。如果ELMO采取Transformer作为特征提取器，那么估计Bert的反响远不如现在的这种火爆场面。

- 另外一点，**ELMO采取双向拼接这种融合特征的能力可能比Bert一体化的融合特征方式弱**，但是，这只是一种从道理推断产生的怀疑，目前并没有具体实验说明这一点。

预训练方法对比

- 除了**以ELMO为代表的这种基于特征融合的预训练方法外**
- NLP里还有一种典型做法，这种做法和图像领域的方式就是看上去一致的了，一般将这种方法称为“**基于Fine-tuning的模式**”，而GPT就是这一模式的典型开创者。



#### GPT

![RdKpLO](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/RdKpLO.png)

GPT是“Generative Pre-Training”的简称，从名字看其含义是指的生成式的预训练。**GPT也采用两阶段过程，第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务**。

- 上图展示了GPT的预训练过程，其实和ELMO是类似的，主要不同在于两点：首先，**特征抽取器不是用的RNN，而是用的Transformer**，上面提到过它的特征抽取能力要强于RNN，这个选择很明显是很明智的；其次，GPT的预训练虽然仍然是以语言模型作为目标任务，**但是采用的是单向的语言模型**，所谓“单向”的含义是指：语言模型训练的任务目标是根据 ![[公式]](https://www.zhihu.com/equation?tex=W_i) 单词的上下文去正确预测单词 ![[公式]](https://www.zhihu.com/equation?tex=W_i) ， ![[公式]](https://www.zhihu.com/equation?tex=W_i) 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。
- ELMO在做语言模型预训练的时候，预测单词 ![[公式]](https://www.zhihu.com/equation?tex=W_i) 同时使用了上文和下文，而GPT则**只采用Context-before这个单词的上文来进行预测，而抛开了下文**。**这个选择现在看不是个太好的选择**，原因很简单，它没有把单词的下文融合进来，这限制了其在更多应用场景的效果，比如阅读理解这种任务，在做任务的时候是可以允许同时看到上文和下文一起做决策的。如果预训练时候不把单词的下文嵌入到Word Embedding中，是很吃亏的，白白丢掉了很多信息。

![mJVvU1](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/mJVvU1.png)

上面讲的是GPT如何进行第一阶段的预训练，那么假设预训练好了网络模型，**后面下游任务怎么用？它有自己的个性，和ELMO的方式大有不同**。

- 上图展示了GPT在第二阶段如何使用。首先，对于不同的下游任务来说，本来你可以任意设计自己的网络结构，现在不行了，你**要向GPT的网络结构看齐，把任务的网络结构改造成和GPT的网络结构是一样的**。
- 然后，在做下游任务的时候，**利用第一步预训练好的参数初始化GPT的网络结构，这样通过预训练学到的语言学知识就被引入到你手头的任务里来了**，这是个非常好的事情。再次，**你可以用手头的任务去训练这个网络，对网络参数进行Fine-tuning，使得这个网络更适合解决手头的问题**。就是这样。看到了么？这有没有让你想起最开始提到的图像领域如何做预训练的过程（请参考上图那句非常容易暴露年龄的歌词）？对，这跟那个模式是一模一样的。

![NqQIBq](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/NqQIBq.png)

这里引入了一个新问题：对于NLP各种花样的不同任务，怎么改造才能靠近GPT的网络结构呢？

- GPT论文给了一个改造施工图如上，其实也很简单：
- 对于分类问题，不用怎么动，加上一个起始和终结符号即可；
- 对于句子关系判断问题，比如Entailment，两个句子中间再加个分隔符即可；
- 对文本相似性判断问题，把两个句子顺序颠倒下做出两个输入即可，这是为了告诉模型句子顺序不重要；
- 对于多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入即可。从上图可看出，这种改造还是很方便的，不同任务只需要在输入部分施工即可。

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/1NIiFH.png" alt="1NIiFH" style="zoom:33%;" />

#### Bert

![BxDasT](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/BxDasT.png)

Bert采用和GPT完全相同的两阶段模型，首先是语言模型预训练；其次是使用Fine-Tuning模式解决下游任务。**和GPT的最主要不同在于在预训练阶段采用了类似ELMO的双向语言模型**，当然另外一点是语言模型的数据规模要比GPT大。所以这里Bert的预训练过程不必多讲了。

![iciQZi](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/iciQZi.png)

第二阶段，Fine-Tuning阶段，这个阶段的做法和GPT是一样的。当然，它也面临着下游任务网络结构改造的问题，在改造任务方面Bert和GPT有些不同，下面简单介绍一下。

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/PSbs4u.png" alt="PSbs4u" style="zoom: 33%;" />

在介绍Bert如何改造下游任务之前，先大致说下NLP的几类问题，说这个是为了强调Bert的普适性有多强。通常而言，绝大部分NLP问题可以归入上图所示的四类任务中：

- 一类是序列标注，这是最典型的NLP任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。
- 第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。
- 第三类任务是句子关系判断，比如Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系；
- 第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。

![ph6ZiV](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ph6ZiV.jpg)

对于种类如此繁多而且各具特点的下游NLP任务，Bert如何改造输入输出部分使得大部分NLP任务都可以使用Bert预训练好的模型参数呢？上图给出示例，对于句子关系类任务，很简单，和GPT类似，加上一个起始和终结符号，句子之间加个分隔符即可。对于输出来说，把第一个起始符号对应的Transformer最后一层位置上面串接一个softmax分类层即可。对于分类问题，与GPT一样，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造；对于序列标注问题，输入部分和单句分类是一样的，只需要输出部分Transformer最后一层每个单词对应位置都进行分类即可。从这里可以看出，上面列出的NLP四大任务里面，除了生成类任务外，Bert其它都覆盖到了，而且改造起来很简单直观。尽管Bert论文没有提，但是稍微动动脑子就可以想到，其实对于机器翻译或者文本摘要，聊天机器人这种生成式任务，同样可以稍作改造即可引入Bert的预训练成果。只需要附着在S2S结构上，encoder部分是个深度Transformer结构，decoder部分也是个深度Transformer结构。根据任务选择不同的预训练数据初始化encoder和decoder即可。这是相当直观的一种改造方法。当然，也可以更简单一点，比如直接在单个Transformer结构上加装隐层产生输出也是可以的。不论如何，从这里可以看出，NLP四大类任务都可以比较方便地改造成Bert能够接受的方式。这其实是Bert的非常大的优点，这意味着它几乎可以做任何NLP的下游任务，具备普适性，这是很强的。

