---
title: "朴素贝叶斯"
subtitle: "naive bayes"
layout: post
author: "echisenyang"
header-style: text
hidden: true
catalog: true
tags:
  - 输出计划
  - 李航统计学习第二版
---



![pRJJ68](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/pRJJ68.jpg)

### 朴素贝叶斯做分类

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/iZl53i.png" alt="iZl53i" style="zoom:50%;" />



### 极大似然估计

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/JaoeE4.png" alt="JaoeE4" style="zoom:50%;" />



### 贝叶斯估计（极大似然估计 vs 拉普拉斯平滑）

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/rFXnoJ.png" alt="rFXnoJ" style="zoom:50%;" />



### 极大似然估计引入的原因

- **问题引出**

  - 但是在实际问题中并不都是这样幸运的，**我们能获得的数据可能只有有限数目的样本数据，而先验概率 $p(w_i)$ 和类条件概率(各类的总体分布) $p(x \mid w_i)$ 都是未知的**。**根据仅有的样本数据进行分类时，一种可行的办法是我们需要先对先验概率和类条件概率进行估计**，然后再套用贝叶斯分类器。

  - **先验概率的估计较简单**

    1. 每个样本所属的自然状态都是已知的（有监督学习）；
    2. 依靠经验；
    3. 用训练样本中各类出现的频率估计。

  - **类条件概率的估计（非常难）**原因包括：

    1. 概率密度函数包含了一个随机变量的全部信息；
    2. 样本数据可能不多；
    3. 特征向量x的维度可能很大等等。

    总之要直接估计类条件概率的密度函数很难。**解决的办法就是，把估计完全未知的概率密度 $p(x \mid w_i)$ 转化为估计参数。**这里就将概率密度估计问题转化为参数估计问题，**极大似然估计就是一种参数估计方法**。当然了，概率密度函数的选取很重要，模型正确，在样本区域无穷时，我们会得到较准确的估计值，如果模型都错了，那估计半天的参数，肯定也没啥意义了。

- **重要前提**

  - 上面说到，**参数估计问题只是实际问题求解过程中的一种简化方法**（由于直接估计类条件概率密度函数很困难）。所以能够使用极大似然估计方法的样本必须需要满足一些前提假设。

  - 重要前提：**训练样本的分布能代表样本的真实分布。每个样本集中的样本都是所谓独立同分布的随机变量（$iid$ 条件），且有充分的训练样本**。

- **似然函数: 联合概率密度函数 $P(D \mid \theta)$ 称为相对于 $\lbrace x_{1}, x_{2}, \cdots, x_{N} \rbrace$ 的 $\theta$ 的似然函数。**

  - 由于样本集中的样本都是**独立同分布**，可以只考虑一类样本集 $D$，来估计参数向量 $\theta$。记已知的样本集为：$D=\lbrace x_{1}, x_{2}, \cdots, x_{N} \rbrace$

  $$
  l(\theta)=p(D  \mid  \theta)=p\left(x_{1}, x_{2}, \cdots, x_{N}  \mid  \theta\right)=\prod_{i=1}^{N} p\left(x_{i}  \mid  \theta\right)
  $$

  - 如果 $\hat{\theta}$ 是参数空间中能使似然函数 $l(\theta)$ 最大的 $\theta$ 值，则 $\hat{\theta}$ 应该是“最可能”的参数值，那么 $\hat{\theta}$ 就是 $\theta$ 的极大似然估计量。
    
    $$
    \hat{\theta}(x_{1}, x_{2}, \cdots, x_{N})\text{称为极大似然函数的估计值}
    $$
    

- 求解极大似然函数

  - ML估计：求使得出现该组样本的概率最大的 $\theta$ 值。
    
    $$
    \hat{\theta}=\arg \max_{\theta} l(\theta)=\arg \max_{\theta} \prod_{i=1}^{N} p\left(x_{i}  \mid  \theta\right)
    $$
  
- 实际中为了便于分析，定义了对数似然函数：
  
    $$
    \begin{array}{c}{H(\theta)=\ln l(\theta)} \\ {\hat{\theta}=\arg \max_{\theta} H(\theta)=\arg \max_{\theta} \ln l(\theta)=\arg \max_{\theta} \sum_{i=1}^{N} \ln p\left(x_{i}  \mid  \theta\right)}\end{array}
    $$
  
- 求解
  
    1. 未知参数只有一个（ $\theta$ 为标量）
    
       $$
       \frac{d l(\theta)}{d \theta}=0 \text{或者等价于} 
       \frac{d H(\theta)}{d \theta}=\frac{d \ln l(\theta)}{d \theta}=0
       $$
       
    2. 未知参数有多个（ $\theta$ 为向量）
       $$
       \nabla_{\theta} H(\theta)=\nabla_{\theta} \ln l(\theta)=\sum_{i=1}^{N} \nabla_{\theta} \ln P\left(x_{i}  \mid  \theta\right)=0
       $$
       



### 独立同分布理解

独立同分布（independent and identically distributed，i.i.d.）在概率统计理论中，指随机过程中，任何时刻的取值都为随机变量，如果这些随机变量服从同一分布，并且互相独立，那么这些随机变量是独立同分布。

- 一个简单的例子：
  - **独立就是每次抽样之间是没有关系的，不会相互影响**。就像我抛色子每次抛到几就是几，这就是独立的。但若我要两次抛的和大于8，其余的不算，那么第一次抛和第二次抛就不独立了，因为第二次抛的时候结果是和第一次相关的。
  - **同分布的意思就是每次抽样，样本都服从同样的一个分布**，抛色子每次得到任意点数的概率都是1/6，这就是同分布的。但若我第一次抛一个六面的色子，第二次抛一个正12面体的色子，就不再是同分布了。



### MLE vs. MAP

- 对于**先验概率分布**与**条件概率分布**可以视为采用**MLE**
  - 计算**条件概率分布**时用到了**特征条件独立假设**
  - 用**MLE**对**条件概率分布**估计时用到了**独立同分布假设**
- 对于朴素贝叶斯做分类任务时，计算的**后验概率分布**可以视为**MAP**

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/aE6H8Z.jpg)



### 贝叶斯定理补充

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ekl7e3.jpg" style="zoom: 33%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/TTFYJZ.jpg" style="zoom: 33%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/GZlTqZ.jpg" style="zoom:33%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/AiDdjd.jpg" style="zoom:33%;" />

