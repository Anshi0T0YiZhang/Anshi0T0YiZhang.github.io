---
title: "支持向量机"
subtitle: "support vector machine"
layout: post
author: "echisenyang"
header-style: text
hidden: true
catalog: true
tags:
  - 输出计划
  - 李航统计学习第二版
---



TODO

- 基石【VC Theory 正则化等基础理论讲的特别好】
- 技法【SVM 决策树 随机森林等机器学习讲的特别好 深度学习涉及少】

![8YnP71](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/8YnP71.jpg)

![7.SVM-2](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/7.SVM-2.jpg)
![7.SVM-3](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/7.SVM-3.jpg)
![7.SVM-4](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/7.SVM-4.jpg)
![7.SVM-5](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/7.SVM-5.jpg)
![7.SVM-6](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/7.SVM-6.jpg)



### 函数间隔与几何间隔（间隔最大化的含义）

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/jEWIzZ.png" alt="jEWIzZ" style="zoom:50%;" />

- 支持向量机学习的**基本想法与直观解释**

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/nPrMc0.png" alt="nPrMc0" style="zoom:50%;" />

- **支持向量在确定分离超平面中起着决定性作用**

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/BZ5mlp.png" alt="BZ5mlp" style="zoom:50%;" />

### 对偶问题

#### 1.hard-margin 对偶问题

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/S69fJC.png" alt="S69fJC" style="zoom:50%;" />

#### 2.soft-margin 对偶问题

- **<font color=red>soft-margin中soft的含义：允许一丢丢错误</font>**

- **引入松弛变量与对应的代价函数，将硬间隔->软间隔**

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/AEt3tK.png" alt="AEt3tK" style="zoom:50%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/RqT2Hv.png" alt="RqT2Hv" style="zoom:50%;" />

- **松弛变量与支持向量的关系**

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/v4w0jP.png" alt="v4w0jP" style="zoom:50%;" />

- **引入 hinge loss（合页损失）**

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/BC6fW9.jpg" alt="BC6fW9" style="zoom: 67%;" />

很像一本打开的书吧！于是就是合页了。

hinge-loss的公式是：

![ybhcH4](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ybhcH4.jpg)

第一项是损失，第二项是正则化项。这个公式就是说 ![[公式]](https://www.zhihu.com/equation?tex=y_i%28w%C2%B7x_i%2Bb%29) 大于1时loss为0， 否则loss为 ![[公式]](https://www.zhihu.com/equation?tex=1-y_i%28w%C2%B7x_i%2Bb%29) 。对比感知机的 0-1 损失函数 来说，**hinge loss不仅要分类正确，而且置信度足够高的时候，损失才为0，对学习有更高的要求**。对比一下感知机损失和hinge loss的图像，明显Hinge loss更加严格。

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/gVRhsv.png" alt="gVRhsv" style="zoom:50%;" />

- soft-margin对偶算法

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/jVKcVg.png" alt="jVKcVg" style="zoom:50%;" />

#### 3.kernel svm

- 非线性分类问题

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/8pq1YO.png" alt="8pq1YO" style="zoom:50%;" />

- 核技巧

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/tr9kuH.png" alt="tr9kuH" style="zoom:50%;" />

- 核技巧在支持向量机中的应用

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/hzsAWy.png" alt="hzsAWy" style="zoom:50%;" />

- kernel svm 算法

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/rcIqmN.png" alt="rcIqmN" style="zoom:50%;" />



### 非线性问题的三种解决方法（svm的启发）

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/mTxziE.jpg" style="zoom:100%" />
</p>

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/hZkJUs.jpg" style="zoom:100%" />
</p>

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/seUwQx.jpg" style="zoom:100%" />
</p>

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ZtkWMh.jpg" style="zoom:100%" />
</p>

