---
title: "词向量对比「上」"
subtitle: "word2vec/glove/fastText/elmo/GPT/bert"
layout: post
author: "echisenyang"
header-style: text
hidden: true
catalog: true
tags:
  - 输出计划
---

### Word Embeddings

![1tdi3C](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/1tdi3C.png)

> *“The gift of words is the gift of deception and illusion” ~Children of Dune*

With this understanding, we can proceed to look at trained word-vector examples (also called word embeddings) and start looking at some of their interesting properties.

This is a word embedding for the word “king” (GloVe vector trained on Wikipedia):

`[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]`

It’s a list of 50 numbers. We can’t tell much by looking at the values. But let’s visualize it a bit so we can compare it other word vectors. Let’s put all these numbers in one row:

![dZgg0p](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/dZgg0p.png)

Let’s color code the cells based on their values (red if they’re close to 2, white if they’re close to 0, blue if they’re close to -2):

![F4LOjc](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/F4LOjc.png)

We’ll proceed by ignoring the numbers and only looking at the colors to indicate the values of the cells. Let’s now contrast “King” against other words:

![3nEzcU](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/3nEzcU.png)

See how “Man” and “Woman” are much more similar to each other than either of them is to “king”? This tells you something. **These vector representations capture quite a bit of the information/meaning/associations of these words.**

Here’s another list of examples (compare by vertically scanning the columns looking for columns with similar colors):

![UcxguO](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/UcxguO.png)



### 文本表示哪些方法

- 基于one-hot、tf-idf、textrank等的bag-of-words；
- 主题模型：LSA（SVD）、pLSA、LDA；
- 基于词向量的固定表征 (静态词向量)：word2vec、fastText、glove
- 基于词向量的动态表征 (动态词向量)：elmo、GPT、bert



### **各种词向量的特点**

- One-hot 表示 ：维度灾难、语义鸿沟；
- 分布式表示 (distributed representation) ：
  - 矩阵分解（LSA）：利用全局语料特征，但SVD求解计算复杂度大；
  - word2vec、fastText：优化效率高，但是基于局部语料；
  - glove：基于全局预料，结合了LSA和word2vec的优点；
  - elmo、GPT、bert：动态特征，解决了**静态词向量是固定表征的，无法解决一词多义等问题**；



### 词向量的发展历史

- **在word vector之前**，计算机中可用的描述词的含义的是：**WordNet**
  - 可以找到一个词的许多相似词，但是WordNet忽略了这些相似词之间的差异性；
  - 并且WordNet对新词的扩展性也不好；
  - WordNet的另一个严重问题，由于是采用**分类学表征**构建，意味着所有的词都是**离散化表征**，即每个词都是一个 **one-hot** 向量（one-hot的严重缺陷就是当词典非常大时，每一个单词的**维度会非常**大；另外，每一个词与其他的词都是**正交**的，这意味着**丢失了词之间的相似度**，there is **no natural notion of similarity** in a set of one-hot vectors）。

- **Distributed representation(分布式表示)**：即用密集型向量表示这些词汇的含义，它与one-hot的表示完全不同。
  - **Distributional similarity(分布式相似性)**是一种关于词汇语义的理论，可以通过理解单词出现的上下文来描述词汇的意思。这是一个**天才的想法**，如果想知道一个词的含义，我们只需去观察几个包含这个词的例句，即根据这个词的上下文就可以推断这个词的含义。
  - 成功的应用在**早期的 statistical NLP 中**，n-gram 模型
  - **深度学习的解决方案**：学习中心词的周围词出现的概率，并据此建立损失函数，从而来不断调整中心词的词向量表征。



### 词向量的获取方式

词向量的获取方式可以大体分为两类：一类是**基于统计方法**（例如：基于共现矩阵、SVD），另一种是**基于语言模型**的。

- **基于共现矩阵 co-occurrence vectors**
  - 通过统计一个事先指定大小的窗口内的word共现次数，以word周边的共现词的次数做为当前word的vector。具体来说，我们通过从大量的语料文本中构建一个共现矩阵来定义word representation。
  - Problems with simple co-occurrence vectors
    1. Increase in size with vocabulary
    2. Very high dimensional: require a lot of storage
    3. 矩阵定义的词向量在一定程度上缓解了one-hot向量相似度为0的问题，但没有解决数据稀疏性和维度灾难的问题。

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/YiWvzj.png" alt="YiWvzj" style="zoom:25%;" />

- ##### **SVD（奇异值分解）**

  - 既然基于co-occurrence矩阵得到的离散词向量存在着高维和稀疏性的问 题，一个自然而然的解决思路是对原始词向量进行降维，从而得到一个稠密的连续词向量。Idea: **store “most” of the important information** in a ﬁxed, small number of dimensions: a dense vector
  - Problems with SVD
    1. 需要手动去掉停用词（如although, a,...），不然这些频繁出现的词会影响矩阵分解的效果。
    2. huge complex operation to carry out the SVD to find these lower rank representations
    3. Hard to incorporate new words or documents, add one more word in the vocabulary have to do whole co-occurence matrix and also do the whole SVD. 
    4. Bad for millions of words or documents

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/JkHoL4.png" alt="JkHoL4" style="zoom:25%;" />

- ##### **语言模型**
  - N-gram模型
  - 后续的word2vec模型



### 基于词向量的固定表征 (静态词向量)

#### 什么是静态词向量

- 词向量分为**静态词向量**和**动态(上下文)词向量**，静态词向量指的是一个单词不管上下文如何变化只有一个唯一的词向量表示，所以它最大的缺点是无法表达多意性，动态词向量指的会根据上下文动态适应性的调整词向量，可以一定程度上解决单词多意性。

- 一般而言，动态词向量会归类为预训练模型，例如elmo，bert模型。但是其实严格来说，不管静态还是动态词向量本身都是一种预训练的思想， 即第一阶段训练词向量，第二阶段根据使用预训练的词向量并根据需要是否进行fine-tuning。
- 应用于下游任务例如问答、翻译、情感分析的时候，**词向量不用随机的初始化，而是采用从预训练词表里查表初始化即可**。
- 那么Word2Vec有什么问题呢？**<font color=red>其实问题就在于词向量训练的时候有一个假设：将语义相似的词分配相似的词向量，以确保它们在向量空间的距离尽可能的近(一义多词)</font>**。所以问题就在于通过这种方式训练之后的词向量是静态的，上下文无关的，**不能解决一词多义**。这实际上是一个比较大的问题，因为多义词在语言中还是非常见的，也是语言灵活性和高效性的一种体现。

#### word2vec[2013]

- word2vec包含了**两个模型**（skip-gram与CBOW），还包含了**两种高效的训练算法**（层级softmax与negative sampling）

![GhtO5p](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/GhtO5p.png)

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/BaoZnA.png" alt="BaoZnA" style="zoom:25%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/B19OM0.png" alt="B19OM0" style="zoom:25%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ylnXvv.png" alt="ylnXvv" style="zoom:25%;" />

![ub0zcL](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ub0zcL.png)

- **hierarchical softmax 实质上生成一棵带权路径最小的哈夫曼树，让高频词搜索路径变小；把路径上经过的每个节点视为一次二分类，那么最终的概率就是路径上所有二分类概率的乘积**。

- **negative sampling更为直接，实质上对每一个样本中每一个词都进行负例采样。**

- **Skip-gram**：在each estimation step，都取一个词作为中心词（比如这里的banking），然后尝试去预测它一定范围内的上下文词汇。故 *Skip-gram is a probability of a word appearing in the context given this center word。*

  - Truth 表征的是 center words，softmax 得到的结果是 context words

  - The **training objective** is to learn word vector representations that are good at predicting the nearby words. More formally, given a sequence of training words $w_1 , w_2 , w_3 , . . . , w_T$ , the objective of the Skip-gram model is to maximize the average log probability

    $$
    \frac{1}{T} \sum_{t=1}^{T} \sum_{c \leq j \leq c, j \neq 0} \log p\left(w_{t+j} \mid w_{t}\right)
    $$
    
    The basic Skip-gram formulation deﬁnes $p(w_{t+j} \mid w_t )$ using the softmax function:
    
    $$
    p\left(w_{O} \mid w_{I}\right)=\frac{\exp \left(v_{w_{O}}^{\prime} \bar{v}_{w_{I}}\right)}{\sum_{w=1}^{W} \exp \left({v_{w}^{\prime}}^{\top} v_{w_{I}}\right)}
    $$

  - 瓶颈：采用softmax效率太低！分母中的 $W$ 是全量的词表

  - 改进一：**Hierarchical Softmax**

    1. **有进步但还不够好**
    2. A computationally efﬁcient approximation of the full softmax is the hierarchical softmax. **instead of evaluating W output nodes, only about $log_2 (W)$ nodes.** 

  - 改进二：**Negative sampling**

    **Negative sampling** is actually **a simplified model** of an approach called **Noise Contrastive Estimation** (NCE). We train a model **to differentiate the target word from noise**. We can thus reduce the problem of predicting the correct word to a binary classification task, where the model tries to distinguish positive, genuine data from noise samples. 

    <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/wibZEy.jpg" alt="wibZEy" style="zoom:50%;" />

     As we need labels to perform our binary classification task, we designate all correct words $w_i$ given their **context** $c_i$ as **true** ($y=1$) and **all noise samples** $\tilde{w}_{i k}$ as **false** ($y=0$).

    We can now use logistic regression **to minimize the negative log-likelihood**, i.e. cross-entropy of our training examples against the noise
    
    $$
    J_{\theta}=-\sum_{w_{i} \in V}\left[\log P\left(y=1 \mid  w_{i}, c_{i}\right)+k \mathbb{E}_{\tilde{w}_{i k} \sim Q}\left[\log P\left(y=0 \mid  \tilde{w}_{i j}, c_{i}\right)\right]\right]
    $$
    
    **上式的含义：中心词 $w_{i}$ 与背景词 $c_{i}$ 同时出现 $(y=1)$ 概率，且中心词 $w_{i}$ 与噪音词 $\tilde{w}_{ij}$ 不同时出现 $(y=0)$ 的概率**

    **Instead of computing the expectation** $\mathbb{E}_{\tilde{w}_{i k} \sim Q}$ of our noise samples, which would still require summing over all words in V to predict the normalised probability of a negative label, we can again **take the mean with the Monte Carlo approximation**:
    
    $$
    J_{\theta}=-\sum_{w_{i} \in V}\left[\log P\left(y=1 \mid  w_{i}, c_{i}\right)+k \sum_{j=1}^{k} \frac{1}{k} \log P\left(y=0 \mid  \tilde{w}_{i j}, c_{i}\right)\right]
    $$
    
    which reduces to:
    
    $$
    J_{\theta}=-\sum_{w_{i} \in V}\left[\log P\left(y=1 \mid  w_{i}, c_{i}\right)+\sum_{j=1}^{k} \log P\left(y=0 \mid  \tilde{w}_{i j}, c_{i}\right)\right]
    $$
    
    By generating $k$ noise samples for every genuine word $w_i$ given its context $c$, we are effectively sampling words from **two different distributions**: **Correct** words are sampled from the empirical distribution of the **training** set $P_{train}$ and depend on their context $c$, whereas **noise samples** come from the **noise distribution** $Q$. We can thus represent **the probability of sampling either a positive or a noise sample as a mixture of those two distributions**, which are weighted based on the number of samples that come from each:
    
    $$
    P(y, w \mid  c)=\frac{1}{k+1} P_{\text {train }}(w \mid  c)+\frac{k}{k+1} Q(w)
    $$
    
    Given this mixture, we can now calculate the probability that a sample came from the training $P_{train}$ distribution as a conditional probability of $y$ given ww and $c$:
    
    $$
    P(y=1 \mid  w, c)=\frac{\frac{1}{k+1} P_{\operatorname{train}}(w \mid  c)}{\frac{1}{k+1} P_{\operatorname{train}}(w \mid  c)+\frac{k}{k+1} Q(w)}
    $$
    
    which can be simplified to:
    
    $$
    P(y=1 \mid  w, c)=\frac{P_{\operatorname{train}}(w \mid  c)}{P_{\operatorname{train}}(w \mid  c)+k Q(w)}
    $$
    
    As we don't know $P_{train}$  (which is what we would like to calculate), we replace $P_{train}$  with the probability of our model $P$
    
    $$
    P(y=1 \mid  w, c)=\frac{P(w \mid  c)}{P(w \mid  c)+k Q(w)}
    $$

    Note that computing $P(w \mid c)$, i.e. the probability of a word ww given its context $c$ is essentially the definition of our softmax:
    
    $$
    P(w \mid c)=\frac{\exp \left(h^{\top} v_{w}^{\prime}\right)}{\sum_{w_{i} \in V} \exp \left(h^{\top} v_{w_{i}}^{\prime}\right)}=\frac{\exp \left(h^{\top} v_{w}^{\prime}\right)}{Z(c)}
    $$
    
    Having to compute $P(w \mid c)$ means that -- again -- we need to compute $Z(c)$, which **requires us to sum over the probabilities of all words** in $V$. In the case of NCE, there exists **a neat trick to circumvent this issue**: We can treat the normalisation denominator $Z(c)$ as a parameter that the model can learn.
    Mnih and Teh (2012) and Vaswani et al. (2013) [[16\]](https://ruder.io/word-embeddings-softmax/index.html#fn16) actually keep $Z(c)$ **fixed at 1, which they report does not affect the model's performance**. This assumption has the nice side-effect of reducing the model's parameters, while ensuring that the model self-normalises by not depending on the explicit normalisation in $Z(c)$. Indeed, Zoph et al. (2016) [[17\]](https://ruder.io/word-embeddings-softmax/index.html#fn17) find that even when learned, $Z(c)$ is **close to 1 and has low variance**.
    
    $$
    P(w \mid  c)=\exp \left(h^{\top} v_{w}^{\prime}\right)
    $$

    $$
    P(y=1 \mid  w, c)=\frac{\exp \left(h^{\top} v_{w}^{\prime}\right)}{\exp \left(h^{\top} v_{w}^{\prime}\right)+k Q(w)}
    $$

    $$
    J_{\theta}=-\sum_{w_{i} \in V}\left[\log \frac{\exp \left(h^{\top} v_{w_{i}}^{\prime}\right)}{\exp \left(h^{\top} v_{w_{i}}^{\prime}\right)+k Q\left(w_{i}\right)}+\sum_{j=1}^{k} \log \left(1-\frac{\exp \left(h^{\top} v_{\tilde{w}_{i j}}^{\prime}\right)}{\exp \left(h^{\top} v_{\tilde{w}_{i j}}^{\prime}\right)+k Q\left(\tilde{w}_{i j}\right)}\right)\right]
    $$

##### Skipgram补充

它不根据前后文(前后单词)来猜测目标单词，而是推测当前单词可能的前后单词。我们设想一下滑动窗在训练数据时如下图所示。这里粉框颜色深度呈现不同，是因为滑动窗给训练集产生了4个独立的样本。

![5B7fLu](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/5B7fLu.png)

![eziqMp](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/eziqMp.png)

The model conducts the three steps and outputs a prediction vector (with a probability assigned to each word in its vocabulary). Since the model is untrained, it’s prediction is sure to be wrong at this stage. But that’s okay. We know what word it should have guessed – the label/output cell in the row we’re currently using to train the model:

目标单词概率为1，其他所有单词概率为0，这样数值组成的向量就是“目标向量”。模型的偏差有多少？将两个向量相减，就能得到偏差向量。现在这一误差向量可以被用于更新模型了，所以在下一轮预测中，如果用not作为输入，我们更有可能得到thou作为输出了。

![n6U8qn](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/n6U8qn.png)

**以上确实有助于我们理解整个流程，但这依然不是word2vec真正训练的方法。我们错过了一些关键的想法。**

- Negative Sampling

Recall the three steps of how this neural language model calculates its prediction:

![cXhxG8](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/cXhxG8.png)

The third step is very expensive from a computational point of view – especially knowing that we will do it once for every training sample in our dataset (easily tens of millions of times). We need to do something to improve performance.

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/DCSLFg.png" alt="DCSLFg" style="zoom:50%;" />

**This simple switch changes the model we need from a neural network, to a logistic regression model – thus it becomes much simpler and much faster to calculate.**

This switch requires we switch the structure of our dataset – the label is now a new column with values 0 or 1. They will be all 1 since all the words we added are neighbors.

![9jI5zw](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/9jI5zw.png)

现在的计算速度可谓是神速啦——在几分钟内就能处理数百万个例子。但是我们还需要解决一个漏洞。如果所有的例子都是邻居（目标：1），我们这个”天才模型“可能会被训练得永远返回1——准确性是百分百了，但它什么东西都学不到，只会产生垃圾嵌入结果。

**为了解决这个问题，我们需要在数据集中引入负样本 - 不是邻居的单词样本**。我们的模型需要为这些样本返回0。模型必须努力解决这个挑战——而且依然必须保持高速。

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ervmYX.png" alt="ervmYX" style="zoom:50%;" />

但是我们作为输出词填写什么呢？**我们从词汇表中随机抽取单词**

![9RbWiO](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/9RbWiO.png)

这个想法的灵感来自噪声对比估计。我们将实际信号（相邻单词的正例）与噪声（随机选择的不是邻居的单词）进行对比。这导致了计算和统计效率的巨大折衷。

##### **Word2vec训练流程**

在训练过程开始之前，我们预先处理我们正在训练模型的文本。在这一步中，我们确定一下词典的大小（我们称之为vocab_size，比如说10,000）以及哪些词被它包含在内。

在训练阶段的开始，我们**创建两个矩阵：Embedding矩阵和Context矩阵**。这两个矩阵在我们的词汇表中嵌入了每个单词（所以vocab_size是他们的维度之一）。第二个维度是我们希望每次嵌入的长度（embedding_size：300是一个常见值，但我们在前文也看过50的例子）。

![5DtDci](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/5DtDci.png)

在训练过程开始时，我们用**随机值初始化这些矩**阵。然后我们开始训练过程。在每个训练步骤中，我们采取一个相邻的例子及其相关的非相邻例子。我们来看看我们的第一组：

![mG3LmN](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/mG3LmN.png)

Now we have **four** words: **the input word** `not` and **output/context words**: `thou` (the **actual neighbor**), `aaron`, and `taco` (the **negative examples**). We proceed to look up their embeddings – for the **input word**, we look in the `Embedding` matrix. For the **context words**, we look in the `Context` matrix (even though both matrices have an embedding for every word in our vocabulary).

![jfW9Ul](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/jfW9Ul.png)

Then, we **take the dot product of the input embedding with each of the context embeddings**. In each case, that would result in a number, that number indicates the similarity of the input and context embeddings. 

Now we need a way to **turn these scores into something that looks like probabilities** – we need them to all be positive and have values between zero and one. This is a great task for [sigmoid](https://jalammar.github.io/feedforward-neural-networks-visual-interactive/#sigmoid-visualization), the [logistic operation](https://en.wikipedia.org/wiki/Logistic_function).

![SwnKS8](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/SwnKS8.png)

现在我们可以将sigmoid操作的输出视为这些示例的模型输出。您可以看到taco得分最高，aaron最低，无论是sigmoid操作之前还是之后。既然未经训练的模型已做出预测，而且我们确实拥有真实目标标签来作对比，那么让我们**计算模型预测中的误差**吧。为此我们只需**从目标标签中减去sigmoid分数**。

![lvmC16](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/lvmC16.png)

Here comes the “learning” part of “machine learning”. We can now **use this error score to adjust the embeddings** of `not`, `thou`, `aaron`, and `taco` so that the next time we make this calculation, the result would be closer to the target scores.

![hPGMEV](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/hPGMEV.png)

训练步骤到此结束。我们从中得到了这一步所使用词语更好一些的嵌入（not，thou，aaron和taco）。我们现在进行下一步（下一个相邻样本及其相关的非相邻样本），并再次执行相同的过程。当我们循环遍历整个数据集多次时，嵌入会继续得到改进。然后我们就可以停止训练过程，**丢弃Context矩阵，并使用Embeddings矩阵作为下一项任务的已被训练好的嵌入**。



#### GloVe[2014]

- GloVe的全称叫**Global Vectors for Word Representation**，它是一个基于**全局词频统计**（count-based & overall statistics）的词表征（word representation）工具。**glove则是基于全局语料库、并结合上下文语境构建词向量，结合了LSA和word2vec的优点。**
  
- **idea: Combining the best of both worlds**, combines the advantages of the **two major model families** in the literature: **global matrix factorization** (such as LSA) and **local context window methods** (such as skip-gram model).
  
  - Currently, **both families suffer signiﬁcant drawbacks**. While methods like LSA efﬁciently leverage statistical information, they do relatively poorly on the word analogy task (因为**most frequent words contribute a disproportionate amount to the similarity measure**), indicating a sub-optimal vector space structure. Methods like skip-gram may do better on the analogy task, but they poorly utilize the statistics of the corpus since they **train on separate local context windows instead of on global co-occurrence counts**.
  
- GloVe是如何实现的？

  - 第一步：根据语料库（corpus）构建一个共现矩阵（Co-ocurrence Matrix）$P$，**矩阵中的每一个元素 $P_{ij}$ 代表单词 i 和上下文单词 j 在特定大小的上下文窗口（context window）内共同出现的次数。**一般而言，这个次数的最小单位是1，但是GloVe不这么认为：它根据两个单词在上下文窗口的距离 d，提出了一个衰减函数（decreasing weighting）：$decay=1/d$ 用于计算权重，也就是说**距离越远的两个单词所占总计数（total count）的权重越小**。

    *In all cases we use a decreasing weighting function, so that word pairs that are d words apart contribute 1/d to the total count.*

  - 第二步：构建词向量（Word Vector）和共现矩阵（Co-ocurrence Matrix）之间的近似关系，论文的作者提出以下的公式可以近似地表达两者之间的关系，其中 $u_{i}$ 是 word vectors，${v}_{j}$ 是 separate context word vectors. 
    
    $$
    u_{i}^{T} {v}_{j}+b_{i}+\tilde{b}_{j}=\log \left(P_{i j}\right)
    $$

  - 第三部：构造它的loss function：


<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/0sW6MI.png" alt="0sW6MI" style="zoom:25%;" />

- GloVe中cost function的由来

  - 【GloVe: Global Vectors for Word Representation】
- 理解这个表格的重点在最后一行，它表示的是两个概率的比值（ratio），**我们可以使用它观察出两个单词i和j相对于单词k哪个更相关（relevant）。**比如，ice和solid更相关，而stream和solid明显不相关，于是我们会发现 $P(solid \mid ice)/P(solid \mid steam)$ 比1大更多。同样的gas和steam更相关，而和ice不相关，那么 $P(gas \mid ice)/P(gas \mid steam)$ 就远小于1；当都有关（比如water）或者都没有关(fashion)的时候，两者的比例接近于1；这个是很直观的。因此，**以上推断可以说明通过概率的比例而不是概率本身去学习词向量可能是一个更恰当的方法**
  

![5AKypf](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/5AKypf.png)

  - 为了捕捉上面提到的概率比例，我们可以构造如下函数
    
    $$
    F\left(w_{i}, w_{j}, \tilde{w}_{k}\right)=\frac{P_{i k}}{P_{j k}}
    $$
    
    Since vector spaces are **inherently linear structures**, the most natural way to do this is with vector differences. 其中，函数 $F$ 的参数和具体形式未定，它有三个参数 $w_i,w_j$ 和 $\tilde{w_k}$，$w$ 和 $\tilde{w}$ 是不同的向量；因为向量空间是线性结构的，所以要表达出两个概率的比例差，最简单的办法是作差，于是我们得到：
    
    $$
    F\left(w_{i}-w_{j}, \tilde{w}_{k}\right)=\frac{P_{i k}}{P_{j k}}
    $$
    
    上述公式的右侧是一个数量，而左侧则是一个向量，于是我们把左侧转换成两个向量的内积形式：
    
    $$
    F\left(\left(w_{i}-w_{j}\right)^{T} \tilde{w}_{k}\right)=\frac{P_{i k}}{P_{j k}}
    $$
    
    为了满足对称性的条件，首先，我们要求函数 $F$ 要满足同态特性（homomorphism）
    
    $$
    F\left(w_{i}^{T} \tilde{w}_{k}\right)=P_{i k}=\frac{X_{i k}}{X_{i}}
    $$
    
    $$
    F\left(\left(w_{i}-w_{j}\right)^{T} \tilde{w}_{k}\right)=\frac{F\left(w_{i}^{T} \tilde{w}_{k}\right)}{F\left(w_{j}^{T} \tilde{w}_{k}\right)}
    $$
    
    $$
    w_{i}^{T} \tilde{w}_{k}=\log \left(P_{i k}\right)=\log \left(X_{i k}\right)-\log \left(X_{i}\right)
    $$
    
    此时，我们发现因为等号右侧的 $log(X_i)$ 的存在，公式是不满足对称性（symmetry）的，而且这个 $log(Xi)$ 其实是跟 $k$ 独立的，它只跟 $i$ 有关，于是我们可以针对 $w_i$ 增加一个bias term $b_i$ 把它替换掉

    $$
    w_{i}^{T} \tilde{w}_{k}+b_{i}+\tilde{b}_{k}=\log \left(X_{i k}\right)
    $$

    A main drawback to this model is that it **weighs all co-occurrences equally**, even those that happen rarely or never.

    $$
    J=\sum_{i, j=1}^{V} f\left(X_{i j}\right)\left(w_{i}^{T} \tilde{w}_{j}+b_{i}+\tilde{b}_{j}-\log X_{i j}\right)^{2}
    $$

    In fact, Mikolov et al. (2013a) observe that performance can be increased by ﬁltering the data so as to reduce the effective value of the weighting factor for frequent words.

    $$
    \hat{J}=\sum_{i, j} f\left(X_{i j}\right)\left(w_{i}^{T} \tilde{w}_{j}-\log X_{i j}\right)^{2}
    $$

- **What to do with the two sets of vectors?**
  - 事实表明，在优化期间有独立的向量并且最后把他们整合到一起会更稳定。
  - 这与word2vec中对center words与context wrods维护独立的向量是一个道理

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/4iriq7.png" alt="4iriq7" style="zoom:25%;" />

#### fastText[2016]

- fastText是Facebook于2016年开源的一个词向量计算和文本分类工具，在学术上并没有太大创新。但是它的优点也非常明显，在文本分类任务中，**fastText（浅层网络）往往能取得和深度网络相媲美的精度，却在训练时间上比深度网络快许多数量级**。在标准的多核CPU上， 能够训练10亿词级别语料库的词向量在10分钟之内，能够分类有着30万多类别的50多万句子在1分钟之内。

- 字符级别的n-gram

  - **上面说的词向量一般来说都是对于word级别的**，但是自然语言是在不断发展变化的，特别是各种网络词，那么**以word为原子单位进行embedding势必会有OOV（out of vocabulary）问题**，subword可以在一定程度上缓解该问题。

  - word2vec把语料库中的每个单词当成**原子的**，它会为每个单词生成一个向量。这忽略了单词内部的形态特征，比如：“apple” 和“apples”，这个例子中，两个单词都有较多公共字符，即它们的内部形态类似，但是**在传统的word2vec中，这种单词内部形态信息因为它们被转换成不同的id丢失了**。

    为了克服这个问题，**fastText使用了字符级别的n-grams来表示一个单词**。对于单词“apple”，假设n的取值为3，则它的trigram有 “<ap”, “app”, “ppl”, “ple”, “le>” 其中，<表示前缀，>表示后缀。于是，我们可以用这些trigram来表示“apple”这个单词，进一步，我们可以用这5个trigram的向量叠加来表示“apple”的词向量。

- **fastText的核心思想就是**：将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。这中间涉及到**两个技巧**：字符级n-gram特征的引入以及分层Softmax分类。

- 【Enriching Word Vectors with Subword Information/Bag of Tricks for Efﬁcient Text Classiﬁcation】

  - 是skip-gram/cbow模型的扩展

  - the skipgram model ignores the internal structure of words 

  - fastText模型也只有三层：输入层、隐含层、输出层（Hierarchical Softmax），输入都是多个经向量表示的单词，输出都是一个特定的target，隐含层都是对多个词向量的叠加平均。不同的是，CBOW的输入是目标单词的上下文，fastText的输入是多个单词及其n-gram特征，这些特征用来表示单个文档；CBOW的输入单词被onehot编码过，fastText的输入特征是被embedding过；CBOW的输出是目标词汇，fastText的输出是文档对应的类标。

  - In 1), Bojanowski et al. use the same skipgram model with negative sampling from [Mikolov et al.](https://arxiv.org/abs/1310.4546) to score similarity between a fixed target word wt and a word within the context wc. The difference is that **the similarity function is** not a direct dot product between the word vectors but rather **a dot product between two words represented as sums of n-gram vectors**. 

  - In 2), for text classification, each word is represented as a sum of n-grams, and then **a latent text embedding is derived as an average of the word embeddings**, and the text embedding is used to predict the label. The overall objective function is to minimize ,

    $$
    -\frac{1}{N} \sum_{n=1}^{N} y_{n} \log \left(f\left(B A x_{n}\right)\right)
    $$
    
    **$x_n$ represents an n-gram feature, $A$ represents the lookup table to an average text embedding, and $B$ converts the embedding to pre-softmax values for each class**. A hierarchical softmax is applied given the large number of classes to minimize computational complexity.

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/5Ed4zt.png" alt="5Ed4zt" style="zoom: 50%;" />

