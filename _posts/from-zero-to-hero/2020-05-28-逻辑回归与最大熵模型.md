---
title: "逻辑回归与最大熵模型"
subtitle: "Logistic Regression and Maximum Entropy Model"
layout: post
author: "echisenyang"
header-style: text
hidden: false
catalog: true
tags:
  - 输出计划
  - 李航统计学习第二版
---



### 最大熵模型追求熵增，但是决策树却追求熵减，是否矛盾？

最大熵和决策树，两个模型中关于熵的定义完全一样，均用来表征模型的有序程度。熵值越大，越是无序，不确定性越大，数据机纯度越低。

但是，两个模型的基本原理并不矛盾，理由如下：

- **二者应用的前提不同**
  1. 对最大熵模型而言，在所有满足约束条件的模型中，**如果没有其他的参考信息，则选用熵最大的模型**；**即在没有更多信息的情况下，认为那些不确定的部分都是“等可能”的。**最大熵原理通过熵的最大化来表示等可能性。
  2. 而决策树模型中，**由于提供了特征属性这样的额外参考信息，每个特征属性有自己的条件分布，自然不满足“等可能性”，因此不能直接应用最大熵原理**。

- **决策树并没有使用最小熵模型**
  1. 我们都知道，完全生长决策树的熵是最小的，然而却常常不是最好的模型（容易“过拟合”），经过剪枝后的决策树反而能够反映真实数据分布。**如果说树的分裂意味着熵的减小，则剪枝意味着熵的增加；这样看来，我们选择的其实是应用了所有已知信息之后熵较大的模型。**

附：可以从物理学的角度来理解最大熵模型：**根据热力学第二定理，如果没有外力干扰，系统的熵值是趋于不断增加的。由此，在没有其它额外参考信息的情况下，选择熵值最大的模型是最可靠的**，因为没有外在动力时，宇宙本来就是趋于无序的啊。

![Uxcy7e](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Uxcy7e.jpg)







### Linear Regression vs Logistic Regression

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/pKFgxq.jpg)

**Main difference between them is how they are being used**

- The Linear Regression is used for **solving Regression problems** whereas Logistic Regression is used for **solving the Classification problems**. 

<table class="alt">
<tbody><tr>
	<th>Linear Regression</th>
	<th>Logistic Regression</th>
</tr>
<tr>
  <td>Linear regression is used to predict the <b>continuous dependent variable</b> using a given set of independent variables.</td>
  <td>Logistic Regression is used to predict the <b>categorical dependent variable </b> using a given set of independent variables.</td>
</tr>
<tr>
  <td>Linear Regression is used for <b>solving Regression problem</b>.</td>
  <td>Logistic regression is used for <b>solving Classification problems</b>.</td>
</tr>
<tr>
  <td>In Linear regression, we predict the value of <b>continuous variables</b>.</td>
  <td>In logistic Regression, we predict the values of <b>categorical variables</b>.</td>
</tr>
<tr>
  <td>In linear regression, we find <b>the best fit line</b>, by which we can easily predict the output.</td>
  <td>In Logistic Regression, we find <b>the S-curve</b> by which we can classify the samples.</td>
</tr>
<tr>
  <td><b>Least square estimation method</b> is used for estimation of accuracy.</td>
  <td><b>Maximum likelihood estimation method</b> is used for estimation of accuracy.</td>
</tr>
<tr>
  <td>The output for Linear Regression must be a <b>continuous value</b>, such as price, age, etc.</td>
	<td>The output of Logistic Regression must be a <b>Categorical value</b> such as 0 or 1, Yes or No, etc.</td>
</tr>
<tr>
	<td>In Linear regression, it is required that relationship between dependent variable and independent variable must be linear.</td>
	<td>In Logistic regression, it is not required to have the linear relationship between the dependent and independent variable.</td>
</tr>
<tr>
	<td>In linear regression, there may be collinearity between the independent variables.</td>
	<td>In logistic regression, there should not be collinearity between the independent variable.</td>
</tr>
</tbody></table>


### 逻辑回归（对数几率回归）迭代法求解：牛顿法+梯度下降

![b4RPiL](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/b4RPiL.png)

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/wYMvHe.jpg" alt="b4RPiL" style="zoom: 33%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Ef30sB.jpg" alt="b4RPiL" style="zoom:33%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Kckl7a.jpg" alt="b4RPiL" style="zoom:33%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/N9t7yl.jpg" alt="b4RPiL" style="zoom:33%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/QotN3P.jpg" alt="b4RPiL" style="zoom: 25%;" />

### 细节

#### 1.求梯度与并行化计算

![6.逻辑回归-1](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/6.逻辑回归-1.jpg)

![6.逻辑回归-2](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/6.逻辑回归-2.jpg)

#### 2.为什么不用均方误差？

**[The cost function used in linear regression won't work here](https://www.internalpointers.com/post/cost-function-logistic-regression)**

- If we try to use the cost function of the linear regression (**均方误差，即最小二乘法**) in Logistic Regression $\sum^m_{i=1}(y^{(i)}-\frac{1}{1+e^{-\theta^T x}})^2$, then it would be of no use as it would **end up being a non-convex function with many local minimums**, in which it would be very **difficult to minimize the cost value and find the global minimum**.

![](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/LQSAIt.jpg)

- 逻辑回归采用的是**交叉熵损失函数**

$$
cost\left(h_{\theta}(x), y\right)=\left\{\begin{array}{ll}{-\log \left(h_{\theta}  
(x)\right)} & {\text { if } y=1} \\ {-\log \left(1-h_{\theta}(x)\right)} & {\text { if } y=0}\end{array}\right.
$$

$$
\begin{aligned} J(\theta) &=\frac{1}{m} \sum_{i=1}^{m} 
cost\left(h_{\theta}\left(x^{(i)}\right), y^{(i)}\right) \\ &=-\frac{1}{m}\left[\sum_{i=1}^{m} y^{(i)} \log    
\left(h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]   \end{aligned}
$$

#### 3.为什么适合离散特征？

我们在使用逻辑回归的时候很少会把数据直接丢给 LR 来训练，我们一般会对特征进行离散化处理，这样做的优势大致有以下几点：

1. 离散后稀疏向量内积乘法运算速度更快，计算结果也方便存储，容易扩展；
2. **离散后的特征对异常值更具鲁棒性，如 age>30 为 1 否则为 0，对于年龄为 200 的也不会对模型造成很大的干扰；**
3. **LR 属于广义线性模型，表达能力有限，经过离散化后，每个变量有单独的权重，这相当于引入了非线性，能够提升模型的表达能力，加大拟合**；
4. 离散后特征可以进行特征交叉，提升表达能力，由 M+N 个变量编程 M*N 个变量，进一步引入非线形，提升了表达能力；
5. **特征离散后模型更稳定，如用户年龄区间，不会因为用户年龄长了一岁就变化**；

总的来说，特征离散化以后起到了加快计算，简化模型和**增加泛化能力**的作用。

### 代码实现

```python
class LogisticReressionClassifier:
    def __init__(self, max_iter=200, learning_rate=0.01):
        self.max_iter = max_iter
        self.learning_rate = learning_rate
        
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))
    
    def fit(self, X, y):
        X = np.hstack((np.ones((X.shape[0],1)),X))
        self.weights = np.zeros((X.shape[1], 1))
        y = np.expand_dims(y, axis=1)
        
        for iter_ in range(self.max_iter):
            h = self.sigmoid(np.dot(X, self.weights))
            error = y - h
            self.weights = self.weights + self.learning_rate * np.dot(X.T, error)
            
    def predict(self, x):
        x = np.hstack((np.ones((x.shape[0],1)),x))
        pred = self.sigmoid(np.dot(x, self.weights))
        if pred > 0:
            return 1
        else:
            return 0
    
    def score(self, X_test, y_test):
        right = 0
        X_test = np.hstack((np.ones((X_test.shape[0],1)),X_test))
        res = np.dot(X_test, self.weights)
        for (result, y) in zip(res, y_test):
            if (result > 0 and y == 1) or (result < 0 and y == 0):
                right += 1
        return right / len(X_test)
```

![ozKlMl](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ozKlMl.jpg)