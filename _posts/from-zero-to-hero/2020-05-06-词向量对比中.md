---
title: "词向量对比「中」"
subtitle: "Contextual Word Embeddings「CS224N Winter 2019」"
layout: post
author: "echisenyang"
header-style: text
hidden: true
catalog: true
tags:
  - 输出计划
---



![I3FNOP](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/I3FNOP.png)

### 1. Representations for a word

Originally, we basically had one representation of words:

- The word vectors that we learned about at the beginning: Word2vec, GloVe, fastText

These have two problems:

- Always the same representation for a word type regardless of the context in which a word token occurs. 静态词向量的共性问题，即**一个单词不管上下文如何变化只有一个唯一的词向量表示**，所以它最大的缺点是**无法表达多意性**。**根源在于词向量训练的时候有一个假设**：将语义相似的词分配相似的词向量，以确保它们在向量空间的距离尽可能的近 (**将单词所有的含义叠加在一起**)。
- We just have one representation for a word, but words have different aspects, including semantics, syntactic behavior, and register/connotations. 同一个单词的多种形态，无法由同一个向量来表示，而是由不同的词向量来表达，这**丢失了某种句法或者语义信息 (比如arrival与arrived)**。

实际上，我们在之前的LSTM模型中，就已经用到了特定单词的上下文表达形式，比如下图的隐层状态。

![kG9o8z](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/kG9o8z.png)

### 2. Pre-ELMo and ELMo

#### Pre-ELMo

![TRH6IN](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/TRH6IN.png)

![Ifc6b5](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Ifc6b5.png)

#### pre-training

大数据不需要预训练，小数据需要。

- **如果财力支撑不了一个大的训练集的话，老老实实pretrain吧**，论文暗示的一个点是：pretrain准确率不会下降的，速度还会提高，做项目就要追求这种简单好用的东西。另外，我做项目也没有像论文所说的那样会固定pretrain网络的参数，依然是trainable的，我的**实验结果表示没固定参数比固定参数的要好**，而**论文的对比实验里pretrain网络的参数是固定的**。

- **pretrain能够加速拟合，缩短训练时间**。做项目的时候经常要花一天时间使得模型收敛，这种情况下调个参都得老久了，这是不利于我们这种干项目的人的，时间不等人啊。所以我希望调参的时间越短越好，**最好的莫过于在已经收敛的网络上把参数改改**，这样很快又能达到新的拟合了，**次好的就是用pretrain的结果了**，**最次的就是随机初始化了**
- pretrain难度不大。日常使用TensorFlow。各种pretrain的资源很多。这里再安利一个Google的结果[tensorflow/models](https://link.zhihu.com/?target=https%3A//github.com/tensorflow/models/tree/master/research/slim)，里面有经典的网络的checkpoint。

回到Tag LM，pre-trained bi-LM 一旦预训练完毕，其中的参数会被 frozen，当加入左边具体的task时，这部分的参数会被设置成untrainable（学术界的做法）。

#### ELMo

![o7lWuz](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/o7lWuz.png)

![dZgtbg](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/dZgtbg.png)

![rFuSWP](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/rFuSWP.png)

![NdZN1O](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/NdZN1O.png)

### 3. Transformer models

![MreqHi](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/MreqHi.png)

### 4.Illustrated bert

2018年可谓是自然语言处理（NLP）的元年，在我们如何以**最能捕捉潜在语义关系的方式**  **来辅助计算机对的句子概念性的理解** 这方面取得了极大的发展进步。此外， NLP领域的一些开源社区已经发布了很多强大的组件，我们可以在自己的模型训练过程中免费的下载使用。（可以说今年是NLP的ImageNet时刻，因为这和几年前计算机视觉的发展很相似）

![EwAesh](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/EwAesh.png)

上图中，最新发布的BERT是一个NLP任务的里程碑式模型，它的发布势必会带来一个NLP的新时代。BERT是一个算法模型，它的出现打破了大量的自然语言处理任务的记录。在BERT的论文发布不久后，Google的研发团队还开放了该模型的代码，并提供了一些在大量数据集上预训练好的算法模型下载方式。Goole开源这个模型，并提供预训练好的模型，这使得所有人都可以通过它来构建一个涉及NLP的算法模型，节约了大量训练语言模型所需的时间，精力，知识和资源。
![5k0Dyp](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/5k0Dyp.png)

BERT集成了最近一段时间内NLP领域中的一些顶尖的思想，包括但不限于 [Semi-supervised Sequence Learning](https://arxiv.org/abs/1511.01432) (by [Andrew Dai](https://twitter.com/iamandrewdai) and [Quoc Le](https://twitter.com/quocleix)), [ELMo](https://arxiv.org/abs/1802.05365) (by [Matthew Peters](https://twitter.com/mattthemathman) and researchers from [AI2](https://allenai.org/) and [UW CSE](https://www.engr.washington.edu/about/bldgs/cse)), [ULMFiT](https://arxiv.org/abs/1801.06146) (by fast.ai founder [Jeremy Howard](https://twitter.com/jeremyphoward) and [Sebastian Ruder](https://twitter.com/seb_ruder)), the [OpenAI transformer](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) (by OpenAI researchers [Radford](https://twitter.com/alecrad), [Narasimhan](https://twitter.com/karthik_r_n), [Salimans](https://twitter.com/timsalimans), and [Sutskever](https://twitter.com/ilyasut)), and the Transformer ([Vaswani et al](https://arxiv.org/pdf/1706.03762.pdf)).

#### Example: Sentence Classification

The most straight-forward way to use BERT is to use it to classify a single piece of text. This model would look like this:

![IPgQUU](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/IPgQUU.png)

To train such a model, you **mainly have to train the classifier**, with minimal changes happening to the BERT model during the training phase. This training process is called **Fine-Tuning**, and has roots in [Semi-supervised Sequence Learning](https://arxiv.org/abs/1511.01432) and ULMFiT.

#### Model Architecture

![6p5ZiZ](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/6p5ZiZ.png)

The paper presents two model sizes for BERT:

- **BERT BASE** – Comparable in size to the OpenAI Transformer in order to compare performance
- **BERT LARGE** – A ridiculously huge model which achieved the state of the art results reported in the paper

BERT is basically a trained Transformer Encoder stack. 

![lHFbS9](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/lHFbS9.png)

**Both BERT model sizes have a large number of encoder layers** (which the paper calls Transformer Blocks) – twelve for the Base version, and twenty four for the Large version. These also have **larger feedforward-networks** (768 and 1024 hidden units respectively), and **more attention heads** (12 and 16 respectively) than the default configuration in the reference implementation of the Transformer in the initial paper (6 encoder layers, 512 hidden units, and 8 attention heads).

#### Model Inputs

![h10szK](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/h10szK.png)

The first input token is supplied with a **special [CLS] token** for reasons that will become apparent later on. **CLS here stands for Classification**.

Just like the vanilla encoder of the transformer, **BERT takes a sequence of words as input which keep flowing up the stack**. Each layer applies self-attention, and passes its results through a feed-forward network, and then hands it off to the next encoder.

![piRfme](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/piRfme.png)

#### Model Outputs

**Each position outputs a vector of size *hidden_size*** (768 in BERT Base). For the **sentence classification** example we’ve looked at above, we focus on the output of **only the first position (that we passed the special [CLS] token to)**.

![vZeOTW](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/vZeOTW.png)

**That vector can now be used as the input for a classifier of our choosing. The paper achieves great results by just using a single-layer neural network as the classifier.**

![sr9WUC](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/sr9WUC.png)

If you have **more labels** (for example if you’re an email service that tags emails with ***“spam”, “not spam”, “social”, and “promotion”***), you just **tweak the classifier network to have more output neurons** that then pass through softmax.

#### Parallels with Convolutional Nets（BERT VS 卷积神经网络）

对于那些具有计算机视觉背景的人来说，这个矢量切换应该让人联想到VGGNet等网络的卷积部分与网络末端的完全连接的分类部分之间发生的事情。你可以这样理解，实质上这样理解也很方便。

![1jl72r](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/1jl72r.png)

#### 词嵌入的新时代〜

BERT的开源随之而来的是一种词嵌入的更新。到目前为止，词嵌入已经成为NLP模型处理自然语言的主要组成部分。诸如Word2vec和Glove 等方法已经广泛的用于处理这些问题，在我们使用新的词嵌入之前，我们有必要回顾一下其发展。

---

##### Word Embedding Recap

为了让机器可以学习到文本的特征属性，我们需要一些将**文本数值化的表示的方式**。Word2vec算法通过使用一组固定维度的向量来表示单词，计算其方式可以**捕获到单词的语义及单词与单词之间的关系**。使用Word2vec的向量化表示方式可以用于判断单词是否相似，对立，或者说判断“男人‘与’女人”的关系就如同“国王”与“王后”。另外还能捕获到一些语法的关系，这个在英语中很实用。例如“had”与“has”的关系如同“was”与“is”的关系。

这样的做法，我们可以**使用大量的文本数据来预训练一个词嵌入模型，而这个词嵌入模型可以广泛用于其他NLP的任务**，这是个好主意，这使得一些初创公司或者计算资源不足的公司，也能通过下载已经开源的词嵌入模型来完成NLP的任务。

---

##### ELMo: Context Matters

If we’re using this GloVe representation, then the word “stick” would be represented by this vector **no-matter what the context was**. “*stick*”” has multiple meanings depending on where it’s used. **Why not give it an embedding based on the context it’s used in – to both capture the word meaning in that context as well as other contextual information?”**. And so, *contextualized* word-embeddings were born.

**Instead of using a fixed embedding for each word**, EMLo 在为每个单词分配词向量之前先查看整个句子，然后使用bi-LSTM来训练它对应的词向量。

![neem4E](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/neem4E.png)

ELMo为解决NLP的语境问题作出了重要的贡献，它的LSTM可以使用与我们任务相关的大量文本数据来进行训练，然后将训练好的模型用作其他NLP任务的词向量的基准。

---

##### What’s ELMo’s secret?

- **ELMo gained its language understanding from being trained to predict the next word in a sequence of words** - a task called *Language Modeling*. **This is convenient because we have vast amounts of text data that such a model can learn from without needing labels**.

![4r62Gu](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/4r62Gu.jpg)

从上图可以发现，每个展开的LSTM都在最后一步完成预测。真正的ELMo会更进一步，它不仅能判断下一个词，还能预测前一个词。（Bi-Lstm）

![iZtk6C](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/iZtk6C.png)

ELMo comes up with the **contextualized embedding** through **grouping together the hidden states (and initial embedding)** in a certain way (concatenation followed by weighted summation).

![VUX0XK](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/VUX0XK.png)

---

##### ULM-FiT：NLP领域应用迁移学习

ULM-FiT机制让模型的预训练参数得到更好的利用。所利用的参数不仅限于embeddings，也不仅限于语境embedding，ULM-FiT引入了Language Model和一个有效微调该Language Model来执行各种NLP任务的流程。这使得NLP任务也能像计算机视觉一样方便的使用迁移学习。

---

##### The Transformer：超越LSTM的结构

Transformer论文和代码的发布，以及其在机器翻译等任务上取得的优异成果，**让一些研究人员认为它是LSTM的替代品，事实上却是Transformer比LSTM更好的处理long-term dependancies（长程依赖）问题**。Transformer Encoding和Decoding的结构非常适合机器翻译，但是怎么利用他来做文本分类的任务呢？实际上你只用使用它来预训练可以针对其他任务微调的语言模型即可。

---

##### OpenAI Transformer：用于语言模型的Transformer解码器预训练

事实证明，我们并不需要**一个完整的transformer结构来使用迁移学习**和**一个很好的语言模型来处理NLP任务**。我们只需要Transformer的解码器就行了。The decoder is a good choice because it’s a natural choice for language modeling (predicting the next word) since it’s built to mask future tokens – a valuable feature when it’s generating a translation word by word.
![CfvZsN](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/CfvZsN.png)

该模型堆叠了十二个Decoder层。 由于在该设置中没有Encoder，因此这些Decoder将不具有Transformer Decoder层具有的Encoder - Decoder attention层。 然而，取而代之的是一个self attention层（masked so it doesn’t peak at future tokens）。

---

##### Transfer Learning to Downstream Tasks

![hj8t1o](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/hj8t1o.png)

**The OpenAI paper** outlines a number of input transformations to handle the inputs for different types of tasks. The following image from the paper shows the structures of the models and input transformations to carry out different tasks.

![gugPkB](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/gugPkB.png)

---

##### BERT: From Decoders to Encoders

OpenAI transformer为我们提供了基于Transformer的精密的预训练模型。但是从LSTM到Transformer的过渡中，我们发现少了些东西。**ELMo的语言模型是双向的，但是OpenAI的transformer是前向训练的语言模型**。我们能否让我们的Transformer模型也具有Bi-Lstm的特性呢？

![YAzXYX](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/YAzXYX.png)

**解释一下Mask：**

- 语言模型会根据前面单词来预测下一个单词，但是self-attention的注意力只会放在自己身上，那么这样100%预测到自己，毫无意义，所以用Mask，把需要预测的词给挡住。

---

##### BERT: Two-sentence Tasks

To make BERT better at handling relationships between multiple sentences, the pre-training process includes an additional task: Given two sentences (A and B), is B likely to be the sentence that follows A, or not?

![liE0HM](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/liE0HM.png)

---

##### BERT: Task specific-Models

![t0nCqP](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/t0nCqP.png)

##### BERT for feature extraction

微调方法并不是使用BERT的唯一方法，就像ELMo一样，你可以使用预选训练好的BERT来创建语境化词嵌入。然后你可以将这些嵌入提供给现有的模型。

![0awVHT](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/0awVHT.jpg)

**Which vector works best as a contextualized embedding**? I would think it depends on the task. The paper examines six choices (Compared to the fine-tuned model which achieved a score of 96.4):

![sUQl2B](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/sUQl2B.png)

#### Take BERT out for a spin

- [Bert时代的创新：Bert应用模式比较及其它](https://zhuanlan.zhihu.com/p/56382372)

- [A Visual Guide to Using BERT for the First Time](https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)

The best way to try out BERT is through the [BERT FineTuning with Cloud TPUs](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb) notebook hosted on Google Colab. If you’ve never used Cloud TPUs before, this is also a good starting point to try them as well as the BERT code works on TPUs, CPUs and GPUs as well.

The next step would be to look at the code in the [BERT repo](https://github.com/google-research/bert):

- The model is constructed in [modeling.py](https://github.com/google-research/bert/blob/master/modeling.py) (`class BertModel`) and is pretty much identical to a vanilla Transformer encoder.
- [run_classifier.py](https://github.com/google-research/bert/blob/master/run_classifier.py) is an example of the fine-tuning process. It also constructs the classification layer for the supervised model. If you want to construct your own classifier, check out the `create_model()` method in that file.
- Several pre-trained models are available for download. These span BERT Base and BERT Large, as well as languages such as English, Chinese, and a multi-lingual model covering 102 languages trained on wikipedia.
- BERT doesn’t look at words as tokens. Rather, it looks at WordPieces. [tokenization.py](https://github.com/google-research/bert/blob/master/tokenization.py) is the tokenizer that would turns your words into wordPieces appropriate for BERT.

You can also check out the [PyTorch implementation of BERT](https://github.com/huggingface/pytorch-pretrained-BERT). The [AllenNLP](https://github.com/allenai/allennlp) library uses this implementation to [allow using BERT embeddings](https://github.com/allenai/allennlp/pull/2067) with any model.

