---
title: "决策树"
subtitle: "Decision Tree"
layout: post
author: "echisenyang"
header-style: text
hidden: true
catalog: true
tags:
  - 输出计划
  - 李航统计学习第二版
---



### 决策树模型

- 在分类问题中，决策树树形结构表示**基于特征对实例进行分类的过程**
- 可以认为是 **if-then 规则**的集合

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/xAgjlz.png" alt="xAgjlz" style="zoom:50%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/1zBquM.png" alt="1zBquM" style="zoom:50%;" />

- 决策树还可以**表示给定条件下类的条件分布概率**

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/fGbDY2.png" alt="fGbDY2" style="zoom:50%;" />

- 决策树学习
  - **决策树学习目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类**
  - **决策树学习的本质是从训练数据集中归纳出一组分类规则**
  - **决策树学习算法是一个递归的选择最优特征**
    1. 自下而上**生成**对应**局部最优**（**贪心的思想**）
    2. 自上而下**剪枝**对应**全局最优**（**提升泛化能力，防止过拟合现象，使树变得简单**）

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/PYtTa9.png" alt="PYtTa9" style="zoom:50%;" />



### 如何选择最优特征：特征选择

- **特征选择在于选取对训练数据具有分类能力的特征**。
  - 这样可以提高决策树学习的效率。
  - **如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的**。经验上扔掉这样的特征对决策树学习的精度影响不大。

- 究竟选择哪个特征更好些？
  - 通常特征选择的准则是**信息增益或信息增益比**。

![8TpvZn](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/8TpvZn.jpg)

![jhUSNP](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/jhUSNP.png)

#### 1.信息增益（ID3算法）

- 纯度：**希望决策树分支包含的样本尽可能属于同一类别**，即结点的“纯度”越来越高

- 条件熵：**表示已知属性 a 的情况下，样本集 D 的不确定性**
  - 其中，**样本数越多的分支，节点的影响越大**
- 信息增益：**信息增益越大，表示用属性 a 来进行划分所获的的“纯度提升”越大**

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/RW7NAV.png" alt="RW7NAV" style="zoom:50%;" />

#### 【代码实现】

```python
"""数据集：前四个为特征，最后一个为啦label
array([[1, 1, 1, 1, 1],
       [1, 1, 1, 2, 1],
       [1, 0, 1, 2, 0],
       [1, 0, 0, 1, 0],
       [1, 1, 1, 1, 1],
       [2, 1, 1, 1, 1],
       [2, 1, 1, 2, 1],
       [2, 0, 0, 2, 0],
       [2, 1, 0, 3, 0],
       [2, 1, 0, 3, 0],
       [3, 1, 0, 3, 0],
       [3, 1, 0, 2, 0],
       [3, 0, 1, 2, 0],
       [3, 0, 1, 3, 0],
       [3, 1, 1, 1, 1]])
"""

# 熵
def entropy(y):
    N = len(y)
    count = []
    # 计算有几个类别，每个类别的数量
    for value in set(y):
        count.append(len(y[y == value]))
    count = np.array(count)
    entro = -np.sum((count / N) * (np.log2(count / N)))
    return entro

# 条件熵
def cond_entropy(X, y, cond=0):
    """cond对应特征A
    """
    N = len(y)
    cond_X = X[:, cond]
    tmp_entro = []
    for val in set(cond_X):
        tmp_y = y[np.where(cond_X == val)]
        tmp_entro.append(len(tmp_y)/N * entropy(tmp_y))
    cond_entro = sum(tmp_entro)
    return cond_entro

# 信息增益
def info_gain(X, y, cond):
    return entropy(y) - cond_entropy(X, y, cond)

# 信息增益比
def info_gain_ratio(X, y, cond):
    return (entropy(y) - cond_entropy(X, y, cond))/cond_entropy(X, y, cond)


def best_split(X,y, method='info_gain'):
    """根据method指定的方法使用信息增益或信息增益比来计算各个维度的最大信息增益（比），返回特征的axis"""
    _, M = X.shape
    info_gains = []
    if method == 'info_gain':
        split = info_gain
    elif method == 'info_gain_ratio':
        split = info_gain_ratio
    else:
        print('No such method')
        return
    for i in range(M):
        tmp_gain = split(X, y, i)
        info_gains.append(tmp_gain)
    best_feature = np.argmax(info_gains)
    
    return best_feature

def majorityCnt(y):
    """当特征使用完时，返回类别数最多的类别"""
    unique, counts = np.unique(y, return_counts=True)
    max_idx = np.argmax(counts)
    return unique[max_idx]


class DecisionTreeClassifer:
    """
    决策树生成算法，
    method指定ID3或C4.5,两方法唯一不同在于特征选择方法不同
    info_gain:       信息增益即ID3
    info_gain_ratio: 信息增益比即C4.5
    
    
    """
    def __init__(self, threshold, method='info_gain'):
        self.threshold = threshold
        self.method = method
        
    def fit(self, X, y, labels):
        labels = labels.copy()
        M, N = X.shape
        if len(np.unique(y)) == 1:
            return y[0]
        
        if N == 1:
            return majorityCnt(y)
        
        bestSplit = best_split(X,y, method=self.method)
        bestFeaLable = labels[bestSplit]
        Tree = {bestFeaLable: {}}
        del (labels[bestSplit])
        
        feaVals = np.unique(X[:, bestSplit])
        for val in feaVals:
            idx = np.where(X[:, bestSplit] == val)
            sub_X = X[idx]
            sub_y = y[idx]
            sub_labels = labels
            Tree[bestFeaLable][val] = self.fit(sub_X, sub_y, sub_labels)
            
        return Tree
```



#### 2.信息增益比（C4.5算法）

- 信息增益的缺点：
  - 倾向于选择 **属性取值较多的属性** （编号这一属性取值有 N 个，属性取值最多）
    1. 比如，用“**编号**”这一无意义列名来划分属性，它的信息增益达0.998，**远大于**其他候选划分属性
    2. 用“编号”产生的每个分支仅包含一个样本，这些分支节点的纯度已经最大，故信息增益也最大。
  - 可以用**信息增益比**对这个问题进行校正

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/78lwAi.png" alt="78lwAi" style="zoom:50%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/JBLZ79.png" alt="JBLZ79" style="zoom:50%;" />

#### 3.基尼系数（CART算法）

- **基尼系数越小，数据集的纯度越高**

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/BTTr8z.png" alt="BTTr8z" style="zoom:50%;" />

#### 三种算法的比较

- ID3

  - **缺点**
    1. ID3 没有剪枝策略，容易过拟合；
    2. 信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于 1；
    3. 只能用于处理离散分布的特征；
    4. 没有考虑缺失值。
- C4.5

  - C4.5 算法最大的特点是克服了 ID3 对特征数目的偏重这一缺点，引入信息增益率来作为分类标准。
  - **C4.5 相对于 ID3 的缺点对应有以下改进方式：**
    
    1. 引入悲观剪枝策略进行后剪枝
    2. 引入信息增益率作为划分标准
    3. 将连续特征离散化，假设 n 个样本的连续特征 A 有 m 个取值，C4.5 将其排序并取相邻两样本值的平均数共 m-1 个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点；
    4. 在特征值缺失的情况下进行划分特征的选择？（即如何计算特征的信息增益率）C4.5 的做法是：对于具有缺失值特征，用没有缺失的样本子集所占比重来折算；
    5. 选定该划分特征，对于缺失该特征值的样本如何处理？（即到底把这个样本划分到哪个结点里）C4.5 的做法是：将样本同时划分到所有子节点，不过要调整样本的权重值，其实也就是以不同概率划分到不同节点中。
  - **缺点**
    1. 剪枝策略可以再优化；
    2. C4.5 用的是多叉树，用二叉树效率更高；
    3. C4.5 只能用于分类；
    4. C4.5 使用的熵模型拥有大量耗时的对数运算，连续值还有排序运算；
- CART
  - CART 在 C4.5 的基础上进行了很多提升。
    1. C4.5 为多叉树，运算速度慢，CART 为二叉树，运算速度快；
    2. C4.5 只能分类，CART 既可以分类也可以回归；
    3. **CART 使用 Gini 系数作为变量的不纯度量，减少了大量的对数运算；**
    4. CART 采用代理测试来估计缺失值，而 C4.5 以不同概率划分到不同节点中；
    5. CART 采用“基于代价复杂度剪枝”方法进行剪枝，而 C4.5 采用悲观剪枝方法。

除了之前列出来的划分标准、剪枝策略、连续值确实值处理方式等之外，我再介绍一些其他差异：

- **划分标准的差异：**ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。
- **使用场景的差异：**ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快；
- **样本数据的差异：**ID3 只能处理离散数据且缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ；
- **样本特征的差异：**ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征；
- **剪枝策略的差异：**ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正树的准确性，而 CART 是通过代价复杂度剪枝。

### 决策树的剪枝（正则化思想）

#### 从损失函数角度理解剪枝

- **第一项表示对训练数据的预测误差**
  - 理解：熵为0代表纯度最高，那么既然不为0则表示离最高的纯度还有差距，这个差距可以理解为预测误差；从而第一项的乘积表示每个节点的误差之和。
- **第二项表示模型的复杂度**
  - 理解：节点数越多，表示模型越复杂，因为分的越细必定用到的规则越多；因此节点的数量正比于模型的复杂度

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/YrF05F.png" alt="YrF05F" style="zoom:50%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/fJaCcq.png" alt="fJaCcq" style="zoom: 50%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/3hOYWi.png" alt="3hOYWi" style="zoom:50%;" />

#### 预剪枝

- 如何判断泛化性能得到提升？
  - 看验证集的精度（即准确率），对比使用评估指标进行划分与不进行划分的精度区别

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/89vE9V.png" alt="89vE9V" style="zoom:50%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/I1UlIO.png" alt="I1UlIO" style="zoom:50%;" />


##### 预剪枝的优势与劣势

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/DZN1kh.png" alt="DZN1kh" style="zoom:50%;" />

#### 后剪枝及其与预剪枝的对比

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ixAIAg.png" alt="ixAIAg" style="zoom:50%;" />

### 补充：连续与缺失值

由于连续属性的可取值数目不再有限，因此，不能直接根据连续属性的可取值来对结点进行划分。此时，连续属性离散化技术可派上用场。最简单的策略是**采用二分法（bi-partition）对连续属性进行处理**，这正是 **C4.5 决策树**算法中采用的机制。

- 设置滑动窗口，讲连续值数据分为两部分，然后计算其信息增益，取最大的信息增益作为划分点

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/aOl3C3.png" alt="aOl3C3" style="zoom:50%;" />

##### 两个问题

1. 如何在属性值缺失的情况下进行划分属性选择？

   **答：对一个样本，如果只是部分属性缺失，那么还有利用的价值，没有必要删除。**

   **考虑属性a，根据其在属性值a上没有缺失值的样本来按信息增益公式进行计算，从而进行划分**

2. 给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？

   **答：若样本x在属性a上的取值已知，则根据属性的划分取值对样本x直接进行划分；若样本x在属性值a上取值未知，则将x同时划入所有的子分类节点，但是根据已知样本在各分类节点中的个数占比来调整样本x的权值。**

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/OnWv7q.png" alt="OnWv7q" style="zoom:50%;" />



### CART树

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/7Cwo1S.png" alt="7Cwo1S" style="zoom:50%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/BSkFUM.png" alt="BSkFUM" style="zoom:50%;" />

