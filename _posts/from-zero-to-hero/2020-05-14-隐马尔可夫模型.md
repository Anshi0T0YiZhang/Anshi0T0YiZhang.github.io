---
title: "隐马尔可夫模型"
subtitle: "Hidden Markov Model"
layout: post
author: "echisenyang"
header-style: text
hidden: true
catalog: true
tags:
  - 输出计划
  - 李航统计学习第二版
  - PRML
---

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/ih7IZu.jpg" alt="ih7IZu" style="zoom: 25%;" />


马尔科夫模型 (Markov model)，其中我们**假定未来的预测仅与最近的观测有关，⽽独⽴于其他所有的观测**。

虽然这样的模型可以计算，但是仍然具有很严重的局限性。通过**引⼊潜在变量，我们可以得 到⼀个更加⼀般的框架**， 同时仍然保持计算上的可处理性， 这就引出了**状态空间模型**（state space model）

- 我们关注状态空间模型的**两个最重要的例⼦**，即**隐马尔可夫模型**（hidden Markov model），其中潜在变量是离散的， 以及线性动态系统（linear dynamical system）， 其中潜在变量服从⾼斯分布。 这两个模 型都使⽤具有树结构（没有环）的有向图描述，这样就可以使⽤加和-乘积算法来⾼效地进⾏推断。

### 马尔科夫模型

处理顺序数据的最简单的⽅式是忽略顺序的性质，将观测看做独⽴同分布，对应于图13.2所 ⽰的图。如果我们将所有的数据都看成独⽴同分布的， 那么我们能够从数据中得到的唯⼀的信息就是数据的相对频率。但是这样明显会带来问题。

$$
p\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{N}, \boldsymbol{z}_{1}, \ldots, \boldsymbol{z}_{N}\right)=p\left(\boldsymbol{z}_{1}\right)\left[\prod_{n=2}^{N} p\left(\boldsymbol{z}_{n}  \mid  \boldsymbol{z}_{n-1}\right)\right] \prod_{n=1}^{N} p\left(\boldsymbol{x}_{n}  \mid  \boldsymbol{z}_{n}\right)
$$

![Z4w7Qm](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Z4w7Qm.png)

使⽤**d-划分准则**，我们看到**总存在⼀个路径通过潜在变量连接了任意两个观测变量 $x_n$ 和 $x_m$ ，并 且这个路径永远不会被阻隔**。因此对于观测变量 $x_{n+1}$ 来说，给定所有之前的观测，**条件概率分布 $p(x_{n+1} \mid x_1 , . . . , x_n )$ 不会表现出任何的条件独⽴性，因此我们对 $x_{n+1}$ 的预测依赖于所有之前的观测**。然⽽，观测变量不满⾜任何阶数的马尔科夫性质。

对于**顺序数据**来说，这个图描述了两个重要的模型。

- 如果**潜在变量是离散的**，那么我们得到了隐马尔科夫模型（hidden Markov model）或者HMM（Elliott et al., 1995）。 注意，**HMM中的观测变量可以是离散的或者是连续的**，并且可以使⽤许多不同的条件概率分布进⾏建模。
- 如果**潜在变量和观测变量都是⾼斯变量**（结点的条件概率分布对于⽗结点的依赖是线性⾼斯的形式），那么我们就得到了**线性动态系统**（linear dynamical system）。

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/DMmVGV.jpg" alt="DMmVGV" style="zoom:25%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/UPjRpE.jpg" alt="UPjRpE" style="zoom:25%;" />

![10.隐马尔可夫模型-52](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/10.隐马尔可夫模型-5%202.jpg)

![Olp8M3](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Olp8M3.jpg)

![10.隐马尔可夫模型-7](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/10.隐马尔可夫模型-7.jpg)

![10.隐马尔可夫模型-8](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/10.隐马尔可夫模型-8.jpg)

![10.隐马尔可夫模型-9](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/10.隐马尔可夫模型-9.jpg)

![10.隐马尔可夫模型-10](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/10.隐马尔可夫模型-10.jpg)

### 隐马尔科夫模型

![h6CW3n](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/h6CW3n.png)

#### 转移概率

由于**潜在变量是 $K$ 维⼆值变量（0或1）**，因此条件概率分布对应于数字组成的表格，记作 $A$，它的元素被称为转移概率（transition probabilities）。

- 元素为 $A_{jk} ≡ p(z_{nk} = 1  \mid  z_{n−1,j} = 1)$，表示 第n-1个潜在变量的第j个元素 到 第n个潜在变量的第k个元素的转移概率为 $A_{jk}$
- 由 于它们是概率值，因此满足 $0 \leq A_{jk} \leq 1$ 且 $\sum_k A_{jk} = 1$ 
- 可以显式地将条件概率分布写成：

$$
p(z_n \mid z_{n-1},A) = \prod\limits_{k=1}^K\prod\limits_{j=1}^KA_{jk}^{z_{n-1, j}z_{nk}} \tag{13.7}
$$

有时可以**将状态画成状态转移图中的⼀个结点**， 这样就可以**图形化地表⽰出转移矩阵**。 图 13.6给出了K = 3的情形。注意，这不是⼀个概率图模型，因为结点不是单独的变量⽽是⼀个变量的各个状态，因此我们**⽤⽅框⽽不是圆圈来表⽰状态**。





![HzKjMu](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/HzKjMu.png)

![MyIbWW](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/MyIbWW.png)

#### 发射状态

可以通过定义观测变量的条件概率分布 $p(x_n  \mid  z_n , ϕ)$ 来确定⼀个概率模型， 其中 $ϕ$ 是控制概率分布的参数集合。 这些条件概率被称为发射概率（emission probabilities）， 可以是⾼斯分布（x是连续变量）， 也可以是条件概率表格（x是离散变量）。 由于 $x_n$ 是观测值，因此对于⼀个给定的ϕ值，概率分布 $p(x_n  \mid  z_n , ϕ)$ 由⼀个 $K$ 维的向量组成，对应于⼆值向量 $z_n$ 的 $K$ 个可能的状态。我们可以将发射概率表⽰为
$$
p(x_n \mid z_n,\psi) = \prod\limits_{k=1}^Kp(x_n \mid \phi_k)^{z_{nk}} \tag{13.9}
$$
我们将注意⼒集中在**同质的（homogeneous）模型**上，其中**所有控制潜在变量的条件概率分布都共享相同的参数A**，类似地**所有发射概率分布都共享相同的参数 $ϕ$（推⼴到更⼀般的情形很容易）**。

- 注意，对于⼀个**独⽴同分布**的数据集，⼀个混合模型对应于参数 $A_{jk}$ 对于所有的 $j$ 值都相同的情况，从⽽条件概率分布 $p(z_n  \mid  z_{n−1})$ 与 $z_{n−1}$ ⽆关。这对应于将图13.5所⽰的图模型中的⽔平链接都删除。

从⽽观测变量和潜在变量上的联合概率分布为

$$
p(X,Z \mid \theta) = p(z_1 \mid \pi)\left[\prod\limits_{n=2}^N p(z_n \mid z_{n−1},A)\right]\prod\limits_{m=1}^Np(x_m \mid z_m,\phi) \tag{13.10}
$$

- **初始潜在结点 $z_1$ 很特别**，因为它没有父结点，因此它的边缘概率分布 $p(z1)$ 由一个**概率向量 $π$ 表示**，元素为 $π_k≡p(z_{1k}=1)$，其中 $\sum_k\pi_k = 1$ 

$$
p(z_1  \mid  \pi) = \prod\limits_{k=1}^K\pi_k^{z_{1k}} \tag{13.8}
$$

#### 如何生成样本

从⽣成式的观点考虑隐马尔科夫模型，我们可以更好地理解隐马尔科夫模型。回忆⼀下，为了从⼀个混合⾼斯分布中⽣成样本，我们⾸先随机算侧⼀个分量，选择的概率为混合系数 $π_k$ ， 然后从对应的⾼斯分量中⽣成⼀个样本向量 $x$。这个过程重复 $N$ 次，产⽣ $N$ 个独⽴样本组成的数据集。**在隐马尔科夫模型的情形，这个步骤修改如下**。**⾸先我们选择初始的潜在变量 $z_1$，概率由参数 $π_k$ 控制，然后采样对应的观测 $x_1$ 。现在我们使⽤已经初始化的 $z_1$ 的值， 根据转移概 率 $p(z_2  \mid  z_1 )$ 来选择变量 $z_2$ 的状态。 从⽽我们以概率 $A_{jk}$ 选择 $z_2$ 的状态k ， 其中 $k = 1, . . . , K$ 。 ⼀ 旦我们知道了 $z_2$ ，我们就可以对 $x_2$ 采样，从⽽也可以对下⼀个潜在变量 $z_3$ 采样，以此类推。**这是有向图模型的**祖先采样**的⼀个例⼦。 

- 例如， 如果我们有⼀个模型， 其中对⾓转移元素 $A_{kk}$ ⽐⾮对⾓的元素⼤得多，那么⼀个典型的数据序列中，会有连续很长的⼀系列点由同⼀个概率分布⽣成，⽽从⼀个分量转移到另⼀个分量不会经常发⽣。

这个标准的HMM模型有很多变体， 例如可以通过**对转移矩阵A的形式进⾏限制的⽅式**进⾏限制（Rabiner, 1989）。 这⾥我们介绍⼀种在实际应⽤中很重要的变体， 被称为**从左到右HMM**（left-to-right HMM）， 它将A中 $k < j$ 的元素 $A_{jk}$ 设置为零。 图13.9给出了具有三个

![fromag](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/fromag.png)

#### Evaluation问题：前向算法

前向后向算法除了本⾝具有重要的实际应⽤价值以外，还很好地说明了之前章节中介绍的许多概念。因此我们在本节中会给出前向后向算法的⼀个“传统的”推导，**使⽤概率的加和规则和乘积规则**，并且**利⽤由d-划分从对应的图模型中得到的条件独⽴性质**。

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/tEcvLb.png" alt="tEcvLb" style="zoom:50%;" />

![6DKV2E](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/6DKV2E.png)

#### Evaluation问题：后向算法

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/1Gukp3.png" alt="1Gukp3" style="zoom:50%;" />

#### Learning问题：EM算法

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/lBZNsY.png" alt="lBZNsY" style="zoom:50%;" />

### hmm vs rnn's

**What are the strengths of RNNs over HMMs?**

- **Non-Linearity**. Period.

- Hidden Markov Models and Recurrent Networks both try to capture sequential information in data and they work in exactly the same way. The only difference is that **RNNs apply a point-wise non-linearity to the output at every timestep (非线性激活函数)** and HMMs don’t.

**Why are HMMs replaced by RNNs in many applications?**

- **Non-Linearity** is at the heart of Deep Learning. It is what **makes the networks more expressive and more adaptive at learning from real world data (非线性可以提升模型能力)**. Linear models lag behind by a large margin in terms of the results. Hence it is inevitable that DL models like RNNs will take over linear models like HMMs.

HMM和RNN在基本结构上是挺像的，**都是通过hidden state的演化来刻画序列间的依赖关系**。不同是：

- **HMM本质是一个概率模型**，而RNN不是，**一阶HMM只与前一个状态有关**，而忽略其他过去的状态，但**RNN可以考虑很长的历史信息 (尽管会遇到梯度消失的问题，而无法做到真正意义上的long-time dependency，但是后续的lstm等门控网络从设计上解决了这一问题)**。
- 隐状态的表示: **HMM是one hot, RNN是distributed representation**。**RNN的表示能力强很多**，或者说在面对高维度时，表示效率更高。类似nlp里，对单个token的表示，一种是onehot, 一种是word vector。
- 隐状态的演化方式: **HMM是线性的，RNN是高度非线性**。
- 深度：lstm可以增加depth, 随着depth增加，表示能力指数增加。
- **RNN的最大优势在于，可以真正充分地利用所有上文信息来预测下一个词**，而不像HMM那样，只能开一个n个词的窗口，只用前n个词来预测下一个词。从形式上看，这是一个非常“终极”的模型，毕竟语言模型里能用到的信息，他全用上了。



