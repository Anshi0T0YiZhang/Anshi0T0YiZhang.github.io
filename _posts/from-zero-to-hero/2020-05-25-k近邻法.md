---
title: "k近邻法"
subtitle: "k-nearest neighbor, k-NN"
layout: post
author: "echisenyang"
header-style: text
hidden: true
catalog: true
tags:
  - 输出计划
  - 李航统计学习第二版
---



### 综述

K 近邻法（k- nearest neighbor, k-NN）是一种基本分类与回归方法。

- 本书只讨论分类问题中的 k 近邻法。
- k 近邻法的输入为实例的特征向量，对应于特征空间的点输出为实例的类别，可以取多类。
- k 近邻法假设给定一个训练数据集，其中的实例类别已定。分类时，**对新的实例**，根据其 k 个最近邻的训练实例的类别，通过**多数表决**等方式进行预测。因此，k 近邻法不具有显式的学习过程。**k 近邻法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”**。*k 值的选择、距离度量及分类决策规则* 是 k 近邻法的三个基本要素。k 近邻法 1968 年由 Cover 和 Hart 提出。
- K近邻在局部合成数据可以被理解为一种**集成学习**，降低了方差。但或许也错误的加强了局部的偶然性，从而增加了过拟合风险。但一般来看，优点大于风险

![NKXEQy](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/NKXEQy.jpg)



### 模型

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/sajvuF.png" alt="sajvuF" style="zoom:50%;" />

### k近邻算法的实现：kd树

- kd树的 **每个划分** 都是 **一个k维区域**

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/YgRjzk.png" alt="YgRjzk" style="zoom:50%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/1Ihg0d.png" alt="1Ihg0d" style="zoom:50%;" />

### 【递归构造kd树代码】

```python
# 算法3.2 平衡kd树
class TreeNode:
    def __init__(self, val=0, left=None, right=None):
        self.val = val
        self.left = left
        self.right = right

class KdTree:
    """
    build kdtree recursively along axis, split on median point.
    k:      k dimensions
    method: alternate/variance, 坐标轴轮替或最大方差轴
    """
    
    def __init__(self, k=2, method='alternate'):
        self.k = k
        self.method = method
        self.root = TreeNode()
        
    def 
        
    def build(self, points, depth=0):
        n = len(points)
        if n <= 0:
            return None
        
        if self.method == 'alternate':
            axis = depth % self.k
        elif self.method == 'variance':
            axis = np.argmax(np.var(points, axis=0), axis=0)
        # 按 坐标轴轮替或最大方差轴 选择并排序数据
        sorted_points = sorted(points, key=lambda point: point[axis])
        
        root.val = sorted_points[n // 2], # median point
        root.left = self.build(sorted_points[:n//2], depth+1)
        root.right = self.build(sorted_points[n//2+1:], depth+1)
        
        return root
```





### 搜索kd树

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/TwEXZB.png" alt="TwEXZB" style="zoom:50%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/lJMRVR.png" alt="lJMRVR" style="zoom:50%;" />

### 【搜索kd树代码】

```python
class SearchKdTree:
    """
    search closest point
    """
    def __init__(self, k=2):
        self.k = k
        
    def __closer_distance(self, pivot, p1, p2):
        if p1 is None:
            return p2
        if p2 is None:
            return p1
        
        d1 = distance(pivot, p1)
        d2 = distance(pivot, p2)

        if d1 < d2:
            return p1
        else:
            return p2
    
    def fit(self, root, point, depth=0):
        if root is None:
            return None
        
        axis = depth % self.k
        
        next_branch = None
        opposite_branch = None
        
        # (1) 在 kd 树中找出包含目标点 x 的叶结点：从根结点出发，递归地向下访问 kd 树。若目标点当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点。直到子结点为叶结点为止。
        if point[axis] < root['point'][axis]:
            next_branch = root['left']
            opposite_branch = root['right']
        else:
            next_branch = root['right']
            opposite_branch = root['left']
            
        # (2) 以此叶结点为“当前最近点”。
        # (3) 递归地向上回退，在每个结点进行以下操作：
        # 	(a）如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点”。
        best = self.__closer_distance(point,
                                     self.fit(next_branch,
                                             point,
                                             depth+1),
                                     root['point'])
        # 	(b）当前最近点一定存在于该结点一个子结点对应的区域。检査该子结点的父结点的另一子结点对应的区域是否有更近的点。
        if distance(point, best) > abs(point[axis] - root['point'][axis]):
            best = self.__closer_distance(point,
                                     self.fit(opposite_branch,
                                             point,
                                             depth+1),
                                     best)
        # (4) 当回退到根结点时，搜索结束。最后的“当前最近点”即为 m 的最近邻点。  
        return best
```



### **K-means vs KNN**

- KNN是一种基于实例的学习算法，它不同于贝叶斯、决策树等算法，**KNN不需要训练**，当有新的实例出现时，**直接在训练数据集中找k个最近的实例**，把这个新的实例分配给这k个训练实例中实例数最多类。KNN也称为**懒惰学习**，它不需要训练过程，在类标边界比较整齐的情况下分类的准确率很高。KNN算法需要人为决定K的取值，即找几个最近的实例，k值不同，分类结果的结果也会不同。

- K-means需要训练，不断迭代更新类中心点，直至收敛

